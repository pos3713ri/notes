<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Chapter 11 Regression | Concepts and Computation: An Introduction to Political Methodology</title>
  <meta name="description" content="These are my notes for my class introducing the fundamental conceptual and computational tools to undergraduate students in politial science." />
  <meta name="generator" content="bookdown 0.20 and GitBook 2.6.7" />

  <meta property="og:title" content="Chapter 11 Regression | Concepts and Computation: An Introduction to Political Methodology" />
  <meta property="og:type" content="book" />
  
  
  <meta property="og:description" content="These are my notes for my class introducing the fundamental conceptual and computational tools to undergraduate students in politial science." />
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapter 11 Regression | Concepts and Computation: An Introduction to Political Methodology" />
  
  <meta name="twitter:description" content="These are my notes for my class introducing the fundamental conceptual and computational tools to undergraduate students in politial science." />
  

<meta name="author" content="Rob Lytle and Carlisle Rainey" />


<meta name="date" content="2020-11-10" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="correlation-coefficient.html"/>
<link rel="next" href="appendix-a-normal-table.html"/>
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />









<script src="libs/accessible-code-block-0.0.1/empty-anchor.js"></script>
<script src="libs/kePrint-0.0.1/kePrint.js"></script>
<link href="libs/lightable-0.0.1/lightable.css" rel="stylesheet" />
<script>
    $(document).ready(function () {
        process_solutions();
    });
    function process_solutions() {
        $("div.section[id^='solution']").each(function(i) {
        var soln_wrapper_id = "cvxr_ex_" + i;
        var solution_id = $(this).attr('id');
        var button = $("<button onclick=\"toggle_solution('" + soln_wrapper_id + "')\">Show/Hide</button>");
        var new_div = $("<div id='" + soln_wrapper_id + "' class='solution' style='display: none;'></div>");
        var h = $(this).first();
        var others = $(this).children().slice(1);
        $(others).each(function() {
            $(this).appendTo($(new_div));
        });
        $(button).insertAfter($(h));
        $(new_div).insertAfter($(button));
        })
    }
    function toggle_solution(el_id) {
      $("#" + el_id).toggle();
    } 
</script>


<style type="text/css">
code.sourceCode > span { display: inline-block; line-height: 1.25; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode { white-space: pre; position: relative; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
code.sourceCode { white-space: pre-wrap; }
code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">Concepts and Computation</a></li>

<li class="divider"></li>
<li class="chapter" data-level="1" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i><b>1</b> Overview</a></li>
<li class="chapter" data-level="2" data-path="statistical-computing-with-r.html"><a href="statistical-computing-with-r.html"><i class="fa fa-check"></i><b>2</b> Statistical Computing with R</a><ul>
<li class="chapter" data-level="2.1" data-path="statistical-computing-with-r.html"><a href="statistical-computing-with-r.html#r-as-a-calculator"><i class="fa fa-check"></i><b>2.1</b> R as a Calculator</a></li>
<li class="chapter" data-level="2.2" data-path="statistical-computing-with-r.html"><a href="statistical-computing-with-r.html#scripts"><i class="fa fa-check"></i><b>2.2</b> Scripts</a><ul>
<li class="chapter" data-level="2.2.1" data-path="statistical-computing-with-r.html"><a href="statistical-computing-with-r.html#importance"><i class="fa fa-check"></i><b>2.2.1</b> Importance</a></li>
<li class="chapter" data-level="2.2.2" data-path="statistical-computing-with-r.html"><a href="statistical-computing-with-r.html#comments"><i class="fa fa-check"></i><b>2.2.2</b> Comments</a></li>
</ul></li>
<li class="chapter" data-level="2.3" data-path="statistical-computing-with-r.html"><a href="statistical-computing-with-r.html#object-oriented-programming"><i class="fa fa-check"></i><b>2.3</b> Object-Oriented Programming</a><ul>
<li class="chapter" data-level="2.3.1" data-path="statistical-computing-with-r.html"><a href="statistical-computing-with-r.html#scalars"><i class="fa fa-check"></i><b>2.3.1</b> Scalars</a></li>
<li class="chapter" data-level="2.3.2" data-path="statistical-computing-with-r.html"><a href="statistical-computing-with-r.html#functions"><i class="fa fa-check"></i><b>2.3.2</b> Functions</a></li>
<li class="chapter" data-level="2.3.3" data-path="statistical-computing-with-r.html"><a href="statistical-computing-with-r.html#vectors"><i class="fa fa-check"></i><b>2.3.3</b> Vectors</a></li>
<li class="chapter" data-level="2.3.4" data-path="statistical-computing-with-r.html"><a href="statistical-computing-with-r.html#more-information"><i class="fa fa-check"></i><b>2.3.4</b> More Information</a></li>
</ul></li>
<li class="chapter" data-level="2.4" data-path="statistical-computing-with-r.html"><a href="statistical-computing-with-r.html#missing-values"><i class="fa fa-check"></i><b>2.4</b> Missing Values</a></li>
<li class="chapter" data-level="2.5" data-path="statistical-computing-with-r.html"><a href="statistical-computing-with-r.html#logical-operators"><i class="fa fa-check"></i><b>2.5</b> Logical Operators</a></li>
<li class="chapter" data-level="2.6" data-path="statistical-computing-with-r.html"><a href="statistical-computing-with-r.html#packages"><i class="fa fa-check"></i><b>2.6</b> Packages</a><ul>
<li class="chapter" data-level="2.6.1" data-path="statistical-computing-with-r.html"><a href="statistical-computing-with-r.html#installing-packages"><i class="fa fa-check"></i><b>2.6.1</b> Installing Packages</a></li>
<li class="chapter" data-level="2.6.2" data-path="statistical-computing-with-r.html"><a href="statistical-computing-with-r.html#loading-packages"><i class="fa fa-check"></i><b>2.6.2</b> Loading Packages</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="3" data-path="loading-data-into-r.html"><a href="loading-data-into-r.html"><i class="fa fa-check"></i><b>3</b> Loading Data into R</a><ul>
<li class="chapter" data-level="3.1" data-path="loading-data-into-r.html"><a href="loading-data-into-r.html#the-terms"><i class="fa fa-check"></i><b>3.1</b> The Terms</a><ul>
<li class="chapter" data-level="3.1.1" data-path="loading-data-into-r.html"><a href="loading-data-into-r.html#file-types"><i class="fa fa-check"></i><b>3.1.1</b> File Types</a></li>
<li class="chapter" data-level="3.1.2" data-path="loading-data-into-r.html"><a href="loading-data-into-r.html#comma-separated-value-format-.csv"><i class="fa fa-check"></i><b>3.1.2</b> Comma-Separated Value Format (<code>.csv</code>)</a></li>
<li class="chapter" data-level="3.1.3" data-path="loading-data-into-r.html"><a href="loading-data-into-r.html#the-working-directory"><i class="fa fa-check"></i><b>3.1.3</b> The Working Directory</a></li>
<li class="chapter" data-level="3.1.4" data-path="loading-data-into-r.html"><a href="loading-data-into-r.html#the-path"><i class="fa fa-check"></i><b>3.1.4</b> The Path</a></li>
<li class="chapter" data-level="3.1.5" data-path="loading-data-into-r.html"><a href="loading-data-into-r.html#determining-the-path"><i class="fa fa-check"></i><b>3.1.5</b> Determining the Path</a></li>
<li class="chapter" data-level="3.1.6" data-path="loading-data-into-r.html"><a href="loading-data-into-r.html#rio"><i class="fa fa-check"></i><b>3.1.6</b> <code>rio</code></a></li>
</ul></li>
<li class="chapter" data-level="3.2" data-path="loading-data-into-r.html"><a href="loading-data-into-r.html#data-frames"><i class="fa fa-check"></i><b>3.2</b> Data Frames</a><ul>
<li class="chapter" data-level="3.2.1" data-path="loading-data-into-r.html"><a href="loading-data-into-r.html#working-with-variables-in-data-frames"><i class="fa fa-check"></i><b>3.2.1</b> Working with Variables in Data Frames</a></li>
</ul></li>
<li class="chapter" data-level="3.3" data-path="loading-data-into-r.html"><a href="loading-data-into-r.html#how-well-always-use-r"><i class="fa fa-check"></i><b>3.3</b> How We’ll Always Use R</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="histograms.html"><a href="histograms.html"><i class="fa fa-check"></i><b>4</b> Histograms</a><ul>
<li class="chapter" data-level="4.1" data-path="histograms.html"><a href="histograms.html#nominate-data"><i class="fa fa-check"></i><b>4.1</b> NOMINATE Data</a></li>
<li class="chapter" data-level="4.2" data-path="histograms.html"><a href="histograms.html#histograms-geom_histogram"><i class="fa fa-check"></i><b>4.2</b> Histograms: <code>geom_histogram()</code></a><ul>
<li class="chapter" data-level="4.2.1" data-path="histograms.html"><a href="histograms.html#the-three-critical-components"><i class="fa fa-check"></i><b>4.2.1</b> The Three Critical Components</a></li>
<li class="chapter" data-level="4.2.2" data-path="histograms.html"><a href="histograms.html#drawing-a-histogram"><i class="fa fa-check"></i><b>4.2.2</b> Drawing a Histogram</a></li>
<li class="chapter" data-level="4.2.3" data-path="histograms.html"><a href="histograms.html#but-geom_histogram-uses-counts"><i class="fa fa-check"></i><b>4.2.3</b> But <code>geom_histogram()</code> uses counts!</a></li>
</ul></li>
<li class="chapter" data-level="4.3" data-path="histograms.html"><a href="histograms.html#filtering-filter"><i class="fa fa-check"></i><b>4.3</b> Filtering: <code>filter()</code></a></li>
<li class="chapter" data-level="4.4" data-path="histograms.html"><a href="histograms.html#faceting-facet_wrap"><i class="fa fa-check"></i><b>4.4</b> Faceting: <code>facet_wrap()</code></a></li>
<li class="chapter" data-level="4.5" data-path="histograms.html"><a href="histograms.html#density-plots-geom_density"><i class="fa fa-check"></i><b>4.5</b> Density Plots: <code>geom_density()</code></a></li>
<li class="chapter" data-level="4.6" data-path="histograms.html"><a href="histograms.html#color-and-fill"><i class="fa fa-check"></i><b>4.6</b> Color and Fill</a><ul>
<li class="chapter" data-level="4.6.1" data-path="histograms.html"><a href="histograms.html#color"><i class="fa fa-check"></i><b>4.6.1</b> Color</a></li>
<li class="chapter" data-level="4.6.2" data-path="histograms.html"><a href="histograms.html#fill"><i class="fa fa-check"></i><b>4.6.2</b> Fill</a></li>
<li class="chapter" data-level="4.6.3" data-path="histograms.html"><a href="histograms.html#alpha"><i class="fa fa-check"></i><b>4.6.3</b> Alpha</a></li>
</ul></li>
<li class="chapter" data-level="4.7" data-path="histograms.html"><a href="histograms.html#labels-labs"><i class="fa fa-check"></i><b>4.7</b> Labels: <code>labs()</code></a></li>
<li class="chapter" data-level="4.8" data-path="histograms.html"><a href="histograms.html#themes-theme_bw-and-others"><i class="fa fa-check"></i><b>4.8</b> Themes: <code>theme_bw()</code> and Others</a></li>
<li class="chapter" data-level="4.9" data-path="histograms.html"><a href="histograms.html#putting-it-all-together"><i class="fa fa-check"></i><b>4.9</b> Putting It All Together</a></li>
<li class="chapter" data-level="4.10" data-path="histograms.html"><a href="histograms.html#saving-plots-ggsave"><i class="fa fa-check"></i><b>4.10</b> Saving Plots: <code>ggsave()</code></a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="average-and-sd.html"><a href="average-and-sd.html"><i class="fa fa-check"></i><b>5</b> Average and SD</a><ul>
<li class="chapter" data-level="5.1" data-path="average-and-sd.html"><a href="average-and-sd.html#the-intuition"><i class="fa fa-check"></i><b>5.1</b> The Intuition</a></li>
<li class="chapter" data-level="5.2" data-path="average-and-sd.html"><a href="average-and-sd.html#the-measures"><i class="fa fa-check"></i><b>5.2</b> The Measures</a><ul>
<li class="chapter" data-level="5.2.1" data-path="average-and-sd.html"><a href="average-and-sd.html#the-average"><i class="fa fa-check"></i><b>5.2.1</b> The Average</a></li>
<li class="chapter" data-level="5.2.2" data-path="average-and-sd.html"><a href="average-and-sd.html#the-median"><i class="fa fa-check"></i><b>5.2.2</b> The Median</a></li>
<li class="chapter" data-level="5.2.3" data-path="average-and-sd.html"><a href="average-and-sd.html#the-standard-deviation"><i class="fa fa-check"></i><b>5.2.3</b> The Standard Deviation</a></li>
<li class="chapter" data-level="5.2.4" data-path="average-and-sd.html"><a href="average-and-sd.html#other-summaries"><i class="fa fa-check"></i><b>5.2.4</b> Other Summaries</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="6" data-path="average-and-sd-in-r.html"><a href="average-and-sd-in-r.html"><i class="fa fa-check"></i><b>6</b> Average and SD in R</a><ul>
<li class="chapter" data-level="6.1" data-path="average-and-sd-in-r.html"><a href="average-and-sd-in-r.html#mean-and-sd"><i class="fa fa-check"></i><b>6.1</b> <code>mean()</code> and <code>sd()</code></a><ul>
<li class="chapter" data-level="6.1.1" data-path="average-and-sd-in-r.html"><a href="average-and-sd-in-r.html#a-note-about-rs-sd-function"><i class="fa fa-check"></i><b>6.1.1</b> A Note About R’s <code>sd()</code> Function</a></li>
</ul></li>
<li class="chapter" data-level="6.2" data-path="average-and-sd.html"><a href="average-and-sd.html#other-summaries"><i class="fa fa-check"></i><b>6.2</b> Other Summaries</a></li>
<li class="chapter" data-level="6.3" data-path="average-and-sd-in-r.html"><a href="average-and-sd-in-r.html#data-frame-nuance"><i class="fa fa-check"></i><b>6.3</b> Data Frame Nuance</a></li>
<li class="chapter" data-level="6.4" data-path="average-and-sd-in-r.html"><a href="average-and-sd-in-r.html#the-group_bysummarize-workflow"><i class="fa fa-check"></i><b>6.4</b> The <code>group_by()</code>/<code>summarize()</code> Workflow</a><ul>
<li class="chapter" data-level="6.4.1" data-path="average-and-sd-in-r.html"><a href="average-and-sd-in-r.html#group_by"><i class="fa fa-check"></i><b>6.4.1</b> <code>group_by()</code></a></li>
<li class="chapter" data-level="6.4.2" data-path="average-and-sd-in-r.html"><a href="average-and-sd-in-r.html#summarize"><i class="fa fa-check"></i><b>6.4.2</b> <code>summarize()</code></a></li>
</ul></li>
<li class="chapter" data-level="6.5" data-path="average-and-sd-in-r.html"><a href="average-and-sd-in-r.html#geom_line"><i class="fa fa-check"></i><b>6.5</b> <code>geom_line()</code></a></li>
</ul></li>
<li class="chapter" data-level="7" data-path="the-normal-model.html"><a href="the-normal-model.html"><i class="fa fa-check"></i><b>7</b> The Normal Model</a><ul>
<li class="chapter" data-level="7.1" data-path="average-and-sd.html"><a href="average-and-sd.html#the-intuition"><i class="fa fa-check"></i><b>7.1</b> The Intuition</a></li>
<li class="chapter" data-level="7.2" data-path="the-normal-model.html"><a href="the-normal-model.html#examples"><i class="fa fa-check"></i><b>7.2</b> Examples</a></li>
<li class="chapter" data-level="7.3" data-path="the-normal-model.html"><a href="the-normal-model.html#the-empirical-rule"><i class="fa fa-check"></i><b>7.3</b> The Empirical Rule</a></li>
<li class="chapter" data-level="7.4" data-path="the-normal-model.html"><a href="the-normal-model.html#the-normal-approximation"><i class="fa fa-check"></i><b>7.4</b> The Normal Approximation</a></li>
<li class="chapter" data-level="7.5" data-path="the-normal-model.html"><a href="the-normal-model.html#the-normal-table"><i class="fa fa-check"></i><b>7.5</b> The Normal Table</a></li>
<li class="chapter" data-level="7.6" data-path="the-normal-model.html"><a href="the-normal-model.html#using-a-normal-approximation"><i class="fa fa-check"></i><b>7.6</b> Using a Normal Approximation</a></li>
<li class="chapter" data-level="7.7" data-path="the-normal-model.html"><a href="the-normal-model.html#example-the-extremity-of-party-leaders"><i class="fa fa-check"></i><b>7.7</b> Example: The Extremity of Party Leaders</a></li>
</ul></li>
<li class="chapter" data-level="8" data-path="the-x-y-space.html"><a href="the-x-y-space.html"><i class="fa fa-check"></i><b>8</b> The X-Y Space</a><ul>
<li class="chapter" data-level="8.1" data-path="the-x-y-space.html"><a href="the-x-y-space.html#points"><i class="fa fa-check"></i><b>8.1</b> Points</a></li>
<li class="chapter" data-level="8.2" data-path="the-x-y-space.html"><a href="the-x-y-space.html#lines"><i class="fa fa-check"></i><b>8.2</b> Lines</a><ul>
<li class="chapter" data-level="8.2.1" data-path="the-x-y-space.html"><a href="the-x-y-space.html#the-intercept"><i class="fa fa-check"></i><b>8.2.1</b> The Intercept</a></li>
<li class="chapter" data-level="8.2.2" data-path="the-x-y-space.html"><a href="the-x-y-space.html#slope"><i class="fa fa-check"></i><b>8.2.2</b> Slope</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="9" data-path="the-scatterplot.html"><a href="the-scatterplot.html"><i class="fa fa-check"></i><b>9</b> The Scatterplot</a><ul>
<li class="chapter" data-level="9.1" data-path="the-scatterplot.html"><a href="the-scatterplot.html#geom_point"><i class="fa fa-check"></i><b>9.1</b> <code>geom_point()</code></a></li>
<li class="chapter" data-level="9.2" data-path="the-scatterplot.html"><a href="the-scatterplot.html#example-gamsons-law"><i class="fa fa-check"></i><b>9.2</b> Example: Gamson’s Law</a></li>
<li class="chapter" data-level="9.3" data-path="the-scatterplot.html"><a href="the-scatterplot.html#example-gapminder"><i class="fa fa-check"></i><b>9.3</b> Example: Gapminder</a></li>
</ul></li>
<li class="chapter" data-level="10" data-path="correlation-coefficient.html"><a href="correlation-coefficient.html"><i class="fa fa-check"></i><b>10</b> Correlation Coefficient</a><ul>
<li class="chapter" data-level="10.1" data-path="correlation-coefficient.html"><a href="correlation-coefficient.html#intuition"><i class="fa fa-check"></i><b>10.1</b> Intuition</a></li>
<li class="chapter" data-level="10.2" data-path="correlation-coefficient.html"><a href="correlation-coefficient.html#computing"><i class="fa fa-check"></i><b>10.2</b> Computing</a><ul>
<li class="chapter" data-level="10.2.1" data-path="correlation-coefficient.html"><a href="correlation-coefficient.html#by-hand"><i class="fa fa-check"></i><b>10.2.1</b> By Hand</a></li>
<li class="chapter" data-level="10.2.2" data-path="correlation-coefficient.html"><a href="correlation-coefficient.html#with-r"><i class="fa fa-check"></i><b>10.2.2</b> With R</a></li>
</ul></li>
<li class="chapter" data-level="10.3" data-path="correlation-coefficient.html"><a href="correlation-coefficient.html#interpreting"><i class="fa fa-check"></i><b>10.3</b> Interpreting</a></li>
<li class="chapter" data-level="10.4" data-path="correlation-coefficient.html"><a href="correlation-coefficient.html#example-clark-and-golder-2006"><i class="fa fa-check"></i><b>10.4</b> Example: Clark and Golder (2006)</a></li>
</ul></li>
<li class="chapter" data-level="11" data-path="regression.html"><a href="regression.html"><i class="fa fa-check"></i><b>11</b> Regression</a><ul>
<li class="chapter" data-level="11.1" data-path="regression.html"><a href="regression.html#the-equation"><i class="fa fa-check"></i><b>11.1</b> The Equation</a></li>
<li class="chapter" data-level="11.2" data-path="regression.html"><a href="regression.html#the-conditional-average"><i class="fa fa-check"></i><b>11.2</b> The Conditional Average</a></li>
<li class="chapter" data-level="11.3" data-path="regression.html"><a href="regression.html#the-best-line"><i class="fa fa-check"></i><b>11.3</b> The Best Line</a><ul>
<li class="chapter" data-level="11.3.1" data-path="regression.html"><a href="regression.html#grid-search"><i class="fa fa-check"></i><b>11.3.1</b> Grid Search</a></li>
<li class="chapter" data-level="11.3.2" data-path="regression.html"><a href="regression.html#analytical-optimization"><i class="fa fa-check"></i><b>11.3.2</b> Analytical Optimization</a></li>
</ul></li>
<li class="chapter" data-level="11.4" data-path="regression.html"><a href="regression.html#the-rms-of-the-residuals"><i class="fa fa-check"></i><b>11.4</b> The RMS of the Residuals</a></li>
<li class="chapter" data-level="11.5" data-path="regression.html"><a href="regression.html#r2"><i class="fa fa-check"></i><b>11.5</b> <span class="math inline">\(R^2\)</span></a><ul>
<li class="chapter" data-level="11.5.1" data-path="regression.html"><a href="regression.html#adequacy-of-a-line"><i class="fa fa-check"></i><b>11.5.1</b> Adequacy of a Line</a></li>
</ul></li>
<li class="chapter" data-level="11.6" data-path="regression.html"><a href="regression.html#fitting-regression-models"><i class="fa fa-check"></i><b>11.6</b> Fitting Regression Models</a><ul>
<li class="chapter" data-level="11.6.1" data-path="regression.html"><a href="regression.html#geom_smooth"><i class="fa fa-check"></i><b>11.6.1</b> <code>geom_smooth()</code></a></li>
<li class="chapter" data-level="11.6.2" data-path="regression.html"><a href="regression.html#lm"><i class="fa fa-check"></i><b>11.6.2</b> <code>lm()</code></a></li>
<li class="chapter" data-level="11.6.3" data-path="regression.html"><a href="regression.html#quick-look-at-the-fit"><i class="fa fa-check"></i><b>11.6.3</b> Quick Look at the Fit</a></li>
</ul></li>
<li class="chapter" data-level="11.7" data-path="regression.html"><a href="regression.html#standard-errors-and-p-values"><i class="fa fa-check"></i><b>11.7</b> Standard Errors and <em>p</em>-Values</a></li>
<li class="chapter" data-level="11.8" data-path="regression.html"><a href="regression.html#a-warning"><i class="fa fa-check"></i><b>11.8</b> A Warning</a></li>
<li class="chapter" data-level="11.9" data-path="regression.html"><a href="regression.html#review-exercises"><i class="fa fa-check"></i><b>11.9</b> Review Exercises</a></li>
</ul></li>
<li class="appendix"><span><b>Appendix</b></span></li>
<li class="chapter" data-level="A" data-path="appendix-a-normal-table.html"><a href="appendix-a-normal-table.html"><i class="fa fa-check"></i><b>A</b> Appendix: A Normal Table</a></li>
<li class="chapter" data-level="" data-path="references.html"><a href="references.html"><i class="fa fa-check"></i>References</a></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Concepts and Computation: An Introduction to Political Methodology</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="regression" class="section level1">
<h1><span class="header-section-number">Chapter 11</span> Regression</h1>
<div id="the-equation" class="section level2">
<h2><span class="header-section-number">11.1</span> The Equation</h2>
<p>Let’s start by describing a scatterplot using a line. Indeed, we can think of the regression equation as an <strong>equation for a scatterplot</strong>.</p>
<p>First, let’s agree that we won’t encounter a scatterplot where all the points <span class="math inline">\((x_i, y_i)\)</span> fall exactly along a line. As such, we need a notation that allows us to distinguish the line from the observed values.</p>
<p>We commonly refer to the values along the line as the “fitted values” (or “predicted values” or “predictions” and the observations themselves as the “observed values” or “observations.”</p>
<p>We use <span class="math inline">\(y_i\)</span> to denote the <span class="math inline">\(i\)</span>th observation of <span class="math inline">\(y\)</span> and use <span class="math inline">\(\hat{y}_i\)</span> to denote the fitted value (usually given <span class="math inline">\(x_i\)</span>).</p>
<p>We write the equation for the line as <span class="math inline">\(\hat{y} = \alpha + \beta x\)</span> and the fitted values as <span class="math inline">\(\hat{y}_i = \alpha + \beta x_i\)</span>.</p>
<p>We refer to the intercept <span class="math inline">\(\alpha\)</span> and the slope <span class="math inline">\(\beta\)</span> as <strong>coefficients</strong>.</p>
<p>We refer to the difference between the observed value <span class="math inline">\(y_i\)</span> and the fitted value <span class="math inline">\(\hat{y}_i = \alpha + \beta x_i\)</span> as the <strong>residual</strong> <span class="math inline">\(r_i = y_i - \hat{y}_i\)</span>.</p>
<p>Thus, for <em>any</em> <span class="math inline">\(\alpha\)</span> and <span class="math inline">\(\beta\)</span>, we can write <span class="math inline">\(y_i = \alpha + \beta x_i + r_i\)</span> for the observations <span class="math inline">\(i = \{1, 2, ..., n\}\)</span>.</p>
<p>Notice that we can break each <span class="math inline">\(y_i\)</span> into two pieces components</p>
<ol style="list-style-type: decimal">
<li>the linear function of <span class="math inline">\(x_i\)</span>: <span class="math inline">\(\alpha + \beta x_i\)</span></li>
<li>the residual <span class="math inline">\(r_i\)</span>.</li>
</ol>
<p>In short, we can describe any scatterplot using the model <span class="math inline">\(y_i = \alpha + \beta x_i + r_i\)</span>.</p>
<ol style="list-style-type: decimal">
<li>The black points show the individual observations <span class="math inline">\((x_i, y_i)\)</span>,</li>
<li>The green line shows the equation <span class="math inline">\(\hat{y} = \alpha + \beta x\)</span>.</li>
<li>The purple star shows the prediction <span class="math inline">\(\hat{y}_i\)</span> for <span class="math inline">\(x = x_i\)</span>.</li>
<li>The orange vertical line shows the residual <span class="math inline">\(r_i = y_i - \hat{y}_i\)</span>.</li>
</ol>
<p><img src="notes_files/figure-html/unnamed-chunk-150-1.png" width="576" style="display: block; margin: auto;" /></p>
<p>Using this generic approach, we can describe <em>any</em> scatterplot using <em>any</em> line. Of course, the line above isn’t a very good line.</p>
<p>How can we go about finding a <em>good</em> line?</p>
<p>Before we talk about a good <em>line</em>, let’s talk about a good <em>point</em>. Suppose you have a dataset <span class="math inline">\(y = \{y_1, y_2, ... , y_n\}\)</span> and you want to predict these observations with a single point <span class="math inline">\(\theta\)</span>. It turns out that the <em>average</em> (the one you learned so long ago) is the best predictor of these observations because it minimizes the RMS of the errors (i.e., the deviations from the average).</p>
</div>
<div id="the-conditional-average" class="section level2">
<h2><span class="header-section-number">11.2</span> The Conditional Average</h2>
<p>Have a look at the scatterplot below. What’s the portfolio share of a party in a coalition government with a seat share of 25%?</p>
<p><img src="notes_files/figure-html/unnamed-chunk-151-1.png" width="480" style="display: block; margin: auto;" /></p>
<p>Your eyes probably immediately begin examining a vertical strip above 25%.</p>
<p><img src="notes_files/figure-html/unnamed-chunk-152-1.png" width="480" style="display: block; margin: auto;" /></p>
<p>You probably estimate the average is a little more than 25%… call it 27%. You can see that the SD is about 10% because you’d need to go out about 10 percentage points above and below the average to grab about 2/3rds of the data.</p>
<p><img src="notes_files/figure-html/unnamed-chunk-153-1.png" width="480" style="display: block; margin: auto;" /></p>
<p>Now you’re informed by the data and ready to answer the question.</p>
<ul>
<li>Q: What’s the portfolio share of a party in a coalition government with a seat share of 25%?</li>
<li>A: It’s about 28% give or take 10 percentage points or so.</li>
</ul>
<p>Notice that if we break the data into many small windows, we can visually create an average (and an SD) for each. Freedman, Pisani, and Purves (2008) refer to this as a “graph of averages.” Fox (2008) calls this “naive nonparametric regression.” It’s a conceptual tool to help us understand regression.</p>
<p><img src="notes_files/figure-html/unnamed-chunk-154-1.png" width="480" style="display: block; margin: auto;" /></p>
<p>For some datasets, these averages will fall roughly along a line. In that case, we can described the average value of <span class="math inline">\(y\)</span> for each value of <span class="math inline">\(x\)</span>–that is, the <em>conditional</em> average of <span class="math inline">\(y\)</span>–with a line.</p>
<p><img src="notes_files/figure-html/unnamed-chunk-155-1.png" width="480" style="display: block; margin: auto;" /></p>
<p>Here’s the takeaway: a “good” line is the conditional average.</p>
</div>
<div id="the-best-line" class="section level2">
<h2><span class="header-section-number">11.3</span> The Best Line</h2>
<p>So far, we have to results:</p>
<ol style="list-style-type: decimal">
<li>The average is the point that minimizes the RMS of the deviations.</li>
<li>We want a line that captures the conditional average.</li>
</ol>
<p>Just as the average minimizes the RMS of the deviations, perhaps we should choose the line that minimizes the RMS of the residuals… that’s exactly what we do.</p>
<p><strong>We want the pair of coefficients <span class="math inline">\((\hat{\alpha}, \hat{\beta})\)</span> that minimizes the RMS of the residuals or </strong></p>
<p><span class="math inline">\(\DeclareMathOperator*{\argmin}{arg\,min}\)</span></p>
<p><span class="math display">\[\begin{equation}
(\hat{\alpha}, \hat{\beta}) = \displaystyle \argmin_{( \alpha, \, \beta ) \, \in \, \mathbb{R}^2} \sqrt{\frac{r_i^2}{n}}
\end{equation}\]</span></p>
<p>Let’s have a quick look at two methods to find the coefficients that minimize the RMS of the residuals.</p>
<ol style="list-style-type: decimal">
<li>grid search</li>
<li>analytical optimization</li>
</ol>
<div id="grid-search" class="section level3">
<h3><span class="header-section-number">11.3.1</span> Grid Search</h3>
<p>Because we’re looking for the pair <span class="math inline">\((\hat{\alpha}, \hat{\beta})\)</span> that minimize the sum of the RMS residuals, we could simply check lots of different values. In some applications, this is a reasonable optimization routine. (It is not reasonable in the context of regression, where we have much faster and accurate tools.)</p>
<p>The figure below shows the result of a grid search.</p>
<ul>
<li>In the top-left panel, we see the line and the residuals for each intercept-slope pair. The size of the points indicates the residual <em>squared</em>. Notice that some lines make big errors and other lines make small errors.</li>
<li>In the top-right panel, we see a histogram of the squared residuals.</li>
<li>In the lower-left panel, each point in the grid shows a intercept-slope pair. The label, color, and size indicate the RMS of the residuals. As we move around the grid, the RMS changes–we’re looking for the smallest.</li>
<li>In the lower-right panel, the three lines show the evolution of the intercept, slope, and RMS of the residuals, respectively. Again, we’re looking for the pair that produces the smallest RMS of the residuals.</li>
</ul>
<p>For this search, the intercept -0.36 and the slope 0.89 produce the smallest RMS of the residuals (of the combinations we considered).</p>
<p><img src="img/scatterplots/grid-search.gif" /></p>
</div>
<div id="analytical-optimization" class="section level3">
<h3><span class="header-section-number">11.3.2</span> Analytical Optimization</h3>
<p>In the case of finding the line that minimizes the RMS of the residuals, we have an easy analytical solution. We don’t need a grid search.</p>
<div id="scalar-form" class="section level4">
<h4><span class="header-section-number">11.3.2.1</span> Scalar Form</h4>
<p>Remember that we simply need to minimize the function</p>
<p><span class="math inline">\(f(\alpha, \beta) = \displaystyle \sqrt{\frac{\sum_{i = 1}^n [y_i - (\alpha + \beta x_i)]^2}{n}}\)</span>.</p>
<p>This is equivalent to minimizing <span class="math inline">\(h(\alpha, \beta) = \sum_{i = 1}^n(y_i - \alpha - \beta x_i)^2\)</span>. We sometimes refer to this quantity as the SSR or “sum of squared residuals.”</p>
<p>If you’ve had calculus, you might recognize that we can us calculus to find <span class="math inline">\((\hat{\alpha}, \hat{\beta})\)</span> that minimize the RMS of the residuals. If not, you can skip the calculus part below. However, pay attention to the boxes, which I’m calling “Theorems.”</p>
<p>To minimize <span class="math inline">\(h(\alpha, \beta)\)</span>, remember that we need to solve for <span class="math inline">\(\frac{\partial h}{\partial \alpha} = 0\)</span> and <span class="math inline">\(\frac{\partial h}{\partial \beta} = 0\)</span> (i.e., the first-order conditions).</p>
<p>Using the chain rule, we have the partial derivatives</p>
<p><span class="math inline">\(\frac{\partial h}{\partial \alpha} = \sum_{i = 1}^n [2 \times (y_i - \alpha - \beta x_i) \times (-1)] = -2 \sum_{i = 1}^n(y_i - \alpha + \beta x_i)\)</span></p>
<p>and</p>
<p><span class="math inline">\(\frac{\partial h}{\partial \beta} = \sum_{i = 1}^n 2 \times (y_i - \alpha - \beta x_i) \times (-x_i) = -2 \sum_{i = 1}^n(y_i - \alpha - \beta x_i)x_i\)</span></p>
<p>and the two first-order conditions</p>
<p><span class="math inline">\(-2 \sum_{i = 1}^n(y_i - \hat{\alpha} + \hat{\beta} x_i) = 0\)</span></p>
<p>and</p>
<p><span class="math inline">\(-2 \sum_{i = 1}^n(y_i - \hat{\alpha} + \hat{\beta} x_i)x_i = 0\)</span></p>
<div id="the-1st-first-order-condition" class="section level5">
<h5><span class="header-section-number">11.3.2.1.1</span> The 1st First-Order Condition</h5>
<p><span class="math display">\[\begin{align}
-2 \sum_{i = 1}^n(y_i - \hat{\alpha} - \hat{\beta} x_i) &amp;= 0 \\
\sum_{i = 1}^n(y_i - \hat{\alpha} - \hat{\beta} x_i) &amp;= 0 \text{   (divide both sizes by $-2$)} \\
\sum_{i = 1}^n y_i - \sum_{i = 1}^n \hat{\alpha}  - \sum_{i = 1}^n \hat{\beta} x_i &amp;= 0 \text{   (distribute the sum)} \\
\sum_{i = 1}^n y_i -  n \hat{\alpha}  - \hat{\beta}\sum_{i = 1}^n  x_i &amp;= 0 \text{   (move constant $\beta$ in front and realize that $\sum_{i = 1}^n \hat{\alpha} = n\hat{\alpha}$)} \\
\sum_{i = 1}^n y_i &amp; = n \hat{\alpha}  + \hat{\beta}\sum_{i = 1}^n  x_i \text{   (rearrange)} \\
\frac{\sum_{i = 1}^n y_i}{n} &amp; = \hat{\alpha}  + \hat{\beta} \frac{\sum_{i = 1}^n  x_i}{n} \text{   (divide both sides by $n$)} \\
\overline{y} &amp; = \hat{\alpha}  + \hat{\beta} \overline{x} \text{   (recognize the average of $y$ and of $x$)} \\
\end{align}\]</span></p>

<div class="theorem">
<span id="thm:pt-of-avgs" class="theorem"><strong>Theorem 11.1  </strong></span>The 1st first-order condition implies that the regression line <span class="math inline">\(\hat{y} = \hat{\alpha} + \hat{\beta}x\)</span> equals <span class="math inline">\(\overline{y}\)</span> when <span class="math inline">\(x = \overline{x}\)</span>. Thus, the regression line must go through the point <span class="math inline">\((\overline{x}, \overline{y})\)</span> or “the point of averages”.
</div>

<p>The figure below shows a regression line that goes through the point of averages.</p>
<p><img src="notes_files/figure-html/unnamed-chunk-156-1.png" width="480" style="display: block; margin: auto;" /></p>

<div class="theorem">
<span id="thm:1st-order" class="theorem"><strong>Theorem 11.2  </strong></span>We can rearrange the identity <span class="math inline">\(\hat{y} = \hat{\alpha} + \hat{\beta}x\)</span> from Theorem <a href="regression.html#thm:pt-of-avgs">11.1</a> to obtain the identity <span class="math inline">\(\hat{\alpha} = \overline{y} - \hat{\beta}\overline{x}\)</span>
</div>

</div>
<div id="the-2nd-first-order-condition" class="section level5">
<h5><span class="header-section-number">11.3.2.1.2</span> The 2nd First-Order Condition</h5>
<p>Sometimes, when writing proofs, you obtain a result that’s not particularly interesting, but true and useful later. We refer to these results as “lemmas.” We’ll need the following Lemmas in the subsequent steps.</p>

<div class="lemma">
<span id="lem:ny" class="lemma"><strong>Lemma 11.1  </strong></span><span class="math inline">\(\sum_{i = i}^n y_i = n\overline{y}\)</span>.
</div>


<div class="exercise">
<span id="exr:unnamed-chunk-157" class="exercise"><strong>Exercise 11.1  </strong></span>Prove Lemma <a href="regression.html#lem:ny">11.1</a>.
</div>


<div class="lemma">
<span id="lem:sum-of-dev-prods" class="lemma"><strong>Lemma 11.2  </strong></span><span class="math inline">\(\sum_{i = 1}^n x_i y_i - n \overline{x} \overline{y} = \sum_{i = 1}^n (x_i - \overline{x})(y_i - \overline{y})\)</span>.
</div>


<div class="exercise">
<span id="exr:unnamed-chunk-158" class="exercise"><strong>Exercise 11.2  </strong></span>Prove Lemma <a href="regression.html#lem:sum-of-dev-prods">11.2</a>.
</div>


<div class="lemma">
<span id="lem:sum-of-squared-devs" class="lemma"><strong>Lemma 11.3  </strong></span><span class="math inline">\(\sum_{i = 1}^n x_i ^2 - n \overline{x}^2 = \sum_{i = 1}^n (x_i - \overline{x})^2\)</span>.
</div>


<div class="exercise">
<span id="exr:unnamed-chunk-159" class="exercise"><strong>Exercise 11.3  </strong></span>Prove Lemma <a href="regression.html#lem:sum-of-squared-devs">11.3</a>.
</div>

<p><span class="math display">\[\begin{align}
-2 \sum_{i = 1}^n(y_i - \hat{\alpha} - \hat{\beta} x_i)x_i &amp;= 0 \\
\sum_{i = 1}^n(y_i - \hat{\alpha} - \hat{\beta} x_i)x_i &amp;= 0 \text{   (divide both sides by -2)} \\
\sum_{i = 1}^n(y_i x_i - \hat{\alpha}x_i - \hat{\beta} x_i^2) &amp;= 0 \text{   (distribute the $x_i$)}
\end{align}\]</span></p>
<p>Now we can use use Theorem <a href="regression.html#thm:1st-order">11.2</a> and replace <span class="math inline">\(\hat{\alpha}\)</span> with <span class="math inline">\(\overline{y} - \hat{\beta}\overline{x}\)</span>.</p>
<p><span class="math display">\[\begin{align}
\sum_{i = 1}^n(y_i x_i - (\overline{y} - \hat{\beta}\overline{x})x_i - \hat{\beta} x_i^2) &amp;= 0 \text{   (use the identity $\hat{\alpha} = \overline{y} - \hat{\beta}\overline{x}$)} \\
\sum_{i = 1}^n( x_i y_i - \overline{y} x_i + \hat{\beta}\overline{x} x_i - \hat{\beta} x_i^2) &amp;= 0 \text{   (expand the middle term)} \\
\sum_{i = 1}^n x_i y_i  - \overline{y} \sum_{i = 1}^n x_i + \hat{\beta}\overline{x} \sum_{i = 1}^n x_i - \hat{\beta} \sum_{i = 1}^n x_i^2 &amp;= 0 \text{   (distribute the sum)}
\end{align}\]</span></p>
<p>Now we can use Lemma <a href="regression.html#lem:ny">11.1</a> to replace <span class="math inline">\(\sum_{i = i}^n y_i\)</span> with <span class="math inline">\(n\overline{y}\)</span>.</p>
<p><span class="math display">\[\begin{align}
\sum_{i = 1}^n  x_i y_i - n \overline{y} \overline{x} + \hat{\beta}n \overline{x}^2 - \hat{\beta} \sum_{i = 1}^n x_i^2 &amp;= 0 \text{   (use the identity $\sum_{i = i}^n y_i = n\overline{y}$)}\\
\sum_{i = 1}^n x_i y_i  - n \overline{x} \overline{y}  &amp;= \hat{\beta} \left(\sum_{i = 1}^n x_i^2 - n \overline{x}^2 \right) \text{   (rearrange)}\\
\hat{\beta} &amp;=\dfrac{\sum_{i = 1}^n x_i y_i - n \overline{x} \overline{y}}{\sum_{i = 1}^n x_i^2 - n \overline{x}^2} \text{   (rearrange)}\\
\end{align}\]</span></p>
<p>This final result is correct, but unfamiliar. In order to make sense of this solution, we need to connect this identity to previous results.</p>
<p>Now we can use Lemmas <a href="regression.html#lem:sum-of-dev-prods">11.2</a> and <a href="regression.html#lem:sum-of-squared-devs">11.3</a> to replace the numerator <span class="math inline">\(\sum_{i = 1}^n x_i y_i - n \overline{x} \overline{y}\)</span> with the more familiar expression <span class="math inline">\(\sum_{i = 1}^n (x_i - \overline{x})(y_i - \overline{y})\)</span> and replace the denominator <span class="math inline">\(\sum_{i = 1}^n x_i ^2 - n \overline{x}^2\)</span> with the more familiar expression <span class="math inline">\(\sum_{i = 1}^n (x_i - \overline{x})^2\)</span>.</p>
<p><span class="math display">\[\begin{align}
\hat{\beta} &amp;=\dfrac{\sum_{i = 1}^n (x_i - \overline{x})(y_i - \overline{y})}{\sum_{i = 1}^n (x_i - \overline{x})^2} \\
\end{align}\]</span></p>
<p>Denote the SD of <span class="math inline">\(x\)</span> as <span class="math inline">\(\text{SD}_x\)</span> and the the SD of <span class="math inline">\(y\)</span> as <span class="math inline">\(\text{SD}_y\)</span>. Multiply the top and bottom by <span class="math inline">\(\frac{1}{n \times \text{SD}_x^2 \times \text{SD}_y}\)</span> and rearrange strategically.</p>
<p><span class="math display">\[\begin{align}
\hat{\beta} &amp;=\frac{\frac{\sum_{i = 1}^n \left(\frac{x_i - \overline{x}}{\text{SD}_x} \right)\left(\frac{y_i - \overline{y}}{\text{SD}_y} \right)}{n} \times \frac{1}{\text{SD}_x}}{ \frac{1}{n \times \text{SD}_x^2 \times \text{SD}_y} \times \sum_{i = 1}^n (x_i - \overline{x})^2} \\
\end{align}\]</span></p>
<p>Now we recognize that the left term <span class="math inline">\(\dfrac{\sum_{i = 1}^n \left(\frac{x_i - \overline{x}}{\text{SD}_x} \right)\left(\frac{y_i - \overline{y}}{\text{SD}_y} \right)}{n}\)</span> in the numerator is simply the correlation coefficient <span class="math inline">\(r\)</span> between <span class="math inline">\(x\)</span> and <span class="math inline">\(y\)</span>.</p>
<p><span class="math display">\[\begin{align}
\hat{\beta} &amp;=\dfrac{r \times \frac{1}{\text{SD}_x}}{\frac{1}{\text{SD}_x^2 \times \text{SD}_y}\sum_{i = 1}^n \frac{(x_i - \overline{x})^2}{n}} \\
\end{align}\]</span></p>
<p>Now we recognize that <span class="math inline">\(\sum_{i = 1}^n \frac{(x_i - \overline{x})^2}{n}\)</span> is almost the <span class="math inline">\(\text{SD}_x\)</span>. Conveniently, it’s <span class="math inline">\(\text{SD}_x^2\)</span>, which allows us to cancel those two terms.</p>
<p><span class="math display">\[\begin{align}
\hat{\beta} &amp;=\dfrac{r \times \frac{1}{\text{SD}_x}}{\frac{1}{\text{SD}_y}} \\
            &amp; r \times \frac{\text{SD}_y}{\text{SD}_x}
\end{align}\]</span></p>
<p>This final result clearly connects <span class="math inline">\(\hat{\beta}\)</span> to previous results.</p>

<div class="theorem">
<span id="thm:2nd-order" class="theorem"><strong>Theorem 11.3  </strong></span><span class="math inline">\(\hat{\beta} = r \times \dfrac{\text{SD of } y}{\text{SD of }x} = \dfrac{\sum_{i = 1}^n (x_i - \overline{x})(y_i - \overline{y})}{\sum_{i = 1}^n (x_i - \overline{x})^2} = \dfrac{\sum_{i = 1}^n x_i y_i - n \overline{x} \overline{y}}{\sum_{i = 1}^n x_i^2 - n \overline{x}^2}\)</span>.
</div>

<p>In summary, we can obtain the smallest RMS of the residuals with results from Theorems <a href="regression.html#thm:1st-order">11.2</a> and <a href="regression.html#thm:2nd-order">11.3</a>.</p>
<p><span class="math display">\[\begin{align}
\hat{\beta} &amp;= r \times \dfrac{\text{SD of } y}{\text{SD of }x} \\
\hat{\alpha} &amp;= \overline{y} - \hat{\beta}\overline{x}
\end{align}\]</span></p>
</div>
</div>
</div>
</div>
<div id="the-rms-of-the-residuals" class="section level2">
<h2><span class="header-section-number">11.4</span> The RMS of the Residuals</h2>
<p>Just like the SD offers a give-or-take number around the average, the “RMS of the residuals.” offers a give-or-take number around the regression line. Sometimes the RMS of the residuals is called the “RMS error (of the regression),” the “standard error of the regression,” or denoted as <span class="math inline">\(\hat{\sigma}\)</span>.</p>
<p>Indeed, just like the SD is the RMS of the deviations from the average, the RMS of the residuals is the RMS of the deviations from the regression line. <strong>The RMS of the residuals tells us how far typical points fall from the regression line.</strong></p>
<p>We can compute the RMS of the regression by computing each residual and then taking the root-mean-square. But we can also use the much simpler formula <span class="math inline">\(\sqrt{1 - r^2} \times \text{ SD of }y\)</span>.</p>
<p>This formula makes sense because <span class="math inline">\(y\)</span> has an SD, but <span class="math inline">\(x\)</span> explains some of that variation. As <span class="math inline">\(r\)</span> increases, <span class="math inline">\(x\)</span> explains more and more of the variation. As <span class="math inline">\(x\)</span> explains more variation, then the RMS of the residuals shrinks away from SD of <span class="math inline">\(y\)</span> toward zero. It turns out that the SD of <span class="math inline">\(y\)</span> shrinks toward zero by a factor of <span class="math inline">\(\sqrt{1 - r^2}\)</span>.</p>
</div>
<div id="r2" class="section level2">
<h2><span class="header-section-number">11.5</span> <span class="math inline">\(R^2\)</span></h2>
<p>Some authors use the quantity <span class="math inline">\(R^2\)</span> to assess the fit of the regression model. I prefer the RMS of the residuals because it’s on the same scale as <span class="math inline">\(y\)</span>. Also, <span class="math inline">\(R^2\)</span> computes the what fraction of the <em>variance</em> of <span class="math inline">\(y\)</span>, which is the SD squared, is explained by <span class="math inline">\(x\)</span>. I have a hard time making sense of variances, because they are not on the original scale.</p>
<div id="adequacy-of-a-line" class="section level3">
<h3><span class="header-section-number">11.5.1</span> Adequacy of a Line</h3>
<p>In some cases, a line can describe the average value <span class="math inline">\(y\)</span> quite well. In other cases, a line describes the data poorly.</p>
<p>Remember, the regression line describes the average value of <span class="math inline">\(y\)</span> <em>for different values of <span class="math inline">\(x\)</span></em>. In the figure below, the left panel shows a dataset in which a line does not (and cannot) adequately describe the average values of <span class="math inline">\(y\)</span> for describe low, middle, and high values (at least ad the same time). The right panel shows a data in which a line can adequately describe how the average value of <span class="math inline">\(y\)</span> changes with <span class="math inline">\(x\)</span>. We can see that when <span class="math inline">\(x \approx -2\)</span>, then <span class="math inline">\(y \approx 0\)</span>. Similarly, when <span class="math inline">\(x \approx 0\)</span>, then <span class="math inline">\(y \approx 4\)</span>. A line can describe the average value of <span class="math inline">\(y\)</span> for varying values of <span class="math inline">\(x\)</span> when the average of <span class="math inline">\(y\)</span> changes <em>linearly</em> with <span class="math inline">\(x\)</span>.</p>
<p><img src="notes_files/figure-html/unnamed-chunk-160-1.png" width="672" style="display: block; margin: auto;" /></p>
<p>A line does a great job of describing the relationship between seat shares and portfolio shares in government coalitions.</p>
<p><img src="notes_files/figure-html/unnamed-chunk-161-1.png" width="480" style="display: block; margin: auto;" /></p>
<p>A line poorly describes the relationship between Polity IV’s DEMOC measure and GDP per capita.</p>
<p><img src="notes_files/figure-html/unnamed-chunk-162-1.png" width="480" style="display: block; margin: auto;" /></p>
<p>When we have variable that’s skewed heavily to the right, we can sometimes more easily describe the <em>log</em> of the variable. For this dataset, the line poorly describes the average logged GDP per capita for the various democracy scores.</p>
<p><img src="notes_files/figure-html/unnamed-chunk-163-1.png" width="480" style="display: block; margin: auto;" /></p>
</div>
</div>
<div id="fitting-regression-models" class="section level2">
<h2><span class="header-section-number">11.6</span> Fitting Regression Models</h2>
<p>To fit a regression model in R, we can use the following approach:</p>
<ol style="list-style-type: decimal">
<li>Use <code>lm()</code> to fit the model.</li>
<li>Use <code>coef()</code>, <code>arm::display()</code>, <code>texreg::screenreg()</code>, or <code>summary()</code> to quickly inspect the slope and intercept.</li>
</ol>
<div id="geom_smooth" class="section level3">
<h3><span class="header-section-number">11.6.1</span> <code>geom_smooth()</code></h3>
<p>In the context of ggplot, we can show the fitted line with <code>geom_smooth()</code>.</p>
<div class="sourceCode" id="cb219"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb219-1"><a href="regression.html#cb219-1"></a>gamson &lt;-<span class="st"> </span><span class="kw">read_rds</span>(<span class="st">&quot;data/gamson.rds&quot;</span>)</span>
<span id="cb219-2"><a href="regression.html#cb219-2"></a></span>
<span id="cb219-3"><a href="regression.html#cb219-3"></a><span class="kw">ggplot</span>(gamson, <span class="kw">aes</span>(<span class="dt">x =</span> seat_share, <span class="dt">y =</span> portfolio_share)) <span class="op">+</span><span class="st"> </span></span>
<span id="cb219-4"><a href="regression.html#cb219-4"></a><span class="st">  </span><span class="kw">geom_point</span>() <span class="op">+</span><span class="st"> </span></span>
<span id="cb219-5"><a href="regression.html#cb219-5"></a><span class="st">  </span><span class="kw">geom_smooth</span>()</span></code></pre></div>
<p><img src="notes_files/figure-html/unnamed-chunk-164-1.png" width="480" style="display: block; margin: auto;" /></p>
<p>By default, <code>geom_smooth()</code> fits a smoothed curve rather than a straight line. There’s nothing wrong with a smoothed curve—sometimes it’s preferable to a straight line. But we don’t understand how to fit a smoothed curve. To us the least-squares fit, we supply the argument <code>method = "lm"</code> to <code>geom_smooth()</code>.</p>
<p><code>geom_smooth()</code> also includes the uncertainty around the line by default. Notice the grey band around the line, especially in the top-right. We don’t have a clear since of how uncertainty enters the fit, nor do we understand a standard error, so we should not include the uncertainty in the plot (at least for now). To remove the grey band, we supply the argument <code>se = FALSE</code> to <code>geom_smooth()</code>.</p>
<p>The line <span class="math inline">\(y = x\)</span> is theoretically relevant–that’s the line that indicates a perfectly proportional portfolio distribution. To include it, we can use <code>geom_abline()</code>.</p>
<div class="sourceCode" id="cb220"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb220-1"><a href="regression.html#cb220-1"></a><span class="kw">ggplot</span>(gamson, <span class="kw">aes</span>(<span class="dt">x =</span> seat_share, <span class="dt">y =</span> portfolio_share)) <span class="op">+</span><span class="st"> </span></span>
<span id="cb220-2"><a href="regression.html#cb220-2"></a><span class="st">  </span><span class="kw">geom_point</span>() <span class="op">+</span><span class="st"> </span></span>
<span id="cb220-3"><a href="regression.html#cb220-3"></a><span class="st">  </span><span class="kw">geom_smooth</span>(<span class="dt">method =</span> <span class="st">&quot;lm&quot;</span>, <span class="dt">se =</span> <span class="ot">FALSE</span>) <span class="op">+</span><span class="st"> </span></span>
<span id="cb220-4"><a href="regression.html#cb220-4"></a><span class="st">  </span><span class="kw">geom_abline</span>(<span class="dt">intercept =</span> <span class="dv">0</span>, <span class="dt">slope =</span> <span class="dv">1</span>, <span class="dt">color =</span> <span class="st">&quot;red&quot;</span>)</span></code></pre></div>
<p><img src="notes_files/figure-html/unnamed-chunk-165-1.png" width="480" style="display: block; margin: auto;" /></p>
</div>
<div id="lm" class="section level3">
<h3><span class="header-section-number">11.6.2</span> <code>lm()</code></h3>
<p>The <code>lm()</code> function takes two key arguments.</p>
<ol style="list-style-type: decimal">
<li>The first argument is a formula, which is a special type of object in R. It has a left-hand side and a right-hand side, separated by a <code>~</code>. You put the name of the outcome variable <span class="math inline">\(y\)</span> on the LHS and the name of the explanatory variable <span class="math inline">\(x\)</span> on the RHS.</li>
<li>The second argument is the dataset.</li>
</ol>
<div class="sourceCode" id="cb221"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb221-1"><a href="regression.html#cb221-1"></a>fit &lt;-<span class="st"> </span><span class="kw">lm</span>(portfolio_share <span class="op">~</span><span class="st"> </span>seat_share, <span class="dt">data =</span> gamson)</span></code></pre></div>
</div>
<div id="quick-look-at-the-fit" class="section level3">
<h3><span class="header-section-number">11.6.3</span> Quick Look at the Fit</h3>
<p>We have several ways to look at the fit. Experiment with <code>coef()</code>, <code>arm::display()</code>, <code>texreg::screenreg()</code>, and <code>summary()</code> to see the differences. For now, we only understand the slope and intercept, so <code>coef()</code> works perfectly.</p>
<div class="sourceCode" id="cb222"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb222-1"><a href="regression.html#cb222-1"></a><span class="kw">coef</span>(fit)</span></code></pre></div>
<pre><code>(Intercept)  seat_share 
 0.06913558  0.79158398 </code></pre>
<p>The <code>coef()</code> function outputs a numeric vector with named entries. The intercept is named <code>(Intercept)</code> and the slope is named after its associated variable.</p>
<p>To find the RMS of the residuals, do the following:</p>
<div class="sourceCode" id="cb224"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb224-1"><a href="regression.html#cb224-1"></a>r &lt;-<span class="st"> </span><span class="kw">residuals</span>(fit)</span>
<span id="cb224-2"><a href="regression.html#cb224-2"></a><span class="kw">sqrt</span>(<span class="kw">mean</span>(r<span class="op">^</span><span class="dv">2</span>)) <span class="co"># rms of residuals</span></span></code></pre></div>
<pre><code>[1] 0.06880963</code></pre>
<p>We can get the fitted values with <code>predict()</code>.</p>
<div class="sourceCode" id="cb226"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb226-1"><a href="regression.html#cb226-1"></a><span class="kw">predict</span>(fit)</span></code></pre></div>
<pre><code>         1          2          3          4          5          6          7 
0.08832550 0.43374397 0.47692126 0.44280255 0.48705257 0.43744203 0.49241312 
         8          9         10         11         12         13         14 
0.46223510 0.46762002 0.44463055 0.48522460 0.46240661 0.46744856 0.46240661 
        15         16         17         18         19         20         21 
0.46744856 0.46240661 0.46744856 0.45232274 0.47753241 0.45232274 0.47753241 
        22         23         24         25         26         27         28 
0.76759204 0.16226311 0.76759204 0.16226311 0.47249048 0.45736466 0.52146931 
        29         30         31         32         33         34         35 
0.40838586 0.50890448 0.42095069 0.52238124 0.40747391 0.52238124 0.40747391 
        36         37         38         39         40         41         42 
0.23616707 0.57023001 0.19259364 0.23616707 0.57023001 0.19259364 0.40838586 
        43         44         45         46         47         48         49 
0.52146931 0.40838586 0.52146931 0.24044853 0.68940661 0.68243489 0.24742026 
        50         51         52         53         54         55         56 
0.20212169 0.72773347 0.43854144 0.49131373 0.42843610 0.50141904 0.37310383 
        57         58         59         60         61         62         63 
0.55675131 0.43400632 0.49584882 0.44637482 0.35979533 0.19282058 0.36720117 
        64         65         66         67         68         69         70 
0.16686200 0.13754408 0.29879267 0.16686200 0.23210876 0.13898123 0.45716695 
        71         72         73         74         75         76         77 
0.23986939 0.21368570 0.13108563 0.41330252 0.22056904 0.15861899 0.23210876 
        78         79         80         81         82         83         84 
0.13898123 0.45716695 0.23986939 0.35447399 0.32686061 0.17958917 0.11515791 
        85         86         87         88         89         90         91 
0.16118024 0.23021371 0.19339586 0.32686061 0.17958917 0.11515791 0.16118024 
        92         93         94         95         96         97         98 
0.23688848 0.20543482 0.36794544 0.20019253 0.12680064 0.25006907 0.21614404 
        99        100        101        102        103        104        105 
0.39142334 0.21048987 0.21305995 0.18607412 0.16808358 0.13210249 0.32550084 
       106        107        108        109        110        111        112 
0.18157649 0.25006907 0.21614404 0.39142334 0.21048987 0.25006907 0.21614404 
       113        114        115        116        117        118        119 
0.39142334 0.21048987 0.26528029 0.23725961 0.37035781 0.19522861 0.22056904 
       120        121        122        123        124        125        126 
0.23433572 0.40641920 0.20680237 0.28022466 0.23800684 0.29605632 0.16940289 
       127        128        129        130        131        132        133 
0.15357121 0.25817056 0.30542930 0.32315134 0.18137510 0.30001424 0.25383851 
       134        135        136        137        138        139        140 
0.32640037 0.18787319 0.27185831 0.26220485 0.34908601 0.18497714 0.16176775 
       141        142        143        144        145        146        147 
0.14492554 0.22913661 0.18703107 0.26282103 0.22071550 0.50450678 0.42534837 
       148        149        150        151        152        153        154 
0.51769984 0.41215531 0.66703414 0.18703107 0.14492554 0.66703414 0.18703107 
       155        156        157        158        159        160        161 
0.14492554 0.74637964 0.16588474 0.08672634 0.74637964 0.16588474 0.08672634 
       162        163        164        165        166        167        168 
0.28722504 0.34376677 0.36799892 0.66282357 0.26703158 0.24904104 0.32100321 
       169        170        171        172        173        174        175 
0.11711037 0.38097171 0.14929599 0.29959675 0.11923584 0.49999773 0.17091067 
       176        177        178        179        180        181        182 
0.28399410 0.11436895 0.49885262 0.18221902 0.34053581 0.47623594 0.46492757 
       183        184        185        186        187        188        189 
0.46492757 0.68674507 0.13002666 0.14742411 0.10393049 0.72531704 0.15246021 
       190        191        192        193        194        195        196 
0.12121348 0.77152698 0.15832814 0.78267606 0.14717908 0.30709641 0.31195277 
       197        198        199        200        201        202        203 
0.30709641 0.14198074 0.66018496 0.12190785 0.21689793 0.40403650 0.41012561 
       204        205        206        207        208        209        210 
0.09958112 0.15438309 0.42168980 0.40838586 0.16891508 0.68081410 0.24904104 
       211        212        213        214        215        216        217 
0.36597958 0.36048246 0.14059803 0.20106625 0.46862655 0.46122859 0.39053058 
       218        219        220        221        222        223        224 
0.38457882 0.14650845 0.14650845 0.60019827 0.19939624 0.19939624 0.70480151 
       225        226        227        228        229        230        231 
0.22505365 0.70480151 0.22505365 0.28869903 0.34647887 0.11535947 0.15002738 
       232        233        234        235        236        237        238 
0.23669716 0.44372444 0.16101587 0.16808358 0.29530245 0.44372444 0.16101587 
       239        240        241        242        243        244        245 
0.16808358 0.29530245 0.28265496 0.10559011 0.35556399 0.32431726 0.26703158 
       246        247        248        249        250        251        252 
0.10292271 0.33460583 0.30564544 0.12705636 0.26703158 0.34948824 0.27252869 
       253        254        255        256        257        258        259 
0.11311247 0.13510092 0.44293912 0.34032640 0.12777144 0.15708936 0.47602456 
       260        261        262        263        264        265        266 
0.32806493 0.12092145 0.14311540 0.27744717 0.35035620 0.27223937 0.11600569 
       267        268        269        270        271        272        273 
0.12121348 0.60140755 0.19196758 0.20561558 0.27744717 0.35035620 0.27223937 
       274        275        276        277        278        279        280 
0.11600569 0.12121348 0.29211699 0.37016047 0.28654245 0.11930640 0.27744717 
       281        282        283        284        285        286        287 
0.37862706 0.28339893 0.12865317 0.27744717 0.37862706 0.28339893 0.12865317 
       288        289        290        291        292        293        294 
0.47268822 0.34851817 0.10017809 0.14674186 0.43596719 0.17854150 0.31368999 
       295        296        297        298        299        300        301 
0.13992764 0.40752262 0.12351921 0.14768972 0.38939475 0.43248560 0.15348470 
       302        303        304        305        306        307        308 
0.41302042 0.44771923 0.15173565 0.12420229 0.34446915 0.47602456 0.15791135 
       309        310        311        312        313        314        315 
0.36505483 0.18923799 0.11826838 0.41306517 0.13464598 0.28204437 0.18303256 
       316        317        318        319        320        321        322 
0.13177892 0.35957286 0.13177892 0.33109862 0.14109777 0.17246589 0.28686730 
       323        324        325        326        327        328        329 
0.46769537 0.18279893 0.30864049 0.50755135 0.34879699 0.58105815 0.17251409 
       330        331        332        333        334        335        336 
0.75734107 0.17251409 0.75734107 0.20651793 0.72333723 0.20553848 0.72431668 
       337        338        339        340        341        342        343 
0.13663500 0.19390722 0.66844852 0.13663500 0.19390722 0.66844852 0.15774573 
       344        345        346        347        348        349        350 
0.23158751 0.60965748 0.15774573 0.23158751 0.60965748 0.14776976 0.15825431 
       351        352        353        354        355        356        357 
0.21329823 0.54880401 0.14776976 0.15825431 0.21329823 0.54880401 0.14776976 
       358        359        360        361        362        363        364 
0.15825431 0.21329823 0.54880401 0.42063315 0.50922197 0.17500092 0.75485425 
       365        366        367        368        369        370        371 
0.17500092 0.75485425 0.43171427 0.49814088 0.42894649 0.50090868 0.42894649 
       372        373        374        375        376        377        378 
0.50090868 0.42894649 0.50090868 0.15846794 0.68949918 0.15102358 0.23876072 
       379        380        381        382        383        384        385 
0.33299692 0.42723310 0.44293912 0.48691603 0.41144218 0.51841299 0.26103474 
       386        387        388        389        390        391        392 
0.26103474 0.47692126 0.28502213 0.64483304 0.26703158 0.66282357 0.26703158 
       393        394        395        396        397        398        399 
0.66282357 0.29176858 0.63808657 0.29176858 0.63808657 0.31650558 0.19282058 
       400        401        402        403        404        405        406 
0.48966457 0.38953863 0.54031652 0.34618997 0.34618997 0.30661079 0.35002021 
       407        408        409        410        411        412        413 
0.50323003 0.14574048 0.36865386 0.56120129 0.26220485 0.32012562 0.41666026 
       414        415        416        417        418        419        420 
0.27341531 0.32448525 0.40109015 0.23578484 0.27744717 0.33994063 0.21495368 
       421        422        423        424        425        426        427 
0.28902003 0.64083511 0.36597958 0.56387557 0.31910946 0.61074568 0.23454120 
       428        429        430        431        432        433        434 
0.15183839 0.18728244 0.12820901 0.43539086 0.27238012 0.12262099 0.60398963 
       435        436        437        438        439        440        441 
0.27516428 0.65469084 0.21755758 0.71229757 0.21640702 0.71344810 0.12635852 
       442        443        444        445        446        447        448 
0.80349661 0.12635852 0.80349661 0.32777193 0.60208320 0.38769987 0.50354144 
       449        450        451        452        453        454        455 
0.10774944 0.10822615 0.82162902 0.14031344 0.08854773 0.73130529 0.10795987 
       456        457        458        459        460        461        462 
0.09046569 0.79672924 0.11179580 0.14398450 0.08954893 0.76545729 0.09168070 
       463        464        465        466        467        468        469 
0.83817445 0.09168070 0.83817445 0.11994672 0.77246864 0.10657537 0.12011896 
       470        471        472        473        474        475        476 
0.77485284 0.10401895 0.13064380 0.79921135 0.12942178 0.08486242 0.78470654 
       477        478        479        480        481        482        483 
0.24754959 0.13680986 0.08144000 0.60232684 0.24754959 0.13680986 0.08144000 
       484        485        486        487        488        489        490 
0.60232684 0.24754959 0.13680986 0.08144000 0.60232684 0.26595018 0.08860076 
       491        492        493        494        495        496        497 
0.64443978 0.19025226 0.14483351 0.08860076 0.64443978 0.19025226 0.14483351 
       498        499        500        501        502        503        504 
0.08860076 0.64443978 0.19330562 0.14674186 0.65894327 0.14201157 0.73758428 
       505        506        507        508        509        510        511 
0.11939489 0.19928821 0.13101142 0.10114033 0.63668635 0.20477217 0.13361855 
       512        513        514        515        516        517        518 
0.66060002 0.11139096 0.81846419 0.10993888 0.10721866 0.78183320 0.12353998 
       519        520        521        522        523        524        525 
0.78183320 0.09361756 0.21348325 0.10638660 0.67912087 0.20546394 0.11311247 
       526        527        528        529        530        531        532 
0.10431710 0.64523281 0.20213885 0.11203986 0.10345901 0.63118166 0.08844251 
       533        534        535        536        537        538        539 
0.20213885 0.11203986 0.10345901 0.63118166 0.08844251 0.20816733 0.11398454 
       540        541        542        543        544        545        546 
0.65665682 0.08931761 0.22701982 0.11887993 0.13185672 0.55576506 0.10374035 
       547        548        549        550        551        552        553 
0.22701982 0.11887993 0.13185672 0.55576506 0.10374035 0.22701982 0.11887993 
       554        555        556        557        558        559        560 
0.13185672 0.55576506 0.10374035 0.26650665 0.10483035 0.11322912 0.56046355 
       561        562        563        564        565        566        567 
0.09223220 0.26650665 0.10483035 0.11322912 0.56046355 0.09223220 0.26650665 
       568        569        570        571        572        573        574 
0.10483035 0.11322912 0.56046355 0.09223220 0.26650665 0.10483035 0.11322912 
       575        576        577        578        579        580        581 
0.56046355 0.09223220 0.27814933 0.10693594 0.58944642 0.09359464 0.28915287 
       582        583        584        585        586        587        588 
0.10739946 0.56178300 0.10979095 0.25569832 0.09703282 0.22954467 0.09703282 
       589        590        591        592        593        594        595 
0.42831246 0.09877639 0.28915287 0.10739946 0.56178300 0.10979095 0.31203257 
       596        597        598        599        600        601        602 
0.30552643 0.32287620 0.12769111 0.54744252 0.12787503 0.25654239 0.13626638 
       603        604        605        606        607        608        609 
0.14829398 0.24328406 0.46492757 0.21162071 0.65118262 0.27867253 0.63090484 
       610        611        612        613        614        615        616 
0.29895028 0.43448204 0.49537313 0.43448204 0.49537313 0.38208740 0.54776775 
       617        618        619        620        621        622        623 
0.38208740 0.54776775 0.58861257 0.34124258 0.45572310 0.47413204 0.45572310 
       624        625        626        627        628        629        630 
0.47413204 0.58861257 0.34124258 0.50323003 0.42662511 0.55626420 0.37359097 
       631        632        633        634        635        636        637 
0.43051089 0.49934426 0.42534837 0.50450678 0.42326525 0.50658987 0.42326525 
       638        639        640        641        642        643        644 
0.50658987 0.51149133 0.41836381 0.44546239 0.48439273 0.35035620 0.40243410 
       645        646        647        648        649        650        651 
0.16287579 0.15246021 0.35035620 0.40243410 0.16287579 0.15246021 0.36231483 
       652        653        654        655        656        657        658 
0.36231483 0.18640729 0.15708936 0.38078281 0.37454987 0.16262976 0.15016387 
       659        660        661        662        663        664        665 
0.57287083 0.22334025 0.20277963 0.48176978 0.18703107 0.17018886 0.22913661 
       666        667        668        669        670        671        672 
0.49934426 0.18098984 0.18098984 0.20680237 0.39024985 0.44252425 0.16621664 
       673        674        675        676        677        678        679 
0.69737685 0.23247831 0.45572310 0.20720255 0.17958917 0.22561149 0.40700680 
       680        681        682        683        684        685        686 
0.19463060 0.16567022 0.14636329 0.22359099 0.44353340 0.20819763 0.17610639 
       687        688        689        690        691        692        693 
0.24028888 0.12626020 0.42004394 0.11809954 0.28947338 0.18338481 0.57287083 
       694        695        696        697        698        699        700 
0.35698431 0.38867406 0.19259364 0.41772303 0.27616525 0.65368992 0.50890448 
       701        702        703        704        705        706        707 
0.42095069 0.59685825 0.33299692 0.44571437 0.48414078 0.38749001 0.27563575 
       708        709        710        711        712        713        714 
0.33586498 0.43636526 0.18338481 0.37924064 0.43636526 0.18338481 0.37924064 
       715        716        717        718        719        720        721 
0.21889472 0.22959179 0.24028888 0.37935093 0.24724197 0.19776798 0.24724197 
       722        723        724        725        726        727        728 
0.37587437 0.20453811 0.21495368 0.27744717 0.37118737 0.17062072 0.35329394 
       729        730        731        732        733        734        735 
0.47507611 0.23151179 0.19091774 0.57656123 0.24788035 0.20957790 0.54153250 
       736        737        738        739        740        741        742 
0.18221902 0.54031652 0.27645521 0.63376191 0.29609322 0.57656123 0.35329394 
       743        744        745        746        747        748        749 
0.57624407 0.35361108 0.55353774 0.34087338 0.10457964 0.55353774 0.34087338 
       750        751        752        753        754        755        756 
0.10457964 0.55353774 0.34087338 0.10457964 0.52339684 0.40645831 0.69348352 
       757        758        759        760        761        762        763 
0.23637164 0.70356685 0.22628828 0.74039879 0.18945635 0.44733682 0.24064545 
       764        765        766        767        768        769        770 
0.31100846 0.35862915 0.24102239 0.39933920 0.56581574 0.36403943 0.21348325 
       771        772        773        774        775        776        777 
0.22279601 0.44164569 0.19020137 0.26703158 0.59812679 0.13383235 0.18289616 
       778        779        780        781        782        783        784 
0.64741848 0.13312591 0.10468576 0.19438621 0.69789373 0.10671077 0.77370146 
       785        786        787        788        789        790        791 
0.11124112 0.11404815 0.77370146 0.11124112 0.11404815 0.81383133 0.11602383 
       792        793        794        795        796        797        798 
0.24077354 0.68908162 0.24077354 0.68908162 0.24077354 0.68908162 0.20106625 
       799        800        801        802        803        804        805 
0.72878888 0.42685361 0.50300153 0.76722539 0.16262976 0.74095964 0.18889553 
       806        807        808        809        810        811        812 
0.74095964 0.18889553 0.73869674 0.19115842 0.70590793 0.22394721 0.21950817 
       813        814        815        816        817        818        819 
0.71034696 0.16594801 0.76390712 0.20449938 0.72535577 0.19377726 0.71858430 
       820        821        822        823        824        825        826 
0.08662915 0.22625904 0.70359611 0.17823954 0.75161562 0.75288057 0.17697456 </code></pre>
<p>It can be helpful to plot the fitted values against the residuals.</p>
<div class="sourceCode" id="cb228"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb228-1"><a href="regression.html#cb228-1"></a><span class="co"># create a data frame with the fitted values and the residuals.</span></span>
<span id="cb228-2"><a href="regression.html#cb228-2"></a>data &lt;-<span class="st"> </span><span class="kw">data.frame</span>(<span class="dt">residual =</span> <span class="kw">residuals</span>(fit),</span>
<span id="cb228-3"><a href="regression.html#cb228-3"></a>                   <span class="dt">fitted_value =</span> <span class="kw">predict</span>(fit))</span>
<span id="cb228-4"><a href="regression.html#cb228-4"></a></span>
<span id="cb228-5"><a href="regression.html#cb228-5"></a><span class="co"># plot fitted versus residuals</span></span>
<span id="cb228-6"><a href="regression.html#cb228-6"></a><span class="kw">ggplot</span>(data, <span class="kw">aes</span>(<span class="dt">x =</span> fitted_value, <span class="dt">y =</span> residual)) <span class="op">+</span><span class="st"> </span></span>
<span id="cb228-7"><a href="regression.html#cb228-7"></a><span class="st">  </span><span class="kw">geom_point</span>()</span></code></pre></div>
<p><img src="notes_files/figure-html/unnamed-chunk-170-1.png" width="480" style="display: block; margin: auto;" /></p>
</div>
</div>
<div id="standard-errors-and-p-values" class="section level2">
<h2><span class="header-section-number">11.7</span> Standard Errors and <em>p</em>-Values</h2>
<p>Almost all software reports <em>p</em>-values or standard errors by default. These are testing the null hypothesis that the coefficient (slope or intercept) equals zero. You can multiply the standard errors by plus-and-minus two to get a 95% confidence interval.</p>
<p>The two functions below summarize the fitted model, giving you some quantities you’ve seen and understand and others that you haven’t.</p>
<div class="sourceCode" id="cb229"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb229-1"><a href="regression.html#cb229-1"></a>arm<span class="op">::</span><span class="kw">display</span>(fit, <span class="dt">detail =</span> <span class="ot">TRUE</span>)</span></code></pre></div>
<pre><code>lm(formula = portfolio_share ~ seat_share, data = gamson)
            coef.est coef.se t value Pr(&gt;|t|)
(Intercept)  0.07     0.00   17.12    0.00   
seat_share   0.79     0.01   80.81    0.00   
---
n = 826, k = 2
residual sd = 0.07, R-Squared = 0.89</code></pre>
<div class="sourceCode" id="cb231"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb231-1"><a href="regression.html#cb231-1"></a>texreg<span class="op">::</span><span class="kw">screenreg</span>(fit)</span></code></pre></div>
<pre><code>
=======================
             Model 1   
-----------------------
(Intercept)    0.07 ***
              (0.00)   
seat_share     0.79 ***
              (0.01)   
-----------------------
R^2            0.89    
Adj. R^2       0.89    
Num. obs.    826       
=======================
*** p &lt; 0.001; ** p &lt; 0.01; * p &lt; 0.05</code></pre>
</div>
<div id="a-warning" class="section level2">
<h2><span class="header-section-number">11.8</span> A Warning</h2>
<p>When we simply describe a single set of measurements with a histogram or an average, then we intuitively remain in the world of description. Indeed, making a causal claim requires a comparing factual and counterfactual scenarios. In the case of a single histogram or average we only have one scenario and there is no comparison.</p>
<p>When we have descriptions of multiple sets of measurements, say the average life expectancy for democracies and the average life expectancy for autocracies, we an easily interpret one scenario as factual and the other as counterfactual. On its face, though, both scenarios are factual. We can comfortably say that democracies have healthier populations than autocracies without claiming that the regime type causes this difference. But it is… oh. so. tempting.</p>
<p>Regression models, by design, describe an outcome across a range of scenarios. Indeed, a regression model describes how the average value of <span class="math inline">\(y\)</span> changes as <span class="math inline">\(x\)</span> varies. The temptation to treat these neatly arranged scenarios as factual and counterfactual grows even stronger. But unless one makes a strong argument otherwise, <strong>statistical models describe the factual world</strong>.</p>
<blockquote>
<p>With few exceptions, statistical data analysis describes the outcomes of real social processes and not the processes themselves. It is therefore important to attend to the descriptive accuracy of statistical models, and to refrain from reifying them. —Fox (2008, p.3)</p>
</blockquote>
<p>Note that some methodologists claim that their statistical models can obtain estimates of the causal effects. These models might actually succeed on occasion. However, the researcher should carefully avoid seeing counterfactual worlds from regression models. Usually, credible causal inferences come from careful design in the data collection stage, not from complicated conditioning at the modeling stage.</p>
</div>
<div id="review-exercises" class="section level2">
<h2><span class="header-section-number">11.9</span> Review Exercises</h2>

<div class="exercise">
<p><span id="exr:unnamed-chunk-172" class="exercise"><strong>Exercise 11.4  </strong></span>Use <code>devtools::install_github("pos5737/pos5737data")</code> to get the latest version of the pos5737 data package. Load the data set <code>anscombe</code> into R with <code>data(anscombe, package = "pos5737data")</code>. Use <code>glimpse(anscombe)</code> to get a quick look at the data. Realize that this one data frame actually contains four different datasets stacked on top of each other and numbered I, II, III, and IV.</p>
<ol style="list-style-type: decimal">
<li>Fit a regression model on each “dataset” in the anscombe dataset. To only use a subset of the dataset, you can <code>filter()</code> the dataset before supplying the <code>data</code> to <code>lm()</code> or use can supply the <code>subset</code> argument to <code>lm()</code>. In this case, just supplying <code>subset = dataset == "I"</code> to <code>lm()</code>, for example, is probably easiest. Fit the regression to all four datasets and put the intercept, slope, RMS of the residuals, and number of observations for each regression in a little table. Interpret the results.</li>
<li>For each of the four regression fits, create a scatterplot of the fitted values versus the residuals. Describe any inadequacies.
</div></li>
</ol>

<div class="exercise">
<span id="exr:unnamed-chunk-173" class="exercise"><strong>Exercise 11.5  </strong></span>Use regression to test Clark and Golder’s (2006) theory using the parties dataset. First, create scatterplots between ENEG and ENEP faceted by the electoral system with with the least-squares fit included in each. Then fit three separate regression models. Fit the model <span class="math inline">\(\text{ENEP}_i = \alpha + \beta \text{ENEG}_i + r_i\)</span> for SMD systems, small-magnitude PR systems, and large-magnitude PR systems. (Hint: Use <code>filter()</code> to create three separate data sets–one for each electoral system.)
</div>


<div class="exercise">
<p><span id="exr:unnamed-chunk-174" class="exercise"><strong>Exercise 11.6  </strong></span>This continues Exercise <a href="#exr:description-review"><strong>??</strong></a>. Get the economic-model CSV dataset <a href="https://github.com/pos5737/economic-models/blob/master/data/economic-models.csv">from GitHub</a>.</p>
<ol style="list-style-type: decimal">
<li>In three separate regressions, use GDP, RDI, and unemployment to explain the incumbent’s margin of victory. Which measure of economic performance best describes incumbents’ vote shares? Hint: Maybe the RMS of the residuals will be helpful?</li>
<li>Using the best model of the three, which incumbents did much better than the model suggests? Which incumbents did much worse? Hint: Maybe make a scatterplot of your prefered variable and the margin of victory and use <code>geom_label()</code> with the aesthetic <code>label = incumbent_name</code>.
</div></li>
</ol>

</div>
</div>



            </section>

          </div>
        </div>
      </div>
<a href="correlation-coefficient.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="appendix-a-normal-table.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": ["notes.pdf", "notes.epub"],
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
