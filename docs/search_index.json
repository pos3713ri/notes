[
["index.html", "Concepts and Computation: An Introduction to Political Methodology Chapter 1 Overview", " Concepts and Computation: An Introduction to Political Methodology Rob Lytle and Carlisle Rainey 2020-11-09 Chapter 1 Overview While FPP is a fantastic textbook, it has three shortcomings: The examples are sometimes culturally and chronologically dated. The examples are usually not particularly interesting to political scientists. There is no computation. Rob and Carlisle are making an effort to write a more cohesive set of notes that cover the same content, but with modern, political examples and statistical computation. This is an in-progress effort, so these notes constantly evolve. Eventually, we hope they can replace the textbook, but they cannot at the moment. Instead, we hope these notes helpfully supplement the textbook. "],
["statistical-computing-with-r.html", "Chapter 2 Statistical Computing with R 2.1 R as a Calculator 2.2 Scripts 2.3 Object-Oriented Programming 2.4 Missing Values 2.5 Logical Operators 2.6 Packages", " Chapter 2 Statistical Computing with R R is a complex, powerful statistical programming language. It’s also free! I use R to do all my empirical and methodological work. I use R to wrangle data, fit statistical models, perform simulation studies, and draw graphics. R works by scripts. The user writes a program called a “script”\" and R executes the program. This might intimidate you a little. That’s okay. It’s easier than it sounds, and I’m here to help you. We’ll learn a lot about R this semester, but we’ll learn only some aspects of R. I have to include some features of R and exclude others. Just because I show you one way to tackle a problem doesn’t mean it’s the only (or the best) way. But in order to get you working with data ASAP, we have to exclude some important concepts and tools in R. Rather than use R directly, though, we’ll use RStudio to manage and run our R programs. RStudio is simply a way to organize our R code. I use RStudio for all my R programming. I even use RStudio to write documents and make presentations using RMarkdown. 2.1 R as a Calculator To get started with R, just open up RStudio and look around. If you want, you can use R like a calculator–just type directly into the console as you would a calculator. Fresh installation of RStudio 2 + 2 ## [1] 4 2*3 ## [1] 6 2^3 ## [1] 8 2/3 ## [1] 0.6666667 exp(3) ## [1] 20.08554 log(2) # this is the natural log ## [1] 0.6931472 sqrt(2) ## [1] 1.414214 Review Exercises Using R as a calculator, calculate the following (in the console): \\(32 + 17\\) \\(32 \\times 17\\) \\(\\frac{32}{17}\\) \\(32^2\\) \\(\\sqrt{32}\\) \\(\\log{32}\\) 2.2 Scripts Ultimately, we’ll want to write all of our code in scripts so that we can modify, reproduce, and check our work. From now on, almost everything we do will go in a script. The idea is not to do an analysis, but to write a script that can do an analysis for us. Scripts in RStudio To open a new R script, click File, New File, R Script. You can type lines of code directly into this script. In the upper-right corner of the script window, you’ll see a Run button. This runs the entire line that the cursor is currently on or all the highlighted lines. This is equivalent to Command + Enter (or Control + Enter on Windows). Unless the script takes a long time to run (and I don’t think any of ours will), I recommend hitting Command + A (or Control + A on Windows) to highlight the entire script and then Command + Enter (Control + Enter on Windows) to run the entire script. You need to get into the habit of running the entire script, because you want to entire script to work in one piece when you are done. It is much easier to do this if you’re running the entire script all along. ProTip: To run your code, press command + a (or control + a on Windows) and then press command + enter (or control + enter on Windows). To save this script, simply click File &gt; Save. I discuss where to save files a little later, but for now, just realize that R scripts will have a .R extension, such as my-script.R or important-analysis.R. 2.2.1 Importance Doing your work in a script is important. You might have done a statistical analysis before or at least manipulated data with Excel. Most likely, you went though several steps and perhaps ended with a graph. That’s fantastic, but there are several problems. If you want to re-do your analysis, you must go through the whole process again. You might forget what you did. (I shouldn’t say “might”–you will forget.) You cannot easily show others what you did. Instead, they must take your word for it. You cannot make small changes to your analysis without going through the whole process again. Scripting solves each of these problems. If you want to re-do your analysis, just open your script and click Run. If you forget what you did, just look at your script. If you want to show others exactly what you did, just show them your script. If you want to make a small change to your analysis, just make a small change to your script and click Run. Scripting might seem like a lot more work. At first, it will be more work. By the end of the semester, it will be less work. As part of the papers you’ll write for this class, you’ll write a script. 2.2.2 Comments You can also insert comments into R scripts. This is very important, especially when you are first learning to program. To insert a comment, simply type a pound or hash sign # (i.e., “hashtag” to me) anywhere in the code. Anything on the line after the hash will be ignored. I’ll always carefully comment my R code, and I’ll be even more careful about it for this class. Here’s an example of some commented code for the previous example. Comments in R ProTip: Use comments often to clearly describe the code to others and your future self. 2.3 Object-Oriented Programming But R is much more powerful than a simple calculator, partly because it allows object-oriented programming. You can store things as “objects” to reference later. Just about anything can be stored as an object, including variables, data sets, functions, numbers, and many others. 2.3.1 Scalars Let’s start with a single number, sometimes called a “scalar.” Let’s create an object b that holds or contains the number 2. To do this, we simply use the assignment operator &lt;-, which we read as “gets.” b &lt;- 2 # read as &quot;b gets 2&quot; We can be very creative with naming objects. Rather than b, we could have used myobject, myObject, my.object, or my_object. From a style perspective, I prefer my_object or important_variable. In general, you want to give objects descriptive names so that you code is easy to read, but short names so that the code is compact and easy to read and write. ProTip: Give objects short, descriptive names. We can now repeat some of the calculations from above, using b instead of two. Given that you know b equals two, check that the following calculations make sense. b + 3 ## [1] 5 b*3 ## [1] 6 b^3 ## [1] 8 3^b ## [1] 9 b/3 ## [1] 0.6666667 exp(b) ## [1] 7.389056 log(b) ## [1] 0.6931472 sqrt(b) ## [1] 1.414214 You probably realize that it would be easier to just use 2 rather than b. But we’ll be doing more complicated calculations. Rather than b holding scalars, it might hold thousands of survey responses. Rather than applying a simple function, we might apply many functions. 2.3.2 Functions So what is a function? In the above examples exp(), log(), and sqrt() are functions. Importantly, functions are followed immediately by parentheses (i.e., (), not [] or {}, which have different meanings). Arguments are supplied in the functions that tell the function what to do. You probably didn’t think about it at the time, but you can use many different bases when taking a logarithm. What base did we use when we ran log(b)? To see this, let’s open the help file. help(log) # or, equivalently ?log help file for log() The section “Usage” shows the typical function syntax. The log() function takes up to two arguments. The first argument x is a “numeric vector.” We’ll talk more specifically about numeric vectors below, but for now, we can consider a scalar as a numeric vector. If we provide the arguments in the same order that the appear in the functions in the “Usage” section, then we do not have to name the argument, but we still can. For example, log(b) and log(x = b) are equivalent. ProTip: If you need to know how to use a particular function such as exp(), then type help(exp) or ?exp into the console. You’ll also see from the help file that the default that default is base = exp(1), where exp(1) is just the number \\(e\\), the base of the natural log. This means that if you don’t specify base, it will use base = exp(1). log(b) # natural log ## [1] 0.6931472 log(b, base = exp(1)) # also natural log ## [1] 0.6931472 log(b, base = 10) # base-10 log ## [1] 0.30103 log(b, 10) # also a base-10 log ## [1] 0.30103 Notice that if we put the arguments in the proper order, we do not have to name the argument, so that log(b, base = 10) is equivalent to log(b, 10). However, the meaning of log(b, base = 10) is more clear, so I prefer that approach. ProTip: If arguments are supplied to functions in the correct order, then names are unnecessary. However, names should be included whenever there might be doubt about the meaning of the argument. In practice, this most often means leaving the first argument unnamed and naming the rest. Review Exercises Open a new script and give the object x the scalar 32. Repeat the first set of review exercises using x rather than the number 32. Add comments explaining what the code is doing. 2.3.3 Vectors But if we can only work with single numbers, we won’t get very far. When we do statistical computing, we’ll usually want to work with collections of numbers (or collections of character strings, like \"Republican\" or \"Male\"). In an actual problem, the collection might contain thousands or millions of numbers. Maybe these are survey respondents’ ages or hourly stock prices over the last few years. Maybe they are a respondent’s sex (i.e., \"Male\" or \"Female\") or party identification (i.e., \"Republican\", \"Democrat\", \"Independent\", or \"Other\"). We’ll call this collection of numbers or character strings a “vector” and we’ll refer to the number of elements in the vectors as the “length” of the vector. There are several types of vectors, classified by the sort of elements they contain. numeric: contain numbers, such as 1.1, 2.4, and 3.4. Sometimes numeric variables are subdivided into integer (whole numbers, e.g., 1, 2, 3, etc.) and double (fractions, e.g., 1.47, 3.35462, etc.). character: contain character strings, such as \"Republican\" or \"Argentina (2001)\". factor: contain character strings, such as \"Very Liberal\", \"Weak Republican\", or \"Female\". Similar to character, except the entire set of possible levels (and their ordering) is defined. logical: contain TRUE and/or FALSE. 2.3.3.1 Numeric Vectors Rather than the scalar 2, for example, we might want to work with the collection 2, 5, 9, 7, and 3. Let’s assign the collection above to the object a. We can create a vector using the “collect” function c(). a &lt;- c(2, 5, 9, 7, 3) ProTip: To create a vector, one tool we can use is the “collect” function c(). If we want to look at the object a, we need to enter a on a line by itself. This will print the object a for us to inspect. But since we only need to check this once, maybe we just type it in the console instead of including it in the script. a ## [1] 2 5 9 7 3 We can now apply functions to the vector a just like we did for the scalar b. In each case, the function is applied to each element of the vector. a + 3 ## [1] 5 8 12 10 6 a*3 ## [1] 6 15 27 21 9 a^3 ## [1] 8 125 729 343 27 3^a ## [1] 9 243 19683 2187 27 a/3 ## [1] 0.6666667 1.6666667 3.0000000 2.3333333 1.0000000 exp(a) ## [1] 7.389056 148.413159 8103.083928 1096.633158 20.085537 log(a) ## [1] 0.6931472 1.6094379 2.1972246 1.9459101 1.0986123 log(a, base = 10) ## [1] 0.3010300 0.6989700 0.9542425 0.8450980 0.4771213 sqrt(a) ## [1] 1.414214 2.236068 3.000000 2.645751 1.732051 # sum() adds all the elements together sum(a) ## [1] 26 # mean() finds the average--now we&#39;re doing statistics! mean(a) ## [1] 5.2 So far, we’ve only used numeric vectors–vectors that contain numbers. But we can create and work with other types of vectors as well. For now, let’s just illustrate two types: vectors of character strings, factors (and ordered factors), and logical vectors. Review Exercises In a script (perhaps the script you began in the exercises above), create a numeric vector assigning the collection 2, 6, 4, 3, 5, and 17 to the object my_vector. Create another numeric vector assigning the collection 64, 13, and 67 to the object myOtherVector. Use the sum() function to add the elements of my_vector together. Use the sqrt() function to take the square root of the elements of myOtherVector. Add 3 to the elements of my_vector. Add comments to the script explaining what this code is doing. 2.3.3.2 Character Vectors Character strings are simply letters (or numbers, I suppose) surrounded by quotes, such as \"Republican\" or \"Male\". If we put c() (i.e., “combine”) together multiple character strings, then we have a character vector. # create character vector x &lt;- c(&quot;Republican&quot;, &quot;Democrat&quot;, &quot;Republican&quot;, &quot;Independent&quot;) # print x x ## [1] &quot;Republican&quot; &quot;Democrat&quot; &quot;Republican&quot; &quot;Independent&quot; # for fun, try to multiply x times 3 x*3 # doesn&#39;t work ## Error in x * 3: non-numeric argument to binary operator A comment about escapes: Inside a string (i.e., text surrounded by quotes, e.g., \"Male\"), a backslash \\ is considered an “escape.” For example \\n represents new line and \\t represents tab. # new line cat(&quot;Repub\\nlican&quot;) ## Repub ## lican # tab cat(&quot;Demo\\tcrat&quot;) ## Demo crat This is important because filenames are sometimes represented with back-slashes, such as data\\nominate.csv. If entered this way, R will not read the data properly, because it treats \\n as an escape and tries to put a new line there. The solution is to use two back-slashes to represent one (e.g., data\\\\nominate.csv). Or you can use a forward-slash / instead (e.g., data/nominate.csv). Note that Mac uses forward-slashes in file names by default, so it might not come up. For Windows users, though, you need to use either double back-slashes or switch to forward slashes. Review Exercises Create a character vector containing the elements Male, Female, Male, Male, and Female. Assign this vector to the object sex. Create a character vector containing the elements Liberal, Moderate, Moderate, Conservative, and Liberal. Assign this vector to the object ideology. 2.3.3.3 Factor Vectors A factor vector is much like a character vector, but can only take on predefined values. While we might use a character vector to encode a variable that can have a variety of values (e.g., respondent’s name), we might use a factor to encode a variable that can take on just a few values, such as party identification (e.g., “Republican,” “Independent,” “Democrat,” “Other”). We refer to the possible values of a factor as the “levels.” Creating a factor is more tricky than creating a numeric or character vector. We might take several approaches, but I suggest the following two-step approach: Create a character vector containing the information using c(). Add the levels using the factor function. Factor vectors have two particular advantages over character vectors. It allows us to easily see when one category has zero observations. It allows us to control the order in which the categories appear. This will be useful, even for categorical variables that have no natural ordering (e.g., race, eye color). # create a character vector pid &lt;- c(&quot;Republican&quot;, &quot;Republican&quot;, &quot;Democrat&quot;, &quot;Other&quot;) # check type class(pid) ## [1] &quot;character&quot; # table pid table(pid) # two problems: 1-weird order; 2-&quot;Indpendents&quot; missing ## pid ## Democrat Other Republican ## 1 1 2 We can fix these two problems by using a factor vector instead. # create a factor vector in two steps ## step 1: create a character vector pid &lt;- c(&quot;Republican&quot;, &quot;Republican&quot;, &quot;Democrat&quot;, &quot;Other&quot;) ## step 2: add levels using factor() pid &lt;- factor(pid, levels = c(&quot;Republican&quot;, &quot;Independent&quot;, &quot;Democrat&quot;, &quot;Other&quot;)) # check type class(pid) ## [1] &quot;factor&quot; # table pid table(pid) # two problems fixed ## pid ## Republican Independent Democrat Other ## 2 0 1 1 You can see that by creating a factor variable that contains the level information, we can see that we have no Independents in our sample of four respondents. We can also control the ordering of the categories. Review Exercises Change the character vector sex created above to a factor vector. Be sure to explicitly add the levels. The order does not matter. Assign this new factor variable to the object sex_factor. Change the character vector ideology created above to a factor vector. Be sure to explicitly add the levels. Use an intuitive ordering. Assign this new factor variable to the object ideology_factor. 2.3.3.4 Logical Vectors Logical vectors contain elements that are true or false. R has a special way to represent true and false elements. R uses TRUE (without quotes) to represent true elements and FALSE (again, without quotes) to represent false elements. To create a logical vector, we can c() together a series of TRUE’s and/or FALSE’s. # create logical vector x &lt;- c(TRUE, TRUE, FALSE, TRUE, FALSE) # print x x ## [1] TRUE TRUE FALSE TRUE FALSE Review Excercises Create the logical vector containing. True, False, False, True, and True. Assign it to the object logic1. Multiply logic1 times 3. What do you get? Does that make sense? 2.3.4 More Information You should treat my notes as incomplete. I gloss over some potentially relevant distinction to get you up and going quickly. For a complete description of vectors, see chapter 4 of Advanced R Programming. You can find a similar description in chapter 20 of R4DS. I don’t mention dates and times at all, but R makes it easy to work with vectors of dates and date-times. We’ll discuss those when we get talk about data wrangling, because it’s a more advanced topic. For reference, you can see the lubridate page on the tidyverse website. 2.4 Missing Values Missing data are extremely common in statistics. For example, a survey respondent might refuse to reveal her age or income. Or we might not know the GDP or child mortality rate for a particular country in a particular year. In R, we can represent these values with NA (“not available”). Notice that NA does not have quotes. Different functions handle NA’s differently. Some function will drop missing values (e.g., compute the statistic using the non-missing data) and other functions will fail. Most of the simple functions that we’ll use at first will fail by default (e.g., sum(), mean()), but many of the more advanced functions we’ll use later (e.g., lm()) will drop missing values by default. x &lt;- c(1, 4, 3, NA, 2) log(x) # doesn&#39;t fail: computes the log for observed data, returns NA for missing data ## [1] 0.0000000 1.3862944 1.0986123 NA 0.6931472 sum(x) # fails: can&#39;t know the sum without know the value of the missing data ## [1] NA sum(x, na.rm = TRUE) # doesn&#39;t fail: setting na.rm = TRUE tell the function to drop the missing data ## [1] 10 Review Exercises Create the object x using x &lt;- c(1, 4, 3, NA, 2). Using mean() to find the mean of x with and without using the argument na.rm = TRUE. In a comment, explain why the results are different. Is na.rm = TRUE a reasonable choice? Repeat using sum() rather than mean(). 2.5 Logical Operators Occasionally, we’d like R to test whether a certain condition holds. We’ll use this most often to choose a subset of a data set. For example, we might need only the data from the 100th Congress (from a data set that contains all Congresses) or only data before 1990 (for a data set that contains all years from 1945 to 2000). The logical operators in R are &lt;, &lt;=, ==, &gt;=, &gt;, and !=. Notice that we must use ==, not =, to test for (exact) equality. We use != to test for inequality. We can use &amp; to represent “and” conditions and | to represent “or.” Logical operators return either TRUE or FALSE. Operator Syntax “less than” &lt; “less than or equal to” &lt;= “exactly equal to” == “greater than or equal to” &gt;= “greater than” &gt; “not equal to” != “or” | “and” &amp; Try running some of the following. Make sure you can anticipate the result. # less than 2 &lt; 1 2 &lt; 2 2 &lt; 3 # less than or equal to 2 &lt;= 1 2 &lt;= 2 2 &lt;= 3 # equal to 2 == 1 2 == 2 2 == 3 # greater than or equal to 2 &gt;= 1 2 &gt;= 2 2 &gt;= 3 # greater than 2 &gt; 1 2 &gt; 2 2 &gt; 3 # not equal to 2 != 1 2 != 2 2 != 3 # or (1 &gt; 2) | (3 &gt; 4) (1 &lt; 2) | (2 &gt; 4) (1 &lt; 2) | (3 &lt; 4) # and (1 &gt; 2) &amp; (3 &gt; 4) (1 &lt; 2) &amp; (2 &gt; 4) (1 &lt; 2) &amp; (3 &lt; 4) Review Exercises Use logical operators to test the whether each element of my_vector (created above) is… greater than 3. less than 3. equal to 3 greater than 3 or less than 3. less than or equal to 3 greater than or equal to 3 greater than 2 or less than 1 greater than 2 and less than 1 greater than 1 and less than 2 2.6 Packages Because R is an open-source program, it is easy to write extensions, and thousands of people have. These extensions come in the form of “packages” and these packages contain mostly functions that do stuff with data. For example, I’ve written an R package that does plotting. (I’ve since deferred to ggplot2). I’ve written another package that estimates statistical model. Hadley Wickam has written many R packages, some of which we’ll use. He’s written haven, which we can use to read proprietary data formats, such as Stata (.dta) files. He’s also written readr, which we can use to quickly read in better formats, such as comma-separated values (.csv), which I’ll encourage you to use throughout the course. He’s also written a package that is helpful for creating plots called ggplot2. We see that later. In order to use a package, it must be installed once and then loaded in each session (i.e., after each restart). Some packages come pre-installed in R (e.g., stats, MASS). Some of these pre-installed packages are automatically loaded in each session (e.g., stats), while others must be loaded manually in each session (e.g., MASS) if you want to use them. Other packages that do not come pre-installed with R need to be installed manually (but just once) and loaded in each session. 2.6.1 Installing Packages If you click the “Packages” tab in RStudio (positioned in the upper-right panel by default), it will show you a list packages that are currently installed. I’ve attached a screenshot of my installed packages below, but your’s might look slightly different–I’ve probably installed a lot more than you. List of packages in RStudio If you want to use a function from a package and it is not installed, you must first install it. Most R packages are available on CRAN, and can be installed with the install.packages() function. install.packages(&quot;ggrepel&quot;) install.packages(&quot;ggthemes&quot;) If you want to go ahead and install these packages, feel free–we’ll use them later. RStudio might ask you to choose a mirror. If so, just choose something close to you–it doesn’t really matter. If you look at your list of installed packages again, you should see ggrepel and ggthemes. You only need to install a package once. Once you’ve installed it, you have it on your hard drive. 2.6.2 Loading Packages In order to use function from a package, though, the package must be installed and loaded. In the packages list, the check box (beside the package name) indicates whether the package is loaded. In the screenshot above, you can see that I only had one package loaded at the time I took the screenshot–blme, a package useful for estimating Bayesian linear mixed-effects models. Let’s see how to access functions that are in packages. First, let’s create some data to plot. x &lt;- c(1, 3, 2, 5, 4) y &lt;- c(2, 1, 4, 3, 5) Now let’s try to use the qplot() function in the ggplot2 package to create a scatterplot of x and y. I’m assuming that you’ve already installed the ggplot2 package (it’s included in the tidyverse package). Now, let’s try to use qplot(). qplot(x, y) ## Error in qplot(x, y): could not find function &quot;qplot&quot; You’ll notice that the qplot() function cannot be found. That’s because while ggplot2 is installed (i.e., present in the library), it is not loaded (i.e., off the shelf). If you want to use a function from the ggplot2 package, you need to load the package using the library() function. You need to load the package each time you start a new session (e.g., restart RStudio), so be sure to include this in your script. Notice that you do not need to use quotes around the name of the package when using the library() function. library(ggplot2) ## Warning: package &#39;ggplot2&#39; was built under R version 3.6.2 qplot(x, y) When we use library(), it loads the entire set of functions in the package into our workspace, making them all accessible to us. There are literally hundreds of functions in the ggplot2 package. If we prefer to avoid loading all the functions, we can use the syntax package::function() to tell R where to find the function in the library without loading all the functions into the workspace. ggplot2::qplot(x, y) ggplot2::qplot(x, y) will work whether or not ggplot2 is loaded. I don’t have strong feelings about which approach is better–it depends on the context. If you only need to use one function from package one time, then perhaps it makes more sense to use the package::function() approach. If you’ll be using many functions many times, then it makes sense to use library(). Use whichever makes most sense to you. However, I’ll tend to use package::function() a lot, because it makes it clear where the function is coming from. Review Exercises Install and load the GGally package. Write a simple example of a function in the package. Use example code from the vignette. Install and load the ggdag package. Write a simple example of a function in the package. Use example code from the vignette. Install and load the texreg package. Write a simple example of a function in the package. Perhaps try the code below. If you are feeling ambitious, you might try compiling a LaTeX document with the LaTeX code output by texreg(). If you do this, be sure to start with the LaTeX template from 50-legs or the very minimal template from Homework 1. library(texreg) fit1 &lt;- lm(Fertility ~ . , data = swiss) fit2 &lt;- update(fit1, . ~ . -Examination) screenreg(list(fit1, fit2)) texreg(list(fit1, fit2)) "],
["loading-data-into-r.html", "Chapter 3 Loading Data into R 3.1 The Terms 3.2 Data Frames 3.3 How We’ll Always Use R", " Chapter 3 Loading Data into R Rather than manually entering data using c() or something else, we’ll want to load data in stored in a data file. 3.1 The Terms There are three important ingredients in loading a data set into R. The file type, usually indicated by the extension (.rds, .csv, .dta, .xlsx). The current working directory. The file path (relative to the working directory). 3.1.1 File Types In political science, our data sets are usually one of four types: R data or .rds files. This is the easiest format because it stores factors as factors and all the related information. Read with readr::read_rds(). Note that readr is part of the tidyverse, so library(tidyverse) loads readr. (I explain a “data frame” below, but realize that an .rds file can contain any R object, not just a data frame.) comma-separated value or .csv files. This is a common, sharable, robust data file type. You can open these files with any statistical software. GitHub renders these nicely. However, the csv format does not distinguish between factors as character strings, and treats variables as numbers or characters. Read with readr::read_csv(). Stata or .dta files. Another common data format because many political scientists use Stata. Read these files into R with haven::read_dta(). Excel or .xlsx files. Another common data format because data are easy to enter into spreadsheets like Excel. Read these files into R with readxl::read_excel(). 3.1.2 Comma-Separated Value Format (.csv) Data can be stored in a wide range of formats. One popular format, for example, is Stata’s proprietary .dta format. I typically use (and encourage you to use) the comma-separated values .csv format. The .csv format is excellent because it is open and simple. This means that anyone can use it without acess to proprietary software. It will also be useble by anyone into the foreseeable future. We can see why .csv files are easy to work with if we open it up the file nominate.csv with a text editor. You’ll see that you–with your eyes–can read the file directly. You don’t really need software at all! .csv in a Text Editor I tried the same thing for a similar .dta file. With your eyes, it looks like nonsense. You’ll definitely need Stata (or other speciallized software) to work with this file. .dta in a Text Editor Also, .csv files are easy to support, so they work in almost all data analysis software. For example, we can open up nominate.csv in Excel. You can see that we have six variables in the columns and many cases in the rows (we don’t know how many because they overflow the screen). In this case, each row represents a particular Congressperson from a particular Congress (with Presidents as well. The second row, for example, is for Rep. Callahan (R) from the 1st Congressional District of Alabama. During the 100th Congress, Rep. Calahan has a ideology score of 0.358, which means he’s conservative, but not as conservative as Pres. Reagan, who has a score of 0.747. We’ll work with these data a lot thoughout the semester, so we’ll have plenty of time for closer examination. .csv in Excel 3.1.3 The Working Directory Any time you work in R (or RStudio), you are working from a “working directory.” That is, whenever R needs to locate a file to load it or save it, it looks in the working directory. For example, I’m writing a paper now. I named the project directory wilks/ (which I placed in a folder called projects/ in my Dropbox folder.) If I open this project by double-clicking wilks.Rproj in the wilks/ project directory, I have opened that project in RStudio. If I run getwd() (“get the working directory”), R prints the following: &gt; getwd() [1] &quot;/Users/carlislerainey/Dropbox/projects/wilks&quot; Notice that the working directory is the project directory. This happens because with use .Rproj files to manage the way that RStudio interacts with our projects. If I did not have a project open in RStudio, then I get the following: &gt; getwd() [1] &quot;/Users/carlislerainey&quot; Of course, any files we want to access or save are in the project directory, which is three levels deeper in the file system than carlislerainey/. If I didn’t use an .Rproj file to manage the project, then I could set the working directory whereever I wanted with setwd(). See below: &gt; setwd(&quot;/Users/carlislerainey/Dropbox/projects/wilks&quot;) &gt; getwd() [1] &quot;/Users/carlislerainey/Dropbox/projects/wilks&quot; But we use an .Rproj file to manage our projects, so our working directory is always the project directory. I’ll say it louder for the people in the back. We use an .Rproj file to manage our projects, so our working directory is always the project directory. We don’t (or rarely) need to change or choose the working directory, just realize that it’s the project directory. 3.1.4 The Path Suppose we want to read the Stata data set ddrevisited_data_v1.dta. We know the following: The filetype is dta. We’ll use haven::read_dta(). The working directory is the project directory, as always. Now we just need the file path; that’s the only argument that haven::read_dta() requires. 3.1.4.1 Organizing Your Project Directories Any data set that you want to read into R should be in the project folder. You could put it in the main project directory, but you probably want to put it in a sub-directory (or perhaps even deeper). The diagram below outlines how I organized the files in cool-project/. I have an R/ subdirectory for R scripts. I have a data/ subdirectory for data sets. I put the raw data sets in data/raw/ (i.e., a subsubdirectory) to help protect them. I put the manuscript in doc/ and the subsubdirectories doc/fig/ for figures and doc/tab/ for tables. I have the output subdirectory for intermediate files I create along the way. cool-project/ ├── R/ │ ├── clean-data.R │ ├── fit-models.R │ ├── make-table.R │ └── plot-data.R ├── data/ │ ├── raw/ │ │ ├── ddrevisited_data_v1.dta │ │ └── p4v2018.xls │ ├── clean-data.csv │ └── clean-data.rds ├── doc/ │ ├── fig/ │ │ └── plot1.png │ ├── tab/ │ │ └── table.tex │ ├── cool-project.pdf │ └── cool-project.tex ├── output/ │ ├── model-fit.csv │ ├── model-fit.rds │ ├── model-pred.csv │ └── model-pred.rds ├── README.md └── cool-project.Rproj Below is a screenshot from Finder on macOS that shows the same organization. 3.1.4.2 An Aside on Formats If you’re reading carefully, you’ll notice that I keep both csv and rds versions of the data sets I create. I do this for the following reasons: rds data sets are the most convenient to compute with because preserve factors. I want this. csv data sets are the easiest to share and inspect, both locally (Finder even shows me the file!) and on GitHub (which renders a nice table). I want this. It’s easy to create both and keep them in sync. To create both, I just run write_rds() and write_csv() on consecutive lines. 3.1.5 Determining the Path Because we know the working directory is the project directory, we only need to provide the reading function the path relative to thet project directory. The path is just directions from the working/project directory to the file you want to read. For example, if we wanted to find ddrevisited_data_v1.dta, we would do the follwing (from the working/project directory): Go into data/. Go into raw/. Find ddrevisited_data_v1.dta. That’s it. To create the path, we just put these pieces together: data/raw/ddrevisited_data_v1.dta. Now we just give the path to haven::read_dta(). We get: read_dta(\"data/raw/ddrevisited_data_v1.dta\"). We need to assign the data set to an object, we want something the following: # load packages library(haven) # load raw data set raw_df &lt;- read_dta(&quot;data/raw/ddrevisited_data_v1.dta&quot;) This loads the data set into R. Review Exercises There are 8 total data sets in cool-project/ (all the files in data/ and output/). Answer the following questions for each data set: What function should you use to read that file? What’s the working directory? What’s the path (relative to the working directory)? Given the above, what’s the code to read the data (and store it as an object)? When you think you know, check your work by trying your commands. I put cool-project on Github. Clone it (or click Code &gt; Download ZIP to download the directory) and see if you can load the data sets. Solution for ddrevisited_data_v1.dta The extension is .dta, so I know this is a Stata data set and that I need to use haven::read_dta(). The working directory is cool-project/ because the working directory is always the project directory since I use an .Rproj file to manage my projects. The path is data/raw/ddrevisited_data_v1.dta. I can see this in the directory tree above, or by inspecting the directory on my computer. Below: # load packages library(haven) # load raw data sets raw_df &lt;- read_dta(&quot;data/raw/ddrevisited_data_v1.dta&quot;) 3.1.6 rio Loading data into R is a little bit tricky and tedious. One reason is finding a function to handle the data format. If the data is .Rds, .csv, or .dta. formats, we already know what to do. But what if the data is in a format such as .tsv (tab separated), .xlsx (Microsoft Excel), .ods (OpenDocument spreadsheet), or any number of other formats? The R package rio contains the fuction import() that automatically adapts to the different formats according to the filename extension. It’s just one function—you simply need to point it to the data set. # load packages library(rio) # for generic import() function # read same data stored in three different formats nominate &lt;- import(&quot;data/nominate.csv&quot;) nominate &lt;- import(&quot;data/nominate.rds&quot;) nominate &lt;- import(&quot;data/nominate.dta&quot;) 3.2 Data Frames Almost the statistical computation we do in this class revolves around data sets. In R, it usually makes sense to store data sets as specific objects known as data frames. Data frames are simply a set of vectors that all contain the same number of elements. These might be numeric, character, factor, or logical vectors, or some mixture of types. When you read a data set into R using readr::read_csv, readr::read_rds(), haven::read_dta(), or some other method, it creates a data frame. A data frame is a special R object that holds a set of vectors that all have the name number of elements. If you think of the data set as an Excel spreadsheet, then you can think of the columns of the spreadsheet as the vectors held by the data frame. These vectors or variables can be numeric, character, factor, or logical. As a reminder, here are the variable types: numeric: numbers, such as 1.1, 2.4, and 3.4. Sometimes numeric variables are subdivided into integer (whole numbers, e.g., 1, 2, 3, etc.) and double (fractions, e.g., 1.47, 3.35462, etc.). character: text strings, such as \"Republican\" or \"Argentina (2001)\". factor: cateogories, such as \"Very Liberal\", \"Weak Republican\", or \"Female\". Similar to character, except the entire set of possible levels is defined. A factor variable may be ordered or unordered. logical: true or false, such as TRUE or FALSE. For the .csv files we will usually use, R cannot distinguish between character and factor variables. By default, readr::read_csv() will load these as character variables–there’s no way for R to know the entire set of levels from the .csv file anyway. Sometimes, though, it will be useful to work with factor variables. This is straightforward to change. 3.2.1 Working with Variables in Data Frames A data frame holds the variables, but it also hides the vectors. For example, the data frame nominate, which we loaded above, has a numeric variable ideology, but if we try to sum it, we get an error. sum(ideology) # fails because the variable ideology is hidden in a data frame ## Error in eval(expr, envir, enclos): object &#39;ideology&#39; not found We’ve loaded the data set, but R can’t seem to find the variable. That’s because the variable ideology is hidden in the data frame nominate. In order to access variables in data frames, we need to do one of two things. Use the $ operator. Use the data argument. Some functions, such as exp() are designed to work with vectors, not data frames. This will be the case for most functions we use (with the notable exceptions of plotting in with ggplot() and estimating liner model with lm()). To use the functions on variables stored in data frames, we need to use the $ operator. Suppose we have a data set loaded and given to the object my_data. If my_data contains the variable of interest my_variable, then we can access my_variable using the syntax my_data$my_varible. That is, the syntax data$var means “get the variable var from the data set data.” We’ll use this often, so make sure it’s clear. sum(nominate$ideology) # example of the $ operator ## [1] NA sum(nominate$ideology, na.rm = TRUE) # example of the $ operator ## [1] 164.457 But some functions are designed to work with data frames. For example, the qplot() function in the ggplot2 package is designed to work with data sets. If you open the help file for qplot() (i.e., help(qplot) after library(ggplot2)), you’ll see that one of the arguments is data. If you use this argument to point qplot() to the data frame, it will know where to find your variables. # load ggplot2 package, which contains the qplot function library(ggplot2) # example of a function with a data argument qplot(ideology, data = nominate) # using the data argument ## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`. ## Warning: Removed 1 rows containing non-finite values (stat_bin). Many of the functions we use take a data argument. If they do not, though, we’ll need to use the $ operator. Because we’ll almost always use data stored in data frames, you need to be sure to use one approach or the other. If the function has a data argument, use it. In other cases, use the $ operator. 3.3 How We’ll Always Use R Open RStudio by clicking the .Rproj file for the project. (If you haven’t created the .Rproj file yet, then open RStudio and click File &gt; New Project…). Open a new R script to do something new OR open a previously saved script to continue making progress. Review Exercises Download the nominate data sets from the course website and put them in the data/ subdirectory. Start a new R script that loads the tidyverse package. In the same script, load the each version of the nominate data set using the appropriate function (note that tidyverse automatically loads readr but not haven). Assign each data set to a different object name. Use the glimpse() (part of tidyverse) function to get a quick look at each data set. Repeat using import() (load the rio package first!). Try to use five different functions (mean(), sum(), etc.) on the data frames or variables in the data frames. Using # comments, explain what each function is doing. "],
["histograms.html", "Chapter 4 Histograms 4.1 NOMINATE Data 4.2 Histograms: geom_histogram() 4.3 Filtering: filter() 4.4 Faceting: facet_wrap() 4.5 Density Plots: geom_density() 4.6 Color and Fill 4.7 Labels: labs() 4.8 Themes: theme_bw() and Others 4.9 Putting It All Together 4.10 Saving Plots: ggsave()", " Chapter 4 Histograms Histograms (and their variants, such as density plots and dot plots) serve as a useful tool for understanding the distribution of a single variable. For a histogram to work well, the variable of interest should take on enough values to require “binning” or lumping several values into the same column of the figure. This usually works for variables that we think of as continuous, such as income, age, effective number of political parties, GDP, number of casualties, etc. 4.1 NOMINATE Data To see how histograms work, let’s work with estimates of the ideology of the U.S. House of Representatives. These data are called DW-NOMINATE Scores and are available at voteview.com. The cleaned data set nominate.csv includes the 100th through the 114th Congresses and contains a variable called ideology that captures a representative’s ideology. These data are at the legislator-Congress level, so that each row in the data set represents an individual legislator in a particular Congress. More negative values indicate a more liberal representative. More positive values indicate a more conservative representative. For example, Ron Paul is one of the most conservative representatives in the data set with a score of about 0.86. Dennis Kucinich is among the most liberal and has a score of about -0.58. To get started, let’s load the cleaned data. # load packages library(tidyverse) ## Warning: package &#39;tibble&#39; was built under R version 3.6.2 ## Warning: package &#39;tidyr&#39; was built under R version 3.6.2 ## Warning: package &#39;purrr&#39; was built under R version 3.6.2 ## Warning: package &#39;dplyr&#39; was built under R version 3.6.2 # load data nominate &lt;- read_csv(&quot;data/nominate.csv&quot;) # note: make sure the file &#39;nominate.csv&#39; is in the &#39;data&#39; subdirectory We can use the glimpse() function (part of tidyverse) to check that we properly load the data and get a quick overview. It shows us the variable names, the types of variables we’re working with (e.g., character, integer, double), and the first few values. # quick look at the data glimpse(nominate) ## Rows: 7,080 ## Columns: 7 ## $ congress &lt;dbl&gt; 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, … ## $ chamber &lt;chr&gt; &quot;House&quot;, &quot;House&quot;, &quot;House&quot;, &quot;House&quot;, &quot;House&quot;, &quot;House&quot;, &quot;House… ## $ state &lt;chr&gt; &quot;AL&quot;, &quot;AL&quot;, &quot;AL&quot;, &quot;AL&quot;, &quot;AL&quot;, &quot;AL&quot;, &quot;AL&quot;, &quot;AK&quot;, &quot;AZ&quot;, &quot;AZ&quot;, … ## $ district &lt;dbl&gt; 2, 4, 3, 5, 6, 1, 7, 1, 2, 3, 5, 4, 1, 3, 1, 4, 2, 36, 10, 2… ## $ party &lt;chr&gt; &quot;Republican&quot;, &quot;Democrat&quot;, &quot;Democrat&quot;, &quot;Democrat&quot;, &quot;Democrat&quot;… ## $ name &lt;chr&gt; &quot;DICKINSON, William Louis&quot;, &quot;BEVILL, Tom&quot;, &quot;NICHOLS, William… ## $ ideology &lt;dbl&gt; 0.398, -0.213, -0.042, -0.175, -0.060, 0.373, -0.085, 0.279,… I’ve given the variables very descriptive names, but if you need more information, look in my codebook. 4.2 Histograms: geom_histogram() Now let’s try a histogram. To do this, we’ll use R package ggplot2, which is part of tidyverse. When we ran library(tidyverse), it automatically loaded ggplot2. If we hadn’t already run library(tidyverse), we could run library(ggplot2). Though ggplot2 is a complex package, we’ll get some sense of how it works this semester. It is well-documented online, so feel free to read about it as much as you want or need. 4.2.1 The Three Critical Components There are three critical components to each plot created using ggplot2, which we’ll refer to as a “ggplot.” the data: a formal data frame in R. the aesthetics: the relationship between the variables in the data set and the aesthetics of the plotted objects–location, color, size, shape, etc. the geometry: the type of plot. There are other components, such as scales, statistics, and coordinate systems, but these the three critical components usually provide what we need. 4.2.2 Drawing a Histogram data: the first argument to ggplot(). Because the variable we want to plot, ideology is contained in the data frame nominate, we use nominate as the first argument. aesthetic: the second argument to ggplot(). Because we want to create a histogram, we want ideology to correspond to the location along the horizontal-axis. To create this correspondence, we use aes(x = ideology) as the second argument. geometry: added to ggplot() with +. Because we want a histogram, we use geom_histogram(). (It’s weird to add a function to another function with +. And it is unusual. Nothing else in R works quite like this. It’s how ggplot2 works, though.) # specify data and aesthetic, then add the geometry ggplot(nominate, aes(x = ideology)) + geom_histogram() This histogram makes sense. We have a grouping of Democrats (presumably) on the left and a grouping of Republicans (again, presumably) on the right. You could probably come up with a model that explains why there are few politicians in the center. There are three subtle points that I should emphasize about the line ggplot(nominate, aes(x = ideology)) + geom_histogram(). The first argument supplied to ggplot() is the data. The second argument is the aesthetics. The second argument, aes(...) is itself a function. This function just creates the “aesthetic mapping,” a link between the variables in the data set and space, color, size, shape, etc, in the chart. geom_histogram() is also a function, but it is added to the plot using +. We’ll see several other functions that can be added to the plot. Review Exercises List and describe the three critical components of a ggplot. In the following snippet of code, label the data, the aesthetics, and the geometry: ggplot(nominate, aes(x = ideology)) + geom_histogram(). In this section, we’ve seen three new functions: ggplot(), aes(), and geom_histogram(). Describe what each one does and how to use it. 4.2.3 But geom_histogram() uses counts! FPP uses density on the vertical-axis, not counts. geom_histogram() uses counts by default. This is important, because if the bin widths of the histograms are not equal, then counts create a misleading histogram. However, if the bin widths are equal, then counts and densities produce identically shaped histograms. (The vertical axis has a different scale, though.) Because we tend to use equal bin widths in practice, it often doesn’t matter whether we use counts or densities. If you want to change the default behavior, add y = ..density.. to the aesthetics. (By default, geom_histogram() sets y = ..count...) # specify data and aesthetic, then add the geometry ggplot(nominate, aes(x = ideology, y = ..density..)) + geom_histogram() 4.3 Filtering: filter() Above, I plot the distribution of ideology scores for 15 different Congresses (the 100th through the 114th) in the same histogram. Let’s create a histogram for the 100th Congress only. To do this, first create a data set that contains only cases from the 100th Congress To create a subset of nominate that only contains the 100th Congress, use the filter() function (part of tidyverse). The first argument to filter() is the data frame to be filtered and the second argument is a logical statement that identifies the cases to keep. # filter the 100th congress nominate100 &lt;- filter(nominate, congress == 100) # quick look glimpse(nominate100) ## Rows: 441 ## Columns: 7 ## $ congress &lt;dbl&gt; 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, … ## $ chamber &lt;chr&gt; &quot;House&quot;, &quot;House&quot;, &quot;House&quot;, &quot;House&quot;, &quot;House&quot;, &quot;House&quot;, &quot;House… ## $ state &lt;chr&gt; &quot;AL&quot;, &quot;AL&quot;, &quot;AL&quot;, &quot;AL&quot;, &quot;AL&quot;, &quot;AL&quot;, &quot;AL&quot;, &quot;AK&quot;, &quot;AZ&quot;, &quot;AZ&quot;, … ## $ district &lt;dbl&gt; 2, 4, 3, 5, 6, 1, 7, 1, 2, 3, 5, 4, 1, 3, 1, 4, 2, 36, 10, 2… ## $ party &lt;chr&gt; &quot;Republican&quot;, &quot;Democrat&quot;, &quot;Democrat&quot;, &quot;Democrat&quot;, &quot;Democrat&quot;… ## $ name &lt;chr&gt; &quot;DICKINSON, William Louis&quot;, &quot;BEVILL, Tom&quot;, &quot;NICHOLS, William… ## $ ideology &lt;dbl&gt; 0.398, -0.213, -0.042, -0.175, -0.060, 0.373, -0.085, 0.279,… Use this new data frame to draw the histgram. # create histogram for 100th congress ggplot(nominate100, aes(x = ideology)) + geom_histogram() Repeat that process, but for the 114th Congress. # subset data to only 114th congress nominate114 &lt;- filter(nominate, congress == 114) # quick look glimpse(nominate114) ## Rows: 441 ## Columns: 7 ## $ congress &lt;dbl&gt; 114, 114, 114, 114, 114, 114, 114, 114, 114, 114, 114, 114, … ## $ chamber &lt;chr&gt; &quot;House&quot;, &quot;House&quot;, &quot;House&quot;, &quot;House&quot;, &quot;House&quot;, &quot;House&quot;, &quot;House… ## $ state &lt;chr&gt; &quot;AL&quot;, &quot;AL&quot;, &quot;AL&quot;, &quot;AL&quot;, &quot;AL&quot;, &quot;AL&quot;, &quot;AL&quot;, &quot;AK&quot;, &quot;AZ&quot;, &quot;AZ&quot;, … ## $ district &lt;dbl&gt; 3, 7, 2, 5, 1, 6, 4, 1, 8, 3, 1, 4, 6, 9, 2, 7, 5, 1, 3, 2, … ## $ party &lt;chr&gt; &quot;Republican&quot;, &quot;Democrat&quot;, &quot;Republican&quot;, &quot;Republican&quot;, &quot;Repub… ## $ name &lt;chr&gt; &quot;ROGERS, Mike Dennis&quot;, &quot;SEWELL, Terri&quot;, &quot;ROBY, Martha&quot;, &quot;BRO… ## $ ideology &lt;dbl&gt; 0.338, -0.390, 0.367, 0.600, 0.544, 0.779, 0.361, 0.279, 0.7… # create histogram ggplot(nominate114, aes(x = ideology)) + geom_histogram() Review Exercises Explain what the filter() function does and how to use it. What are the first and second arguments? What type of object does it return? Use help(filter) for the details. 4.4 Faceting: facet_wrap() Let’s investigate this divergence in the parties more carefully. Is the left hump actually Democrats? Is the right hump actually Republicans? Apply a facet to the histogram. A facet simply breaks the data frame into subsets and draws one histogram per subset. Create a facet by adding the function facet_wrap() to the plot. You’ll have to specify the faceting variable as an argument to the facet_wrap() function. In this case, we’ll do it by party, adding + facet_wrap(vars(party)) (i.e., “create a facet by the variable party”) to the ggplot we’ve been using. # build histogram ggplot(nominate100, aes(x = ideology)) + geom_histogram() + facet_wrap(vars(party)) Indeed, the left hump is Democrats and the right hump is Republicans. Review Exercises What does a facet do to a plot? Explain what the facet_wrap() function does and how to use it. Suppose I added + facet_wrap(party). Would that work? What is missing? Suppose you + facet_wrap(vars(state)) to ggplot(nominate100, aes(x = ideology)) + geom_histogram(). What would happen? facet_grid() offers an alternative method to faceting. How is it different from facet_wrap()? You might find this documentation helpful. With an example, show how to use facet_grid(). 4.5 Density Plots: geom_density() FPP sometimes uses a rough sketch of a histogram using a smooth curve rather than a complete histograms with vertical bars. For our purposes, a density plot is a smooth curve that approximates a histogram. We can easily create a density plot rather than a histogram by using geom_density() as the geometry rather than geom_histogram(). # create density plot ggplot(nominate100, aes(x = ideology)) + geom_density() Review Exercises How is a density plot similar to and different from a histogram? The proceedure for estimating the density is actually complicated and there are several ways to do it. I care about the intuition. What does the function geom_density() do and how do you use it? 4.6 Color and Fill 4.6.1 Color A density plot has the advantage it uses less ink. Because of this, we could use color rather than faceting to distinguish Republicans and Democrats. To distinguish Republicans and Democrats with color, add color = party to the aesthetics. # build density plot ggplot(nominate100, aes(x = ideology, color = party)) + geom_density() This look really good! Since we are most interested in the overlap between the parties, I think this density plot makes more sense for us, so let’s stick with it. 4.6.2 Fill Instead of coloring the lines differently, you can “fill” the density with different colors. While the color aesthetic represents the color of the line itself, the fill aesthetic represents the fill inside the line. To distinguish between the parties using fill, add fill = party to the aesthetics. # build density plot ggplot(nominate100, aes(x = ideology, fill = party)) + geom_density() 4.6.3 Alpha Notice that the Republican fill (blue-green) completely covers the Democrat fill (orange-red). But here’s a hint. The alpha argument to geom_density() controls the transparency. alpha = 0 is completely transparent. alpha = 1 is not transparent at all. # build density plot ggplot(nominate100, aes(x = ideology, fill = party)) + geom_density(alpha = 0.5) This slight transparency makes both distributions visible, even when the two overlap. Review Questions Explain the difference between the color and fill aesthetics for density plots. Use color to show the separate distributions of ideology scores for all 15 Congresses. It this useful? Explain how alpha transparency works and why you might use it. Suppose I used alpha = 0.1 instead of alpha = 0.5 in the plot above. How would the plot change? 4.7 Labels: labs() By default, ggplot() uses the variable names to label the axes and legend. By default, there is no title, subtitle, or caption. You will usually want to improve the axis labels and legends. You will sometimes want to add a title, subtitle, and/or caption. Make these changes by adding the labs() function to the plot. To the labs() function, supply one argument per aesthetic, such as x, y, color, or fill. You can also supply arguments for title, subtitle, or caption if you wish. These argument are character strings (surrounded by quotes), such as \"Ideology Score\" or \"A Density Plot of Ideology Scores for the 100th Congress\". # build density plot ggplot(nominate, aes(x = ideology, fill = party)) + geom_density(alpha = 0.5) + labs(x = &quot;Ideology Score&quot;, y = &quot;Density&quot;, fill = &quot;Party&quot;, title = &quot;A Density Plot of Ideology Scores for the 100th Congress&quot;, subtitle = &quot;There Are Few Moderates in Congress&quot;, caption = &quot;Data Source: DW-NOMINATE from voteview.com&quot;) This plot looks pretty nice. Review Exercises By default, how does ggplot() label the axes and legends? How can you change the default labels? How can you add a title, subtitle, or caption? Explain what the labs() function does and how to use it. Explain, in as much detail as you can, what each part the last block of code above does. 4.8 Themes: theme_bw() and Others Use themes to control the overall look of our ggplots. The theme controls elements such as the color of the background, the font family and size, the color and size of the grid, etc. There are the six themes provided with ggplot2. Theme Description theme_grey() The signature ggplot2 theme with a grey background and white gridlines, designed to put the data forward yet make comparisons easy. theme_bw() The classic dark-on-light ggplot2 theme. May work better for presentations displayed with a projector. theme_linedraw() A theme with only black lines of various widths on white backgrounds, reminiscent of a line drawings. Serves a purpose similar to theme_bw(). theme_light() A theme similar to theme_linedraw() but with light grey lines and axes, to direct more attention towards the data. theme_minimal() A minimalist theme with no background annotations. theme_classic() A classic-looking theme, with x and y axis lines and no gridlines. Notice what happens when you add theme_bw() (my favorite!) to the plot. # build density plot ggplot(nominate100, aes(x = ideology, fill = party)) + geom_density(alpha = 0.5) + theme_bw() If you are interested, the ggthemes package includes several other themes. Theme Description theme_economist() based on the Economist theme_economist_white() based on the Economist theme_excel() based on Excel theme_few() based on Few’s “Practical Rules for Using Color in Charts” theme_fivethirtyeight() based on fivethirtyeight.com plots theme_gdocs() based on Google docs theme_igray() inverse grey theme_pander() based on the pander package theme_solarized() based on the Solarized palette theme_solarized_2() based on the Solarized palette theme_stata() based on default Stata graphics theme_tufte() based on Tufte–minimum ink, maximum data theme_wsj() based on Wall Street Journal Try the the infamous (famously ugly) Excel theme. # load packages library(ggthemes) # for additional themes and even more fun! # build density plot ggplot(nominate100, aes(x = ideology, fill = party)) + geom_density(alpha = 0.5) + theme_excel() Review Exercises What are themes in ggplot2? What do they control/change? How do you change the theme of a ggplot? What are a few theme options from ggplot2? From ggthemes? Try a few different themes from both ggplot2 and ggthemes. What is your favorite and why? 4.9 Putting It All Together We’ll add two things back in. First, we’ll put out nice labels back in. Second, let’s go back to the nominate data frame (we’ve been using nominate100) and facet by Congress. # build density plot ggplot(nominate, aes(x = ideology, fill = party)) + geom_density(alpha = 0.5) + facet_wrap(vars(congress)) + labs(x = &quot;Ideology Score&quot;, y = &quot;Density&quot;, fill = &quot;Party&quot;, title = &quot;A Density Plot of Ideology Scores for the 100th Congress&quot;, subtitle = &quot;There Are Few Moderates in Congress&quot;, caption = &quot;Data Source: DW-NOMINATE from voteview.com&quot;) + theme_bw() 4.10 Saving Plots: ggsave() It is quite easy to save a ggplot as a .png or some other file. You just use the function ggsave(), which, by default, saves the last plot you created. Create the plot you want. Decide the file type you want to save your figure as. .png is a fine choice for our purposes. Choose where you want to save your figure. This should be somewhere in your working directory. I use doc/figs/. Choose a compact, descriptive name for your figure. For our final density plot, perhaps use ideology-by-congress.png. Use ggsave(). There are three important arguments to ggsave(). filename: This is the first argument to ggsave(). This is the file path. Given the choices above, it’s \"doc/figs/ideology-density-by-congress.png\". height: Explicitly name this argument. It controls the height of the figure (in inches, by default). height = 4 is a good starting point, but experiment. width: Explicitly name this argument. It controls the width of the figure (in inches, by default). width = 5 is a good starting point, but experiment. # save last plot as png ggsave(&quot;doc/figs/ideology-density-by-congress.pdf&quot;, height = 5, width = 8) Just to review and wrap up, here is the fill R script that I might use to create and save this figure. # load packages library(tidyverse) # loads readr, ggplot2, and others # load data nominate &lt;- read_csv(&quot;data/nominate.csv&quot;) # data are in the data/ subdirectory # quick look at the data glimpse(nominate) # build plot ggplot(nominate, aes(x = ideology, fill = party)) + geom_density(alpha = 0.5) + facet_wrap(vars(congress)) + labs(x = &quot;Ideology Score&quot;, y = &quot;Density&quot;, fill = &quot;Party&quot;, title = &quot;A Density Plot of Ideology Scores for the 100th Congress&quot;, subtitle = &quot;There Are Few Moderates in Congress&quot;, caption = &quot;Data Source: DW-NOMINATE from voteview.com&quot;) + theme_bw() # save last plot as png to the figs subfolder of the doc subfolder ggsave(&quot;doc/figs/ideology-density-by-congress.pdf&quot;, height = 5, width = 8) Review Exercises Explain how to save a ggplot. Be sure to discuss each of the five steps. Identify and describe each of the three arguments we usually supply to ggsave(). Explain, in as much detail as you can, what each part the last block of code above does. "],
["average-and-sd.html", "Chapter 5 Average and SD 5.1 The Intuition 5.2 The Measures", " Chapter 5 Average and SD 5.1 The Intuition If we took a histogram and tried to describe it to someone else without showing it to them, the most important pieces of information are usually the average and standard deviation or SD. We might describe the variable this way: “The values are about __________, give or take ________ or so.” We can think of the first blank as the average and the second blank as the SD. The average describes where the histogram is positioned along the left-right axis. The SD describes the width (or “spread” or “dispersion”) of the histogram. Note that the average and SD are not the only way to describe the “location” and “width” (“dispersion,” “spread”, or “scale”) of the histogram, but they are the most common. Inspect the collection of 20 numbers below. Fill in the blanks: “The values are about __________, give or take ________ or so.” 63 72 78 92 81 62 75 74 81 88 68 79 97 88 82 79 72 98 85 86 If we had to describe these data, we might say that our variable is “about 80, give or take 10 or so.” We can also visually these two summaries with a histogram. While this variable has a particular average (about 80), we can imagine shifting it left or right. The figure below shows some possible shifts. We could shift it way to the left, so that it’s “about 40” or a little bit to the right so that it’s “about 90.” We can also imagine increasing the SD (more spread) or decreasing the SD (less spread). The figure below shows some possible changes in the SD. In each case, the “give or take” number is changing. Exercise 5.1 For the following nine lists, choose whether the average is 0, 5, or 10 and the SD is 2, 10, or 20. You don’t need to do any calculation. -22, -10, 35, 7, -29, 14, 9, -3, 23, 27 -14, 17, 5, 8, 16, 12, 14, 16, 3, 24 10, -13, 6, 6, 4, -1, -8, 19, 6, 21 27, 37, -11, -23, 24, 15, -10, 2, 36, 2 5, 6, 6, 2, 3, 3, 9, 4, 6, 6 -12, -3, 9, 11, -1, 2, -7, 3, 16, -18 12, 12, 9, 10, 12, 12, 6, 10, 11, 7 0, -2, 0, 3, 3, 0, -2, -3, -1, 2 -3, 28, 12, 7, 25, 3, -38, -25, 7, -16 Solution List Mean SD 1 5 20 2 10 10 3 5 10 4 10 20 5 5 2 6 0 10 7 10 2 8 0 2 9 0 20 Exercise 5.2 For the following nine histograms, choose whether the average is 0, 5, or 10 and the SD is 2, 10, or 20. You don’t need to do any calculations. Solution Histogram Mean SD Histogram 1 9.97 20.18 Histogram 2 10.07 1.91 Histogram 3 9.94 9.97 Histogram 4 5.05 2.10 Histogram 5 5.07 10.10 Histogram 6 0.08 20.04 Histogram 7 4.94 20.09 Histogram 8 -0.02 1.97 Histogram 9 0.06 9.97 5.2 The Measures 5.2.1 The Average The most common measure of the location of a variable is the average.1 Suppose we have a variable (a list of numbers) \\(X = \\{x_1, x_2, ..., x_n\\}\\). \\[\\begin{equation} \\text{average} = \\dfrac{\\text{the sum of the list}}{\\text{the number of entries in the list}} = \\dfrac{\\sum_{i = 1}^n x_i}{n} \\nonumber \\end{equation}\\] The average is easy to compute and easy to work with mathematically.2 Unfortunately, the average doesn’t have an easy interpretation. The best interpretation, in my mind, is as the balance-point for the data. If we imagine the left-right axis as a teeter-totter and stack the data along the beam according to their values, then the average is the position of the fulcrum that would balance the data-filled beam. Exercise 5.3 Compute the average of the following lists of numbers. Round to two decimals placed as necessary. {0, 1} {0, 0, 1} {0, 1, 1} {0, 10} {0, 1, 2} {3, 6, 7, 1} {8, -4, 16, 4, 7} {7, -4, 7, 7, -4} Exercise 5.4 Suppose the entries in a list are between 1 and 10 (including both 1 and 10). What is the lowest possible average? What is the highest possible average? Exercise 5.5 To find the percent of observations that fall into a particular category, I count the number of observations in that category and divide that by the total number. That number lies between 0 and one. We refer to that number as a “proportion.” To convert the proportion to a percent, we just multiply the proportion times 100%. Suppose I asked 10 survey respondents whether they voted in the previous presidential election and obtained the following responses: {Yes, No, No, No, Yes, Yes, No, No, No, No}. What percent said “Yes”? What if I changed the “Yes” responses to the number 1 and the “No” responses to the number 0, then found the average? Is that related to the percent that said “Yes”? Is counting the “Yes” responses like changing them to ones, changing everything else to 0, and adding up the 0s and 1s? Exercise 5.6 Suppose a list of 10 numbers with an average of 50. If I add the number 100 to the list, what is the average of all 11 numbers? Solution If the average of the ten numbers is 50, then the sum of the ten numbers must be 500. If the sum of the ten numbers is 500, then the sum of the eleven numbers is 600. Since the sum of the eleven numbers is 600, the average is 600/11 = 54.55. 5.2.2 The Median When we’re describing the location of a variable (or, visually, of the histogram), we typically use the average. However, some researchers use the median. I introduce the median as an alternative to the average for two reasons. First, it helps us better understand the average conceptually. Second, you might encounter it, so it’s good to understand how the median works. The median is the (or one of the) numbers that divides the variable (or, visually, the histogram) into two equal pieces. For example, the number 3 divides the variable \\(X = \\{1, 2, 2, 3, 4, 6, 8\\}\\) into two equal pieces. The three values 1, 2, and 2 fall below the number 3. The three values 4, 6, and 8 fall above 3. Because 3 splits the variable into two equal pieces, it is the median. If you order the variable from highest to lowest, you can think of the median as the “middle” value. Notice, though, that things can run amok when we have an even number of observations in our variable. Consider the variable \\(X = \\{1, 2, 2, 4, 6, 8\\}\\). There is no “middle” value here. Notice that the values 3, 2.7, and 3.76421 all split the variable into equal pieces (1, 2, and 2 fall below each of these values; 4, 6, and 8 fall above). This means that there are mmany values that split the variable into equal pieces. But it isn’t very helpful—we need a single median to summarize the variable. When the number of observations in the variable is even, we just find the two middle values and split the difference. For the variable \\(X = \\{1, 2, 2, 4, 6, 8\\}\\), the numbers 2 and 4 are the two middle numbers. To “split the difference,” we just average them. The average of 2 and 4 is (2 + 4)/2 = 3. In this case, then, the median is three. In summary, we find the median with the following algorithm: Order the values from lowest to highest. If the number of values is odd, then just find the middle value. If the number of values is even, then find the two middle values and split the difference (or average them). Exercise 5.7 Find the average and median of the following variables. (I already ordered them from highest to lowest.) \\(X = \\{1, 3, 4, 6, 8\\}\\) \\(X = \\{1, 3, 4, 6, 80\\}\\) \\(X = \\{1, 3, 4, 6, 800\\}\\) \\(X = \\{1, 3, 4, 600, 800\\}\\) Do you notice something interesting? Explain. What is the average and median for \\(X = \\{1, 3, 400, 600, 800\\}\\)? What changed? The previous exercise illustrates something important about the average and the median. When a variable has a histogram with a longer tail in one direction, that long tail pulls the average away from the median in the direction of the tail. This happens because a adding few unusually large values can change average quite a lot, while changing the median very little. Remember that the average is the balance point for the histogram. As a tail stretches out in one direction, those data exert greater leverage and can pull the average in that direction. When a histogram has a longer tail to the right, we say that histogram is “right-skewed” or “skewed to the right.” When a histogram has a longer tail to the left, we say that histogram is “left-skewed” or “skewed to the left.” The figure below shows examples of a histogram with no skew, one that is skewed to the right, and one that is skewed to the left. It shows the average and median for each. Notice how the long tail pulls the average away from the median toward the long tail. ## Rows: 6 ## Columns: 4 ## $ group &lt;chr&gt; &quot;No Skew&quot;, &quot;No Skew&quot;, &quot;Skewed to the Left&quot;, &quot;Skewed to the Left… ## $ name &lt;chr&gt; &quot;Average&quot;, &quot;Median&quot;, &quot;Average&quot;, &quot;Median&quot;, &quot;Average&quot;, &quot;Median&quot; ## $ value &lt;dbl&gt; 51.08652, 51.05596, 75.87061, 78.36764, 19.71639, 17.28709 ## $ y &lt;dbl&gt; 0, 0, 0, 0, 0, 0 Exercise 5.8 For each histogram below, roughly estimate the average. Next, decide whether the histogram has no skew, is skewed to the right, or is skewed to the left. Then decide whether the median is less than, greater than, or about equal to the average. Solution no skew; median about the average no skew; median about the average skewed to the right; median below the average skewed to the left; median avove the average no skew; median about the average 5.2.3 The Standard Deviation The most common measure of spread is the standard deviation (SD). The intuition is subtle, so let’s look a a simple example. Rember, our goal is a “give-or-take” number. Suppose we have a list of numbers \\(X = \\{1, 2, 3, 4, 5\\}\\). The average of this list is 3, so we can compute the deviation from average or \\(d\\) for each value. \\[\\begin{equation} \\text{deviation from average} = d = \\text{value} - \\text{average} \\nonumber \\end{equation}\\] In this case, \\(d = \\{-2, -1, 0, 1, 2\\}\\). We want to use these deviations to find a give-or-take number. Here’s an initial idea. Just take the absolute values \\(|d| = \\{2, 1, 0, 1, 2\\}\\). These tell us how far each entry falls away from the average. Then we could average the absolute devations to find how far a typical entries falls away from the average of the list. In this case, we get 1.2. This is reasonable approach and we’ll refer to it as the “average absolute devation” or a.a.d. (It turns out that the a.a.d. isn’t a common quantity, so I don’t elevate it with an all-caps acronym.) The a.a.d. has one big problem–it uses an absolute value. This introduces some computational and mathematical difficulties. So let’s do something similar. Rather than take the absolute value, let’s square the deviations, take the average, and then undo the square at the end, so that \\(\\text{SD} = \\sqrt{\\text{avg}(d^2)}\\). Sometimes taking the (3) square root of (2) the average of (1) the squares is called the RMS. In this case, the RMS of the deviations from the average is the SD, so that \\[\\begin{equation} \\text{SD} = \\sqrt{\\text{avg}(d^2)} = \\sqrt{\\dfrac{(x_i - \\text{avg}(X))^2}{n}} = \\text{RMS of deviations from average}. \\nonumber \\end{equation}\\] The SD moves avoid the computation problems with the a.a.d. To calculate the SD, first make this little table, with the list of values, the deviations from the average, and the squares of the deviations. \\(X\\) \\(d\\) \\(d^2\\) 1 -2 4 2 -1 1 3 0 0 4 1 1 5 2 4 Then compute the average of the squares of the deviations, which in this case is 2. Then take the square root of that average, which in this case is about 1.4. Notice that 1.4 is about 1.2 (the a.a.d.). The SD is bounded (weakly) below by the a.a.s., but they’ll usually be close, so we can think of the SD as how far a typical point falls away from the average. Exercise 5.9 Recall that we define the SD as the RMS of the deviations from average. This definition reveals the steps to compute the SD–just work your way backwards through it. Remember that “R” stands for “root” (or “square root”), “M” stands for “mean” (or “average”), and “S” stands for “square.” Find the average. Find the deviations from the average. Find the S of the deviations from the average. Find the MS of the deviations from the average. Find the RMS of the deviations from the average. This is the SD. Now try these calculations for the list of numbers {5, 4, 8, 2, 6}. Clearly divide your work into the five steps above and connect each step back to the definition. Solution 5, 0, -1, 3, -3, 1 0, 1, 9, 9, 1 4+ 2 Exercise 5.10 Find the average and SD of the following lists of numbers. Round to two decimal places where necessary. 4, 2, 5, 6, 7 5, 2, 7, 10, 18 87, 123, 47, 62, 153 5.2.4 Other Summaries The combination of the average and SD is sometimes called a “two-number summary.” This combinations summarizes two important features of a variable using only two numbers. However, there are a few more ways to summarize a variable. Minimum or Min: the smallest value of the varaible. Maximum or Max: the largest value of the variable. 25th Percentile or 1st Quartile: the point which 25% of the data fall below and 75% fall above. To compute, arrange the data from lowest to highest and divide the data into an upper and lower half. If the total number of values is odd, the exclude the median from both halves. The 25th percentile is the median of the lower half. 75th Percentile or 3rd Quartile: the point which 75% of the data fall below and 25% fall above. The 75th percentile is the median of the upper half from above. Interquartile Range or IQR: the difference between the 75th and 25th percentiles. Intuitively, this is the width of the middle 50% of the data. We sometimes refer to the median, 25th and 75th percentiles, and minimum and maximum as the five-number summary. Exercise 5.11 Find the five-number summaries (min, 25th percentile, median, 75th percentile, max) of the following lists of numbers. Also compute the interquartile range. Round to two decimal places where necessary. 27, 9, 6, 38, 51, 61, 61, 95, 72 33, 6, 0, 28, 21, 5, 86, 67 3.57, 1.43, 3.55, 4.82, 2.59, 4.41, 5.14, 2.84, 4.89 3.8, 5.9, 6.3, 4.4, 8.3, 9.1, 6.8, 8.1, 5.4, 9.9, 5.8, 7.2 Exercise 5.12 As political scientists, we think a lot about trends in partisanship and voting behavior. Going back to The American Voter, published in 1960, the partisan identity of an individual has seemed to be the dominant factor in determining for whom they cast a vote. Before Looking at the figures below, how do you think partisan trends have shifted since the 1950s? Have people become more or less partisan? Why do you think this is? Write down a paragraph or so sketching out your model of partisanship trends. Exercise 5.13 The figures below are taken from “Partisanship and Voting Behavior, 1952-1996” published in 2000 by Larry Bartels. Figure 1 shows the proportion of survey respondents who identified as a “Weak Partisan” or a “Strong Partisan”, as well as those who consider themselves “Independent Leaners” or “Pure independents”. An Independent leaner is someone who may not be registered with a party (or does not think of themselves as ‘belonging’ to either party), but nevertheless consistently supports one party over the other. What proportion of the sample identified as partisan in 1952? Is this more or less than in 1996? Assume approximately 10,000 people responded to the survey in each year of the survey. Approximately how many more people considered themselves weak identifiers in 1976 than strong identifiers? Again, assume 10,000 responses in each iteration of the survey: are there more independents in 1996 than partisans? Of the independents, are more “leaners” or “pure”? Figure 2 shows the proportion of voters and nonvoters who identify themselves with one party or the other. In general, are those who vote more or less partisan than those who do not? In terms of partisanship, have voters and nonvoters become more or less similar? Exercise 5.14 How well do the data above support (or not) your hypothesized model? How would you change your model to accommodate this new information? These data end in 1996. If we collected data up to 2020 for figures 1 and 2, do you think the trends you identified would continue? Why? Some people refer to the “average” as the “mean”. I prefer to avoid this because the “mean” might also refer to the expectation of a random variable. I use “average” and “expected value” to differentiate these two meanings.↩︎ The median, alternatively, is not easy to compute and quite difficult to work with mathematically.↩︎ "],
["average-and-sd-in-r.html", "Chapter 6 Average and SD in R 6.1 mean() and sd() 6.2 Other Summaries 6.3 Data Frame Nuance 6.4 The group_by()/summarize() Workflow 6.5 geom_line()", " Chapter 6 Average and SD in R In the last chapter, we learned how to discribe the location and spread of a variable using the average and SD. I suggested that we can describe a large collection of numbers as: “…about ________, give or take ________ or so.” (The average belongs in the first blank; the SD belongs in the second.) We also learned how to compute the average and SD with paper-and-pencil to help us understand how these summaries work. In this chapter, I show how to compute the average, SD, and related summaries. 6.1 mean() and sd() If we have a vector of data, we can easily calculate the average and SD in R using the mean() and sd() functions in R. (Note that the sd() function uses a slightly different formula, but the difference usually isn’t important in practice.) # create variable x = {1, 2, 3, 4, 5} x &lt;- c(1, 2, 3, 4, 5) # compute two-number summary mean(x) # average ## [1] 3 sd(x) # SD; see note below ## [1] 1.581139 Exercise 6.1 In R, create the vector {1, 2, 2, 3, 3, 3, 2, 2, 1} and find the average and SD. Compare the SD from R to the SD using the RMS of the deviations formula we learned in the previous chapter. Exercise 6.2 In R, create the vector {2, 1, ?, 1, 4}. Here, “?” represents a missing value (e.g., perhaps a survey respondent refused to reveal their income to an interviewer). Use R to find the average and SD. You might find helpful to supply the argument na.rm = TRUE to each function (the default is na.rm = FALSE). How does this argument alter the calculations? Does the default beahvior make sense? 6.1.1 A Note About R’s sd() Function For reasons I don’t want to deal with now, R uses the formula \\(SD = \\sqrt{\\dfrac{(x_i - \\text{avg}(X))^2}{n - 1}}\\) rather than \\(\\sqrt{\\dfrac{(x_i - \\text{avg}(X))^2}{n}}\\). At the “M” stage of RMS, the sd() function divides by one less than the number of observations (or \\(n - 1\\)) rather than just the number of observations (\\(n\\)). In fairness to the software developers, the “one less” approach is more common, but is connected to ideas we’ll learn later in the class. For now, just realize that there are two ways to compute the SD and that R’s SD will be slightly larger than the SD with my formula. Importantly, the difference will be tiny in datasets with a typical number of observations. # illustrating the difference in R&#39;s sd() formula # R&#39;s method (see above) # our method deviations &lt;- x - mean(x) # compute the deviations from the average s &lt;- deviations^2 # square: the &quot;s&quot; in RMS of deviations m &lt;- mean(s) # mean: the &quot;m&quot; in RMS of deviations r &lt;- sqrt(m) # root: the &quot;r&quot; in RMS of deviations print(r) # RMS of deviations = SD ## [1] 1.414214 For this small dataset, R’s method (divide by \\(n - 1\\) at the M-step) gives an answer of 1.58. Our method (divide by \\(n\\) at the m-step, the way we usually would for an average) gives an answer of 1.41. 6.2 Other Summaries We can also use other obviously-named functions to compute the other summaries from the previous chapter. The only tricky part is that you must supply the percentiles you want as a proportion to the prob argument for the quantile() function. # compute five-number summary min(x) # minimum ## [1] 1 quantile(x, prob = 0.25) # 25th percentile ## 25% ## 2 median(x) # median ## [1] 3 quantile(x, prob = 0.75) # 75th percentile ## 75% ## 4 max(x) # maximum ## [1] 5 # all at once fivenum(x) ## [1] 1 2 3 4 5 # alternatively, but with average included summary(x) ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## 1 2 3 3 4 5 # interquartile range IQR(x) # note that IQR is all caps ## [1] 2 Exercise 6.3 In R, create the vector {1, 2, 2, 3, 3, 3, 2, 2, 1} and find each component of the five-number summary separately. Then using the fivenum() and summary() functions. Compute the interquartile range. Exercise 6.4 (Continued from a previous exercise.) In R, create the vector {2, 1, ?, 1, 4}. Here, “?” represents a missing value (e.g., perhaps a survey respondent refused to reveal their income to an interviewer). Use R to find the each component of the five-number summary separately. Experiment with supplying the argument na.rm = FALSE (the default) and na.rm = TRUE. How does this argument alter the calculations? Does the default beahvior make sense? Now try the fivenum() and summary() function without supplying na.rm = TRUE. Does the default behavior differ? 6.3 Data Frame Nuance We can compute on variables in data frames using the data$variable syntax we learned earlier in the semester. If the vector (or “variable” or “collection of numbers”) that we want to use is contained in a data frame, then we need to remember to use the data$variable syntax to give R permission to use that variable. # load packages library(tidyverse) # load nominate data nominate_df &lt;- read_rds(&quot;data/nominate.rds&quot;) # compute summaries on variable &quot;ideology&quot; in the data frame &quot;nominate_df&quot; ## compute two-number summary, without removing NA values mean(nominate_df$ideology) # average ## [1] NA sd(nominate_df$ideology) # SD ## [1] NA We run into an issue with the NA values. Because the ideology score is missing for one representative who never cast a vote, we cannot compute the average, which requires us to sum all the observations. If we don’t know all the values, then we don’t know the sum. As a quick fix, we can just ignore the missing values. We can tell R to remove the NA values before computing each statistic using the na.rm = TRUE argument. ## compute two-number summary, without removing NA values mean(nominate_df$ideology, na.rm = TRUE) # average ## [1] 0.02323167 sd(nominate_df$ideology, na.rm = TRUE) # SD ## [1] 0.4183598 ## compute five-number summary min(nominate_df$ideology, na.rm = TRUE) # minimum ## [1] -0.766 quantile(nominate_df$ideology, prob = 0.25, na.rm = TRUE) # 25th percentile ## 25% ## -0.378 median(nominate_df$ideology, na.rm = TRUE) # median ## [1] -0.038 quantile(nominate_df$ideology, prob = 0.75, na.rm = TRUE) # 75th percentile ## 75% ## 0.405 max(nominate_df$ideology, na.rm = TRUE) # maximum ## [1] 0.939 ## all at once fivenum(nominate_df$ideology) # this function drops NA values by default ## [1] -0.766 -0.378 -0.038 0.405 0.939 ## alternatively, but with average included summary(nominate_df$ideology) # this function also drops NA values by default ## Min. 1st Qu. Median Mean 3rd Qu. Max. NA&#39;s ## -0.76600 -0.37800 -0.03800 0.02323 0.40500 0.93900 1 ## interquartile range IQR(nominate_df$ideology, na.rm = TRUE) # note that IQR is all caps ## [1] 0.783 Exercise 6.5 Load the dataset health.rds and compute the two-number and five-number summaries (all separately) for the variable percent_uninsured. 6.4 The group_by()/summarize() Workflow The mean(), sd(), and other functions above work nicely for computing these summaries using all the values of a variable. But in most cases, we are interested in comparing the the average (or another summary) across groups. We usually want to compute the averages for men and women, the average for each of the 50 U.S. states, or the average for each party in each congress, for example. Let’s take a close look at the nominate dataset. # load packages library(tidyverse) # load nominate data nominate_df &lt;- read_rds(&quot;data/nominate.rds&quot;) # quick look glimpse(nominate_df) ## Rows: 7,080 ## Columns: 7 ## $ congress &lt;int&gt; 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, … ## $ chamber &lt;chr&gt; &quot;House&quot;, &quot;House&quot;, &quot;House&quot;, &quot;House&quot;, &quot;House&quot;, &quot;House&quot;, &quot;House… ## $ state &lt;chr&gt; &quot;AL&quot;, &quot;AL&quot;, &quot;AL&quot;, &quot;AL&quot;, &quot;AL&quot;, &quot;AL&quot;, &quot;AL&quot;, &quot;AK&quot;, &quot;AZ&quot;, &quot;AZ&quot;, … ## $ district &lt;int&gt; 2, 4, 3, 5, 6, 1, 7, 1, 2, 3, 5, 4, 1, 3, 1, 4, 2, 36, 10, 2… ## $ party &lt;fct&gt; Republican, Democrat, Democrat, Democrat, Democrat, Republic… ## $ name &lt;chr&gt; &quot;DICKINSON, William Louis&quot;, &quot;BEVILL, Tom&quot;, &quot;NICHOLS, William… ## $ ideology &lt;dbl&gt; 0.398, -0.213, -0.042, -0.175, -0.060, 0.373, -0.085, 0.279,… For these data, we might want to know the average ideology for Republicans and Democrats. We could do it the hard way, using filter() to create separate datasets for Republicans and Democrats. # create a data frame with only republicans rep_df &lt;- filter(nominate_df, party == &quot;Republican&quot;) # compute average mean(rep_df$ideology, na.rm = TRUE) ## [1] 0.4213385 But this is tedious, especially if we wanted to do it by party and Congress. That would be two parties and 16 Congresses–32 separate datasets to create and compute the average with. Much like faceting allows us to draw lots of plots for different subsets of the data, the group_by()/summarize() workflow allows us to do the same thing for averages (or other summaries). As you might guess, the group_by()/summarize() workflow has two steps: a grouping step and a summarizing step. 6.4.1 group_by() group_by() defines groups in the data frame. The first argument is the data frame to group. The remaining arguments are the grouping variables. You can think if the groups as a footnote at the bottom of the data set that just mentions the variables that define the groups of interest. # group the data frame by party and congress grouped_df &lt;- group_by(nominate_df, party, congress) # quick look at the grouped data frame; notice the groups glimpse(grouped_df) ## Rows: 7,080 ## Columns: 7 ## Groups: party, congress [32] ## $ congress &lt;int&gt; 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, … ## $ chamber &lt;chr&gt; &quot;House&quot;, &quot;House&quot;, &quot;House&quot;, &quot;House&quot;, &quot;House&quot;, &quot;House&quot;, &quot;House… ## $ state &lt;chr&gt; &quot;AL&quot;, &quot;AL&quot;, &quot;AL&quot;, &quot;AL&quot;, &quot;AL&quot;, &quot;AL&quot;, &quot;AL&quot;, &quot;AK&quot;, &quot;AZ&quot;, &quot;AZ&quot;, … ## $ district &lt;int&gt; 2, 4, 3, 5, 6, 1, 7, 1, 2, 3, 5, 4, 1, 3, 1, 4, 2, 36, 10, 2… ## $ party &lt;fct&gt; Republican, Democrat, Democrat, Democrat, Democrat, Republic… ## $ name &lt;chr&gt; &quot;DICKINSON, William Louis&quot;, &quot;BEVILL, Tom&quot;, &quot;NICHOLS, William… ## $ ideology &lt;dbl&gt; 0.398, -0.213, -0.042, -0.175, -0.060, 0.373, -0.085, 0.279,… Notice that glimpse() shows us the groups that we defined: party, congress [32]. This means that this data set now has 32 groups, one for each party-Congress combination. 6.4.2 summarize() After grouping, we use summarize() to create summaries for each group. Whenever we use summarize() on a grouped data frame, summarize() will compute quantities for each group. Since we created a group for each party-Congress, we can now use summarize to find the average for each, for example. The first argument to summarize() is the grouped data frame to summarize. The remaining arguments are the summaries to compute. As examples, let’s compute the average, SD, and median. # group the data frame by party and congress grouped_df &lt;- group_by(nominate_df, party, congress) # summarize the grouped data frame (across the groups) summarized_df &lt;- summarize(grouped_df, average = mean(ideology, na.rm = TRUE), sd = sd(ideology, na.rm = TRUE), median = median(ideology, na.rm = TRUE)) ## `summarise()` regrouping output by &#39;party&#39; (override with `.groups` argument) # quick look at ou glimpse() glimpse(summarized_df) ## Rows: 32 ## Columns: 5 ## Groups: party [2] ## $ party &lt;fct&gt; Democrat, Democrat, Democrat, Democrat, Democrat, Democrat, … ## $ congress &lt;int&gt; 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, … ## $ average &lt;dbl&gt; -0.3092901, -0.3130075, -0.3142407, -0.3333065, -0.3615000, … ## $ sd &lt;dbl&gt; 0.1653092, 0.1664293, 0.1658089, 0.1609726, 0.1524251, 0.137… ## $ median &lt;dbl&gt; -0.3200, -0.3200, -0.3200, -0.3360, -0.3815, -0.3835, -0.384… Notice three important things above. summarize() returns a data frame with one row per group. There are two types of variables in the new data frame. First, we have one variable for each grouping variable from the original data frame. Together, these identify each group. Second, the names of the second, third, …, arguments become variables in the resulting data frame. These variables contain the summaries for each group. We now have a data frame with our desired summaries. For the first row in the dataset, we have the average, SD, and median for the Democrats in the 100th Congress. The party and congress variables identify the group. The average, sd, and median variables contain the summaries for that group. Exercise 6.6 Use filter() create a dataset that contains only observations from the 115th Congress. The use group_by() and summarize() to compute the average ideology for each state’s delegation. Select the Environment tab in the upper-right panel. Click on the data frame with the summaries. Explore the values. What state has the most conservative delegation? Most liberal? Exercise 6.7 Use group_by() and summarize() to compute the minimum and maximum ideology for each congress (regardless of party). Select the Environment tab in the upper-right panel. Click on the data frame with the summaries. Explore the values. 6.5 geom_line() Because summarize() returns a data frame, we can easily work with it in ggplot. Let’s draw a **line plot* with congress mapped to the x aesthetic, average mapped to the y aesthetic, and party mapped to the color aesthetic. To draw a line plot rather than a histogram, we can use geom_line() as the geometry. # plot the average ideology by party and congress ggplot(summarized_df, aes(x = congress, y = average, color = party)) + geom_line() Exercise 6.8 Reproduce the plot above of the average ideology across Congress for each party. Create a second plot of the SD over time. Comment briefly on one or two interesting patterns in the data. Exercise 6.9 Use group_by() and summarize() to compute the minimum and maximum ideology for each congress (regardless of party). Select the Environment tab in the upper-right panel. Click on the data frame with the summaries. Explore the values. Create a line plot of the minimum across Congresses. Repeat for the maximum. Do any interesting patterns emerge? Would you say there’s a strong pattern or trend? Or do the results seem idiosyncratic? Solution # load packages library(tidyverse) # load nominate data nominate_df &lt;- read_rds(&quot;data/nominate.rds&quot;) # group the data frame by congress grouped_df &lt;- group_by(nominate_df, congress) # summarize the grouped data frame (across the groups) summarized_df &lt;- summarize(grouped_df, minimum = min(ideology, na.rm = TRUE), maximum = max(ideology, na.rm = TRUE)) # plot the minimum across congresses ggplot(summarized_df, aes(x = congress, y = minimum)) + geom_line() # plot the maximum across congresses ggplot(summarized_df, aes(x = congress, y = maximum)) + geom_line() "],
["the-normal-model.html", "Chapter 7 The Normal Model 7.1 The Intuition 7.2 Examples 7.3 The Empirical Rule 7.4 The Normal Approximation 7.5 The Normal Table 7.6 Using a Normal Approximation 7.7 Example: The Extremity of Party Leaders", " Chapter 7 The Normal Model 7.1 The Intuition Last week, we used the average and SD to reduce an entire variable to two summaries. We use the average and SD to fill in the following sentence: “The values are about ________, give or take ________ or so.” This week, we add an additional assumption. This week, we also say that the histogram of the variable follows the normal curve. The normal curve is a bell-shaped curve with the particular equation \\(f(x) = \\frac{1}{{\\sqrt {2\\pi } }}e^\\frac{{ - x ^2 }}{2}\\). The details of the equation are not important. It is important, though, to realize that the normal curve is an exact, precise shape. It is not a vague description. Nonetheless, a lot of numeric variables have histograms that roughly follow the normal curve. If the histogram of a variable is approximately “bell-shaped,” then we can use the normal curve as an approximate description of the histogram. 7.2 Examples It turns out that many variable’s have a histogram that resembles the normal curve. Because of this, the normal curve can sometimes serve as an effective model for these variables. For example, NOMINATE ideology scores for Republicans in the 115th Congress roughly follow the normal curve. However, the ideology scores for both Republicans and Democrats together does not follow a normal curve. Below are histograms of the ENEP (effective number of electoral political parties) by electoral system type and and the amount social heterogeneity. I overlaid rescaled version of the normal curve to assess how closely the histogram matches the normal curve. Notice that these deviate slightly from the normal curve. “Bell-shaped” is perhaps a small stretch, but not too far off. 7.3 The Empirical Rule If the variable roughly follows the normal curve (or is “bell-shaped”), then we have the following two rules: About 68% of the data (i.e., “most”) fall within 1 SD of the average. About 95% of the data (i.e., “almost all”) fall within 2 SDs of the average. We can evaluate this rule with the parties data above. Some of the nine hisgrams follow the normal curve quite well (e.g., lower-left). Others seem to meaningfully deviate from the normal curve (e.g., middle-left). The table below shows the actual percent of the variable that falls within one and two SDs of the average for each histogram. As you can see, for the lower-left panel (SMD, Top 3rd), the empircal rule of 68% and 95% matches the actual values of 74% and 98% fairly well. For the middle-left panel (SMD, Middle 3rd), the empirical rule matches the actual values of 87% and 93% less well. Across all histograms, it seems fair that the empirical rule works as a rough approximation, even for histograms that do not follow the normal-curve closely. Electoral System Social Heterogeneity within 1 SD within 2 SDs Single-Member District Bottom 3rd of ENEG 87% 96% Single-Member District Middle 3rd of ENEG 87% 93% Single-Member District Top 3rd of ENEG 74% 98% Small-Magnitude PR Bottom 3rd of ENEG 68% 97% Small-Magnitude PR Middle 3rd of ENEG 73% 96% Small-Magnitude PR Top 3rd of ENEG 76% 93% Large-Magnitude PR Bottom 3rd of ENEG 80% 98% Large-Magnitude PR Middle 3rd of ENEG 77% 96% Large-Magnitude PR Top 3rd of ENEG 65% 97% 7.4 The Normal Approximation If the histogram roughly follows the normal curve, then we can use the normal curve as a model to estimate the percent of the observations that fall in a given range. Just like we add up the area of the bars to compute percentages with a histogram, we add up the area under the normal curve to approximate percentages. For example, to find the percent of the data between -1 and +2 in the histogram below, we simply need to find the area of the red shape. 7.5 The Normal Table Unfortunately, the normal curve isn’t a collection of rectangles like a histogram. In this case, we need some understanding of integral calculus and a computer. Fortunately, we can use a simple of areas-under-the-curve without worrying about calculus. But there’s a problem: Histograms of variables can have different averages and different SDs. The normal curve only works for a variable with an average of zero and and SD of one. But there’s a simple solution: We can easily convert to standard units or z-scores to link a histogram with any average and SD to the normal curve. To convert to standard units or z-scores, We simply subtract the average and divide by the SD. \\(z\\text{-score} = \\dfrac{\\text{value} - \\text{average}}{\\text{SD}}\\) Suppose we have the list \\(X = \\{1, 2, 3, 4, 5\\}\\). Then the average is 3, and the SD is about 1.41. We can compute the zscore for the first entry 1 as \\(\\frac{1 - 3}{1.42} \\approx -1.42\\). Similarly, we can convert the entire list to z-scores and get \\(Z = \\{-1.42, -0.71, 0.00, 0.71, 1.42\\}\\). If you compute the average and SD of the list \\(Z\\), you will find zero and one, respectively. These \\(z\\)-scores have a nice interpretation: They tell you the number of SDs a value falls above (or below) the average. For example, a \\(z\\)-score of 1.5 corresponds to a value 1.5 SDs above the average. A \\(z\\)-score of -2.2 refers to a value 2.2 SDs below the average. So long as we convert to standard units, we can use a normal table to compute any areas of interest under the normal curve. As an example, the small table below reports the percent of the normal curve between a particular value \\(z\\) and \\(-z\\) (i.e., the area “in the middle”). (The normal table on p. A-104 of FPP works the same way. There’s a more complete normal table as an appendix to these notes.) z % between -z and z Status 0.00 0% 0.10 8% 0.20 16% 0.30 24% 0.40 31% 0.50 38% 0.75 55% 1.00 68% Important 1.50 87% 1.64 90% 1.96 95% Important 2.00 95% Important 3.00 100% 7.6 Using a Normal Approximation To do a normal approximation, follow these steps: Draw a picture. This is really important. If you can draw the correct picture, it’s really easy to find the correct approximation. Draw the normal curve. Label the points of interest. I find it helpful to label the average as well. Shade the area of interest. Convert the points of interest to z-scores. I like to add the z-scores in parentheses underneath the points of interest. Use the Rules of the Normal Curve. The normal table gives the area between \\(z\\) and its opposite \\(-z\\). See FPP A-104 or the appendix in these notes. Usually start here. The area under the entire curve is 100%. The curve is symmetric, so that the area above a particular value \\(z\\) equals the area below its opposite \\(-z\\). Let’s see an example. 7.7 Example: The Extremity of Party Leaders To illustrate the normal approximation, let’s use it to estimate the percent of each party more extreme than their leaders during the 115th Congress. The histograms below show the ideology of legislators by party. The table below shows the average and SD by party. Party Average SD Democrat -0.39 0.12 Republican 0.49 0.15 The table below lists some of the leaders of each party and their ideology score. For each party leader, we can use two approaches approaches to quickly estimate the percent of the party that is “more extreme” than their leader: inspect the histogram and use the normal approximation. Name Party Position Ideology Score Inspect Histogram Normal Approximation Actual RYAN, Paul D. Republican Speaker of the House 0.56 MCCARTHY, Kevin Republican Majority Leader 0.46 SCALISE, Steve Republican Majority Whip 0.56 McMORRIS RODGERS, Cathy Republican Conference Chair 0.43 PELOSI, Nancy Democrat Minority Leader -0.49 HOYER, Steny Hamilton Democrat Minority Whip -0.38 CLYBURN, James Enos Democrat Assistant Democratic Leader -0.46 LEWIS, John R. Democrat Senior Chief Deputy Minority Whip -0.59 Let’s start with Paul Ryan. First, his ideology score is 0.56 (more conservative than the average for his party). A quick glance at the histogram for Republicans suggests that about 40% of Republicans are more extreme (more conservative) than Paul Ryan. Since this histogram roughly follows the normal curve, we can get an even more precise estimate using the normal approximation. First, convert Paul Ryan’s ideology score to standard units. We get \\(z = \\frac{0.56 - 0.49}{0.15} \\approx 0.47\\). This means that Paul Ryan is about 0.47 SDs more conservative than average. To find the percent more extreme than Paul Ryan, start by drawing a picture. Draw the normal curve, add the points of interest, and shade the area of interest. Next, convert the points of interest to z-scores. The average always becomes zero and 0.56 becomes \\(\\frac{0.56-0.49}{0.15} = 0.47\\). Finally, use the rules. It’s usually best to start with the normal table, so let’s look up 0.47. There’s no z = 0.47, so just choose the closest, which is z = 0.45. According to the table, the percent between -0.45 and 0.45 is 34.73% or 35%. This is shown in orange below. If the orange piece “in the middle” is 35%, then there’s 65% left over in two tails together (green plus grey). Notice, though, that these two tails are symmetric and therefore equal. Then each tail has 65%/2 or about 33%. Therefore, the green area–the area of interest–is about 33%. This means that about 33% of Republican legislators are more extreme than Paul Ryan. Exercise 7.1 Repeat the exercise above for each leader. For Republicans, find the percent to the right. For Democrats, find the percent to the left. Remember to draw a picture, convert to z-scores, and then use the rules. "],
["the-x-y-space.html", "Chapter 8 The X-Y Space 8.1 Points 8.2 Lines", " Chapter 8 The X-Y Space 8.1 Points The scatterplot has two key aesthetics: the horizontal and vertical location of points. We refer to the horizontal location as “x” and the vertical location as “y.” We sometimes refer to this two-dimmensional space (of horizontal-vertical or x-y locations) as the “Cartisian coordinate system.” The table below contains five observations. Each observation has values for variables x and y. (In the context of a data analysis, we typically think of x as the key explanatory variable and y as the outcome varaible.) Observation x y #1 1 1 #2 4 4 #3 -4 -3 #4 2 -2 #5 -2 4 The plot below shows the location of each point in the x-y space. Exercise 8.1 Recreate the x-y space below on a sheet of paper. Add the following points (0, 0), (1, 2), (-3, 4), (2, -3), and (-4, -2). Hint The first number in the (x, y) pair represents the “x” or horizontal location. The second number represents the “y” or vertical location. The location of the second point is… Solution 8.2 Lines We can also draw lines in the x-y space. Remember, the formula for a line is \\(y = mx + b\\). Here, \\(y\\) and \\(x\\) represent variables (i.e., locations in the x-y space), \\(m\\) represents the slope of the line, and \\(b\\) represents the intercept. Consider the following four examples: Example Equation Intercept Slope Example 1 \\(y = x + 0\\) 0 1.0 Example 2 \\(y = -2x + 2\\) 2 -2.0 Example 3 \\(y = 0.5x - 1\\) -1 0.5 Example 4 \\(y = 3x - 2\\) -2 3.0 8.2.1 The Intercept The intercept \\(b\\) tells us where the line crosses the vertical slice of the space where \\(x = 0\\). For the examples below, I centered \\(x = 0\\) nicely in the plot–this will not always be the case. Exercise 8.2 For the lines below, identify the intercept visually, if possible. Hint All the lines look like 45-degree lines (or 315-degree) because the ranges of the axes are rescaled. This makes the problem a little trickier. For each line, identify where the line cross the slide of the space where \\(x = 0\\). When \\(x = 0\\), then \\(y = m \\times 0 + b = b\\). Remember that \\(b\\) represents the intercept. Solution The intercepts are 0, -2, 1, 1.5. You cannot see the third intercept visually, because the slice of the space where \\(x = 0\\) is not included in the plot. 8.2.2 Slope The slope \\(m\\) tells us how fast the line rises or falls as we move from left-to-right. If \\(m\\) is positive, then the line rises. If \\(m\\) is negative, then the line falls. If \\(m\\) is zero, then the line neither rises nor falls (stays constant at the same height). As \\(m\\) gets larger in magnitude, the line rises or falls faster. The best way to think about slope is as the “rise over run.” \\(\\text{slope} = \\dfrac{\\text{rise}}{\\text{run}}\\). Take Example 2 from the table above or \\(y = -2x + 2\\). Consider two scenaarios: one where \\(x = 0\\) and another where \\(x = 3\\). When \\(x = 0\\), we know that \\(y = 2\\) because the intercept \\(b\\) equals 2. When \\(x = 3\\), we have “run” 3 units to the right (i.e., \\(\\text{run} = \\text{2nd value} - \\text{1st value} = 3 - 0 = 3\\)) and \\(y = -2 \\times 3 + 2 = -6 + 2 = -4\\). When we run 3 units, we rise \\(-4 - 2 = -6\\) units (or fall 6 units). The table below summarizes our work. Scenario x y Scenario 1 0 2 Scenario 2 3 -4 \\(\\text{run} = x \\text{ in Scenario 2} - x \\text{ in Scenario 1} = 3 - 0 = 3\\) \\(\\text{rise} = y \\text{ in Scenario 2} - y \\text{ in Scenario 1} = -4 - 2 = -6\\) \\(\\text{slope} = \\dfrac{\\text{rise}}{\\text{run}} = \\dfrac{-6}{3} = -2\\) These calculations match the slope we find by inspecting the original equation \\(y = -2x + 2\\). The figure below shows the rise-over-run logic for each of the four example equations. Exercise 8.3 Add the lines \\(y = 2x - 1\\) and \\(y = -0.5x + 1\\) to the figured you sketched in Exercise 8.1. Hint Remember that a line is completely defined by only two points. Use this rule to draw the line. Choose two values of \\(x\\) and find the corresponding value of \\(y\\). Ideally, choose two values of \\(x\\) that are separated by some distance (so long as the resulting x-y pairs remain on our plot). Let’s try the first line using \\(x = -1\\) and \\(x = 2\\). For the first equation, we have \\(y = 2 \\times -1 -1 = -2 - 1 = -3\\) and \\(y = 2 \\times 2 -1 = 4 - 1 = 3\\), respectively. Then our two points are (-1, -3) and (2, 3), respectively. Just add lightly add these two points to the plot and draw a line that goes through both. Solution Exercise 8.4 What is the slope and intercept of the line below? Hint To find the intercept, find where the line crosses the vertical slide at \\(x = 0\\). This gives the intercept directly. To find the slope, simple choose two points along the line and find the rise (i.e., the vertical distance between the two points) and the run (i.e., the horizontal distance between the two points). The slope is the rise over the run or \\(\\text{slope} = \\dfrac{\\text{rise}}{\\text{run}}\\). Solution The intercept is -1 and the slope is 1.5, so the equation for the line is \\(y = 1.5x - 1\\). Exercise 8.5 What is the slope and intercept of the line below? Solution The intercept is 1 and the slope is -0.5, so the equation for the line is \\(y = -0.5x + 1\\). "],
["the-scatterplot.html", "Chapter 9 The Scatterplot 9.1 geom_point() 9.2 Example: Gamson’s Law 9.3 Example: Gapminder", " Chapter 9 The Scatterplot The scatterplot is the most powerful tool in statistics. The following comes as close to any rote procedure that I would recommend following: Always plot your data using a scatterplot. For some combinations of unordered, qualitative variables with a large number of categories, the scatterplot might not offer useful information. However, the plot itself will not mislead the researcher. Therefore, the scatterplot offers a safe, likely useful starting point for almost all data analysis. 9.1 geom_point() To create scatterplots, we simply use geom_point() as the geometry combined with our same approach to data and aesthetics. Here’s a simple example with hypothetical data. # create a fictional dataset with tribble() df &lt;- tribble( ~x, ~ y, 1, 1, 2, 2, 3, 6, 1, 3, 2.5, 5) # quick look at this fictional data frame glimpse(df) Rows: 5 Columns: 2 $ x &lt;dbl&gt; 1.0, 2.0, 3.0, 1.0, 2.5 $ y &lt;dbl&gt; 1, 2, 6, 3, 5 # create scatterplot ggplot(df, aes(x = x, y = y)) + geom_point() 9.2 Example: Gamson’s Law Here’s a more realistic example. gamson_df &lt;- read_rds(&quot;gamson.rds&quot;) glimpse(gamson_df) Rows: 826 Columns: 2 $ seat_share &lt;dbl&gt; 0.02424242, 0.46060607, 0.51515150, 0.47204968, 0.527… $ portfolio_share &lt;dbl&gt; 0.09090909, 0.36363637, 0.54545456, 0.45454547, 0.545… ggplot(gamson_df, aes(x = seat_share, y = portfolio_share)) + geom_point() Because the data are so dense, especially in the lower-left corner of the plot, we might use alpha transparency to make the density easier to see. ggplot(gamson_df, aes(x = seat_share, y = portfolio_share)) + geom_point(alpha = 0.3) 9.3 Example: Gapminder For a dataset with more variables, we can represent a few other variables using aesthetics other than location in space. For this example, we use country-level data from the gapminder package. We haven’t discussed this yet, but many R packages contain datasets that are useful as examples. In this case, we can load the gapminder dataset from the gapminder package using data(gapminder, package = \"gapminder\"). This is an alternative to downloading the dataset to your computer, uploading it to the project in RStudio Cloud, and reading it into R with, say, read_csv(). # load gapminder dataset from gapminder package data(gapminder, package = &quot;gapminder&quot;) glimpse(gapminder) Rows: 1,704 Columns: 6 $ country &lt;fct&gt; Afghanistan, Afghanistan, Afghanistan, Afghanistan, Afghani… $ continent &lt;fct&gt; Asia, Asia, Asia, Asia, Asia, Asia, Asia, Asia, Asia, Asia,… $ year &lt;int&gt; 1952, 1957, 1962, 1967, 1972, 1977, 1982, 1987, 1992, 1997,… $ lifeExp &lt;dbl&gt; 28.801, 30.332, 31.997, 34.020, 36.088, 38.438, 39.854, 40.… $ pop &lt;int&gt; 8425333, 9240934, 10267083, 11537966, 13079460, 14880372, 1… $ gdpPercap &lt;dbl&gt; 779.4453, 820.8530, 853.1007, 836.1971, 739.9811, 786.1134,… ggplot(gapminder, aes(x = gdpPercap, y = lifeExp, size = pop, color = continent)) + geom_point(alpha = 0.3) Because GDP per capita is skewed so heavily to the right, we might transform the x-axis from a linear scale (the default) to a log (base-10) scale. ggplot(gapminder, aes(x = gdpPercap, y = lifeExp, size = pop, color = continent)) + geom_point(alpha = 0.3) + scale_x_log10() Exercise 9.1 Get the health dataset from the data page and load it into R. Plot the variable percent_uninsured (the percent of each state’s population without health insurance) along the horizontal axis and the variable percent_favorable_aca (the percent of each state with a favorable attitude toward Obamacare) along the vertical axis. Interpret and speculate about any pattern. I encourage you to represent other variables with other aesthetics. Exercise 9.2 Continuing the exercise above, label each point with the state’s two-letter abbreviation. Experiment with the following strategies. geom_text() instead of geom_point() geom_label() instead of geom_point() geom_text_repel() in the ggrepel package in addition to geom_point() geom_label_repel() in the ggrepel package in addition to geom_point() Hint: Review the help files (e.g., ?geom_text()) and the contained examples to understand how to use each geom. The variable state_abbr contains the two-letter abbreviation, so you’ll need to include the aesthetic label = state_abbr in the aes() function. "],
["correlation-coefficient.html", "Chapter 10 Correlation Coefficient 10.1 Intuition 10.2 Computing 10.3 Interpreting 10.4 Example: Clark and Golder (2006)", " Chapter 10 Correlation Coefficient We’ve discussed several ways to reduce data–to summarize the key features of many observations using a single (or a few) numbers. A histogram visually shows the density in chosen bins. The average tells us the location of a set of observations. Remember the seesaw analogy. The SD tells us the spread or disperson of a set of observations. We can describe a list of numbers as being “about [the average] give or take [the SD].” You’ll remember that my view of the scientific method is concepts, models, measurements, and comparisons. We haven’t talked explicity about “comparisons” yet, but we’ve been doing it all along. We compared the histograms of ideology across Congresses. We compared the average ideology across Congresses for both parties. We compared the SDs of ideology across Congresses for both parties. But some numerical summaries directly compare multiple variables. The correlation coefficient allows us to describe the relationship between two variables. Before, we compared variables by comparing their histograms, averages, or SDs. The correlation coefficient is our first summary that compares two variables directly (rather than summarizing just one). 10.1 Intuition The correlation coefficient measures how well two variables “go together.” “Go together” means “as one goes up, the other goes up [or down].” “Go together” has linearity built into the meaning. The correlation coefficient does not describe curved relationships. The figure below shows some scatterplots and how well I might say these variables go together. However, I am firmly opposed to any rules that link particular correlation coefficients to strength of relationship. Imagine the following studies: A study comparing two measures of the same concept. A study comparing the effect of a dose of vitamin D in the first hour after birth on lifespan. A “weak” or “small” correlation in the first study would be impossibly large in the second. The interpretation of the strength of a relationship must be made by a substantive expert in a particular substantive context. I interpret a correlation coefficient my imagining the scatterplot it might imply. I use two guidelines to help me imagine that scatterplot: 0.9 seems a lot stronger than 0.7, but 0.4 seems barely stronger than 0.2. Around 0.4 [-0.4], the a correlation becomes “easily noticeable” without studying the plot carefully. For smaller datasets, this threshold increases toward 0.6 [-0.6] or higher; for larger datasets, the threshold shrinks toward 0.2 [-0.2] or lower. Below are scatterplots of 50 observations to help you get a sense of the relationship between the correlation coefficient and the scatterplot. Notice that the relationship becomes “noticeable” for about 0.5 (or perhaps 0.4) in this small dataset. Below is a similar collection of scatterplots, but for 1,000 observations. You’ll notice that the relationship becomes “noticable” for 0.3 or about 0.2 for this large dataset. Exercise 10.1 Guess the correlation coefficient for each scatterplot below. Solution dataset r Dataset 1 -0.60 Dataset 2 0.45 Dataset 3 0.90 Dataset 4 0.45 Dataset 5 0.55 Dataset 6 0.55 Dataset 7 0.10 Dataset 8 0.85 Dataset 9 0.85 Dataset 10 0.35 Dataset 11 0.60 Dataset 12 0.80 10.2 Computing Suppose we have the dataset below. x y 1 10 3 15 2 12 4 13 5 18 10.2.1 By Hand We can compute the correlation coefficient \\(r\\) as follows: \\(r = \\text{average of} \\left[ (x \\text{ in standard units}) \\times (y \\text{ in standard units}) \\right]\\) This definition implies the following process. Find the average of x. Find the SD of x. Convert x to standard units (i.e., z-scores, subtract the average then divide by the SD). Repeat 1-3 for y. Multiply each standardized x times it’s corresponding standardized y. Notice that the first x belongs to the first y, the second x belongs to the second y, and so on. So it’s important to keep the pairs together. Average these products. We can implement this formula by creating the little table below and then averaging the final column of products. x y x in SUs y in SUs product 1 10 -1.41 -1.32 1.87 3 15 0.00 0.51 0.00 2 12 -0.71 -0.59 0.41 4 13 0.71 -0.22 -0.16 5 18 1.41 1.61 2.28 The average of the final column is 0.88. 10.2.2 With R In R, we can compute the corrlation between x and y using cor(x, y). Note that dropping missing values is more complicated for pairs of data. If you want to drop missing values from the calculations, then cor(x, y, use = pairwise.complete.obs\") is a good choice. We can use the code below to find the correlation in the example above. x &lt;- c(1, 3, 2, 4, 5) y &lt;- c(10, 15, 12, 13, 18) cor(x, y) [1] 0.8814089 Exercise 10.2 Compute the correlation coefficient between each combination of the four variables below. Check your work with R. x y z 2 8 7 4 0 3 5 5 5 6 3 6 4 6 6 3 5 3 10.3 Interpreting In general, a correlation coefficient is NOT particularly useful. I introduce it for two reasons: Other people use it. We use it to obtain more useful quantities. However, the correlation coefficient \\(r\\) has a concrete interpretation: If \\(x\\) is one SD larger, then \\(y\\) is \\(r\\) SDs larger on average. We might also say that “a one SD increase in \\(x\\) leads to an \\(r\\) SD increase in \\(y\\) on average,” but we must take care that “leads to” describes a pattern in the data and does not describe a causal relationship. 10.4 Example: Clark and Golder (2006) For a substantive example, consider Clark and Golder’s data. # load parties dataset parties_df &lt;- read_rds(&quot;parties.rds&quot;) # compute correlation between enep and eneg for each electoral system grouped_df &lt;- group_by(parties_df, electoral_system) summarized_df &lt;- summarize(grouped_df, cor = cor(enep, eneg)) print(summarized_df) # A tibble: 3 x 2 electoral_system cor &lt;fct&gt; &lt;dbl&gt; 1 Single-Member District 0.0369 2 Small-Magnitude PR 0.448 3 Large-Magnitude PR -0.0192 electoral_system cor Single-Member District 0.04 Small-Magnitude PR 0.45 Large-Magnitude PR -0.02 As Clark and Golder expect, we get a correlation coefficient near zero in SMD systems. But contrary to their expectation, we also get a correlation coefficient near zero in large-magnitude PR systems. Exercise 10.3 Interpret the correlation for small-magnitude PR systems above by filling in the following blanks: A one SD increase in ENEG leads to a _____ SD increase in ENEP, on average. A _____ unit increase in ENEG leads to a _____ unit increase in ENEP, on average. Hint How many units is one SD for ENEG? What about for ENEP? Going from SDs to the original units is like going from feet to yards: you just need to know how many feet are in a yard (or how many SDs are in each original unit). Exercise 10.4 Read the excerpt from Clark, Golder, and Golder on pp. 477-478. Download the gamson dataset from the data page. Compute the correlation coefficient \\(r\\) between seat and portfolio shares and create a scatterplot of the two. Comment briefly. Solution # load data gamson_df &lt;- read_rds(&quot;gamson.rds&quot;) # compute correlation coefficient cor(x = gamson_df$seat_share, gamson_df$portfolio_share) [1] 0.9423176 # create scatterplot ggplot(gamson_df, aes(x = seat_share, y = portfolio_share)) + geom_point() Exercise 10.5 Use devtools::install_github(\"pos5737/pos5737data\") to get the latest version of the pos5737 data package. Load the data set anscombe into R with data(anscombe, package = \"pos5737data\"). Use glimpse(anscombe) to get a quick look at the data. Realize that this one data frame actually contains four different datasets stacked on top of each other and numbered I, II, III, and IV. The variable dataset contains the dataset ID. Use a group_by() and summarize() workflow to compute a correlation coefficient for each of the four datasets. How do they compare? What do they suggest about the strength of the relationship between \\(x\\) and \\(y\\)? Create a scatterplot of \\(x\\) and \\(y\\) with separate panels for each “dataset” (i.e., facet by the variable dataset). How do they compare? How would you describe the strength of the relationship between \\(x\\) and \\(y\\) in each panel? Would you say that the correlation coefficient offered a good summary of each dataset? Solution # get latest version of data package devtools::install_github(&quot;pos5737/pos5737data&quot;) # load anscombe data data(anscombe, package = &quot;pos5737data&quot;) # quick look glimpse(anscombe) Rows: 44 Columns: 3 $ dataset &lt;chr&gt; &quot;I&quot;, &quot;I&quot;, &quot;I&quot;, &quot;I&quot;, &quot;I&quot;, &quot;I&quot;, &quot;I&quot;, &quot;I&quot;, &quot;I&quot;, &quot;I&quot;, &quot;I&quot;, &quot;II&quot;, … $ x &lt;dbl&gt; 10, 8, 13, 9, 11, 14, 6, 4, 12, 7, 5, 10, 8, 13, 9, 11, 14, 6… $ y &lt;dbl&gt; 8.04, 6.95, 7.58, 8.81, 8.33, 9.96, 7.24, 4.26, 10.84, 4.82, … # compute correlations for each version of the variable &quot;dataset&quot; grouped_df &lt;- group_by(anscombe, dataset) summarized_df &lt;- summarize(grouped_df, cor = cor(x, y)) print(summarized_df) # A tibble: 4 x 2 dataset cor &lt;chr&gt; &lt;dbl&gt; 1 I 0.816 2 II 0.816 3 III 0.816 4 IV 0.817 # scatterplots ggplot(anscombe, aes(x = x, y = y)) + geom_point() + facet_wrap(vars(dataset)) "],
["regression.html", "Chapter 11 Regression 11.1 The Equation 11.2 The Conditional Average 11.3 The Best Line 11.4 The RMS of the Residuals 11.5 \\(R^2\\) 11.6 Fitting Regression Models 11.7 Standard Errors and p-Values 11.8 A Warning 11.9 Review Exercises", " Chapter 11 Regression 11.1 The Equation Let’s start by describing a scatterplot using a line. Indeed, we can think of the regression equation as an equation for a scatterplot. First, let’s agree that we won’t encounter a scatterplot where all the points \\((x_i, y_i)\\) fall exactly along a line. As such, we need a notation that allows us to distinguish the line from the observed values. We commonly refer to the values along the line as the “fitted values” (or “predicted values” or “predictions” and the observations themselves as the “observed values” or “observations.” We use \\(y_i\\) to denote the \\(i\\)th observation of \\(y\\) and use \\(\\hat{y}_i\\) to denote the fitted value (usually given \\(x_i\\)). We write the equation for the line as \\(\\hat{y} = \\alpha + \\beta x\\) and the fitted values as \\(\\hat{y}_i = \\alpha + \\beta x_i\\). We refer to the intercept \\(\\alpha\\) and the slope \\(\\beta\\) as coefficients. We refer to the difference between the observed value \\(y_i\\) and the fitted value \\(\\hat{y}_i = \\alpha + \\beta x_i\\) as the residual \\(r_i = y_i - \\hat{y}_i\\). Thus, for any \\(\\alpha\\) and \\(\\beta\\), we can write \\(y_i = \\alpha + \\beta x_i + r_i\\) for the observations \\(i = \\{1, 2, ..., n\\}\\). Notice that we can break each \\(y_i\\) into two pieces components the linear function of \\(x_i\\): \\(\\alpha + \\beta x_i\\) the residual \\(r_i\\). In short, we can describe any scatterplot using the model \\(y_i = \\alpha + \\beta x_i + r_i\\). The black points show the individual observations \\((x_i, y_i)\\), The green line shows the equation \\(\\hat{y} = \\alpha + \\beta x\\). The purple star shows the prediction \\(\\hat{y}_i\\) for \\(x = x_i\\). The orange vertical line shows the residual \\(r_i = y_i - \\hat{y}_i\\). Using this generic approach, we can describe any scatterplot using any line. Of course, the line above isn’t a very good line. How can we go about finding a good line? Before we talk about a good line, let’s talk about a good point. Suppose you have a dataset \\(y = \\{y_1, y_2, ... , y_n\\}\\) and you want to predict these observations with a single point \\(\\theta\\). It turns out that the average (the one you learned so long ago) is the best predictor of these observations because it minimizes the RMS of the errors (i.e., the deviations from the average). 11.2 The Conditional Average Have a look at the scatterplot below. What’s the portfolio share of a party in a coalition government with a seat share of 25%? Your eyes probably immediately begin examining a vertical strip above 25%. You probably estimate the average is a little more than 25%… call it 27%. You can see that the SD is about 10% because you’d need to go out about 10 percentage points above and below the average to grab about 2/3rds of the data. Now you’re informed by the data and ready to answer the question. Q: What’s the portfolio share of a party in a coalition government with a seat share of 25%? A: It’s about 28% give or take 10 percentage points or so. Notice that if we break the data into many small windows, we can visually create an average (and an SD) for each. Freedman, Pisani, and Purves (2008) refer to this as a “graph of averages.” Fox (2008) calls this “naive nonparametric regression.” It’s a conceptual tool to help us understand regression. For some datasets, these averages will fall roughly along a line. In that case, we can described the average value of \\(y\\) for each value of \\(x\\)–that is, the conditional average of \\(y\\)–with a line. Here’s the takeaway: a “good” line is the conditional average. 11.3 The Best Line So far, we have to results: The average is the point that minimizes the RMS of the deviations. We want a line that captures the conditional average. Just as the average minimizes the RMS of the deviations, perhaps we should choose the line that minimizes the RMS of the residuals… that’s exactly what we do. We want the pair of coefficients \\((\\hat{\\alpha}, \\hat{\\beta})\\) that minimizes the RMS of the residuals or \\(\\DeclareMathOperator*{\\argmin}{arg\\,min}\\) \\[\\begin{equation} (\\hat{\\alpha}, \\hat{\\beta}) = \\displaystyle \\argmin_{( \\alpha, \\, \\beta ) \\, \\in \\, \\mathbb{R}^2} \\sqrt{\\frac{r_i^2}{n}} \\end{equation}\\] Let’s have a quick look at two methods to find the coefficients that minimize the RMS of the residuals. grid search analytical optimization 11.3.1 Grid Search Because we’re looking for the pair \\((\\hat{\\alpha}, \\hat{\\beta})\\) that minimize the sum of the RMS residuals, we could simply check lots of different values. In some applications, this is a reasonable optimization routine. (It is not reasonable in the context of regression, where we have much faster and accurate tools.) The figure below shows the result of a grid search. In the top-left panel, we see the line and the residuals for each intercept-slope pair. The size of the points indicates the residual squared. Notice that some lines make big errors and other lines make small errors. In the top-right panel, we see a histogram of the squared residuals. In the lower-left panel, each point in the grid shows a intercept-slope pair. The label, color, and size indicate the RMS of the residuals. As we move around the grid, the RMS changes–we’re looking for the smallest. In the lower-right panel, the three lines show the evolution of the intercept, slope, and RMS of the residuals, respectively. Again, we’re looking for the pair that produces the smallest RMS of the residuals. For this search, the intercept -0.36 and the slope 0.89 produce the smallest RMS of the residuals (of the combinations we considered). 11.3.2 Analytical Optimization In the case of finding the line that minimizes the RMS of the residuals, we have an easy analytical solution. We don’t need a grid search. 11.3.2.1 Scalar Form Remember that we simply need to minimize the function \\(f(\\alpha, \\beta) = \\displaystyle \\sqrt{\\frac{\\sum_{i = 1}^n [y_i - (\\alpha + \\beta x_i)]^2}{n}}\\). This is equivalent to minimizing \\(h(\\alpha, \\beta) = \\sum_{i = 1}^n(y_i - \\alpha - \\beta x_i)^2\\). We sometimes refer to this quantity as the SSR or “sum of squared residuals.” If you’ve had calculus, you might recognize that we can us calculus to find \\((\\hat{\\alpha}, \\hat{\\beta})\\) that minimize the RMS of the residuals. If not, you can skip the calculus part below. However, pay attention to the boxes, which I’m calling “Theorems.” To minimize \\(h(\\alpha, \\beta)\\), remember that we need to solve for \\(\\frac{\\partial h}{\\partial \\alpha} = 0\\) and \\(\\frac{\\partial h}{\\partial \\beta} = 0\\) (i.e., the first-order conditions). Using the chain rule, we have the partial derivatives \\(\\frac{\\partial h}{\\partial \\alpha} = \\sum_{i = 1}^n [2 \\times (y_i - \\alpha - \\beta x_i) \\times (-1)] = -2 \\sum_{i = 1}^n(y_i - \\alpha + \\beta x_i)\\) and \\(\\frac{\\partial h}{\\partial \\beta} = \\sum_{i = 1}^n 2 \\times (y_i - \\alpha - \\beta x_i) \\times (-x_i) = -2 \\sum_{i = 1}^n(y_i - \\alpha - \\beta x_i)x_i\\) and the two first-order conditions \\(-2 \\sum_{i = 1}^n(y_i - \\hat{\\alpha} + \\hat{\\beta} x_i) = 0\\) and \\(-2 \\sum_{i = 1}^n(y_i - \\hat{\\alpha} + \\hat{\\beta} x_i)x_i = 0\\) 11.3.2.1.1 The 1st First-Order Condition \\[\\begin{align} -2 \\sum_{i = 1}^n(y_i - \\hat{\\alpha} - \\hat{\\beta} x_i) &amp;= 0 \\\\ \\sum_{i = 1}^n(y_i - \\hat{\\alpha} - \\hat{\\beta} x_i) &amp;= 0 \\text{ (divide both sizes by $-2$)} \\\\ \\sum_{i = 1}^n y_i - \\sum_{i = 1}^n \\hat{\\alpha} - \\sum_{i = 1}^n \\hat{\\beta} x_i &amp;= 0 \\text{ (distribute the sum)} \\\\ \\sum_{i = 1}^n y_i - n \\hat{\\alpha} - \\hat{\\beta}\\sum_{i = 1}^n x_i &amp;= 0 \\text{ (move constant $\\beta$ in front and realize that $\\sum_{i = 1}^n \\hat{\\alpha} = n\\hat{\\alpha}$)} \\\\ \\sum_{i = 1}^n y_i &amp; = n \\hat{\\alpha} + \\hat{\\beta}\\sum_{i = 1}^n x_i \\text{ (rearrange)} \\\\ \\frac{\\sum_{i = 1}^n y_i}{n} &amp; = \\hat{\\alpha} + \\hat{\\beta} \\frac{\\sum_{i = 1}^n x_i}{n} \\text{ (divide both sides by $n$)} \\\\ \\overline{y} &amp; = \\hat{\\alpha} + \\hat{\\beta} \\overline{x} \\text{ (recognize the average of $y$ and of $x$)} \\\\ \\end{align}\\] Theorem 11.1 The 1st first-order condition implies that the regression line \\(\\hat{y} = \\hat{\\alpha} + \\hat{\\beta}x\\) equals \\(\\overline{y}\\) when \\(x = \\overline{x}\\). Thus, the regression line must go through the point \\((\\overline{x}, \\overline{y})\\) or “the point of averages”. The figure below shows a regression line that goes through the point of averages. Theorem 11.2 We can rearrange the identity \\(\\hat{y} = \\hat{\\alpha} + \\hat{\\beta}x\\) from Theorem 11.1 to obtain the identity \\(\\hat{\\alpha} = \\overline{y} - \\hat{\\beta}\\overline{x}\\) 11.3.2.1.2 The 2nd First-Order Condition Sometimes, when writing proofs, you obtain a result that’s not particularly interesting, but true and useful later. We refer to these results as “lemmas.” We’ll need the following Lemmas in the subsequent steps. Lemma 11.1 \\(\\sum_{i = i}^n y_i = n\\overline{y}\\). Exercise 11.1 Prove Lemma 11.1. Lemma 11.2 \\(\\sum_{i = 1}^n x_i y_i - n \\overline{x} \\overline{y} = \\sum_{i = 1}^n (x_i - \\overline{x})(y_i - \\overline{y})\\). Exercise 11.2 Prove Lemma 11.2. Lemma 11.3 \\(\\sum_{i = 1}^n x_i ^2 - n \\overline{x}^2 = \\sum_{i = 1}^n (x_i - \\overline{x})^2\\). Exercise 11.3 Prove Lemma 11.3. \\[\\begin{align} -2 \\sum_{i = 1}^n(y_i - \\hat{\\alpha} - \\hat{\\beta} x_i)x_i &amp;= 0 \\\\ \\sum_{i = 1}^n(y_i - \\hat{\\alpha} - \\hat{\\beta} x_i)x_i &amp;= 0 \\text{ (divide both sides by -2)} \\\\ \\sum_{i = 1}^n(y_i x_i - \\hat{\\alpha}x_i - \\hat{\\beta} x_i^2) &amp;= 0 \\text{ (distribute the $x_i$)} \\end{align}\\] Now we can use use Theorem 11.2 and replace \\(\\hat{\\alpha}\\) with \\(\\overline{y} - \\hat{\\beta}\\overline{x}\\). \\[\\begin{align} \\sum_{i = 1}^n(y_i x_i - (\\overline{y} - \\hat{\\beta}\\overline{x})x_i - \\hat{\\beta} x_i^2) &amp;= 0 \\text{ (use the identity $\\hat{\\alpha} = \\overline{y} - \\hat{\\beta}\\overline{x}$)} \\\\ \\sum_{i = 1}^n( x_i y_i - \\overline{y} x_i + \\hat{\\beta}\\overline{x} x_i - \\hat{\\beta} x_i^2) &amp;= 0 \\text{ (expand the middle term)} \\\\ \\sum_{i = 1}^n x_i y_i - \\overline{y} \\sum_{i = 1}^n x_i + \\hat{\\beta}\\overline{x} \\sum_{i = 1}^n x_i - \\hat{\\beta} \\sum_{i = 1}^n x_i^2 &amp;= 0 \\text{ (distribute the sum)} \\end{align}\\] Now we can use Lemma 11.1 to replace \\(\\sum_{i = i}^n y_i\\) with \\(n\\overline{y}\\). \\[\\begin{align} \\sum_{i = 1}^n x_i y_i - n \\overline{y} \\overline{x} + \\hat{\\beta}n \\overline{x}^2 - \\hat{\\beta} \\sum_{i = 1}^n x_i^2 &amp;= 0 \\text{ (use the identity $\\sum_{i = i}^n y_i = n\\overline{y}$)}\\\\ \\sum_{i = 1}^n x_i y_i - n \\overline{x} \\overline{y} &amp;= \\hat{\\beta} \\left(\\sum_{i = 1}^n x_i^2 - n \\overline{x}^2 \\right) \\text{ (rearrange)}\\\\ \\hat{\\beta} &amp;=\\dfrac{\\sum_{i = 1}^n x_i y_i - n \\overline{x} \\overline{y}}{\\sum_{i = 1}^n x_i^2 - n \\overline{x}^2} \\text{ (rearrange)}\\\\ \\end{align}\\] This final result is correct, but unfamiliar. In order to make sense of this solution, we need to connect this identity to previous results. Now we can use Lemmas 11.2 and 11.3 to replace the numerator \\(\\sum_{i = 1}^n x_i y_i - n \\overline{x} \\overline{y}\\) with the more familiar expression \\(\\sum_{i = 1}^n (x_i - \\overline{x})(y_i - \\overline{y})\\) and replace the denominator \\(\\sum_{i = 1}^n x_i ^2 - n \\overline{x}^2\\) with the more familiar expression \\(\\sum_{i = 1}^n (x_i - \\overline{x})^2\\). \\[\\begin{align} \\hat{\\beta} &amp;=\\dfrac{\\sum_{i = 1}^n (x_i - \\overline{x})(y_i - \\overline{y})}{\\sum_{i = 1}^n (x_i - \\overline{x})^2} \\\\ \\end{align}\\] Denote the SD of \\(x\\) as \\(\\text{SD}_x\\) and the the SD of \\(y\\) as \\(\\text{SD}_y\\). Multiply the top and bottom by \\(\\frac{1}{n \\times \\text{SD}_x^2 \\times \\text{SD}_y}\\) and rearrange strategically. \\[\\begin{align} \\hat{\\beta} &amp;=\\frac{\\frac{\\sum_{i = 1}^n \\left(\\frac{x_i - \\overline{x}}{\\text{SD}_x} \\right)\\left(\\frac{y_i - \\overline{y}}{\\text{SD}_y} \\right)}{n} \\times \\frac{1}{\\text{SD}_x}}{ \\frac{1}{n \\times \\text{SD}_x^2 \\times \\text{SD}_y} \\times \\sum_{i = 1}^n (x_i - \\overline{x})^2} \\\\ \\end{align}\\] Now we recognize that the left term \\(\\dfrac{\\sum_{i = 1}^n \\left(\\frac{x_i - \\overline{x}}{\\text{SD}_x} \\right)\\left(\\frac{y_i - \\overline{y}}{\\text{SD}_y} \\right)}{n}\\) in the numerator is simply the correlation coefficient \\(r\\) between \\(x\\) and \\(y\\). \\[\\begin{align} \\hat{\\beta} &amp;=\\dfrac{r \\times \\frac{1}{\\text{SD}_x}}{\\frac{1}{\\text{SD}_x^2 \\times \\text{SD}_y}\\sum_{i = 1}^n \\frac{(x_i - \\overline{x})^2}{n}} \\\\ \\end{align}\\] Now we recognize that \\(\\sum_{i = 1}^n \\frac{(x_i - \\overline{x})^2}{n}\\) is almost the \\(\\text{SD}_x\\). Conveniently, it’s \\(\\text{SD}_x^2\\), which allows us to cancel those two terms. \\[\\begin{align} \\hat{\\beta} &amp;=\\dfrac{r \\times \\frac{1}{\\text{SD}_x}}{\\frac{1}{\\text{SD}_y}} \\\\ &amp; r \\times \\frac{\\text{SD}_y}{\\text{SD}_x} \\end{align}\\] This final result clearly connects \\(\\hat{\\beta}\\) to previous results. Theorem 11.3 \\(\\hat{\\beta} = r \\times \\dfrac{\\text{SD of } y}{\\text{SD of }x} = \\dfrac{\\sum_{i = 1}^n (x_i - \\overline{x})(y_i - \\overline{y})}{\\sum_{i = 1}^n (x_i - \\overline{x})^2} = \\dfrac{\\sum_{i = 1}^n x_i y_i - n \\overline{x} \\overline{y}}{\\sum_{i = 1}^n x_i^2 - n \\overline{x}^2}\\). In summary, we can obtain the smallest RMS of the residuals with results from Theorems 11.2 and 11.3. \\[\\begin{align} \\hat{\\beta} &amp;= r \\times \\dfrac{\\text{SD of } y}{\\text{SD of }x} \\\\ \\hat{\\alpha} &amp;= \\overline{y} - \\hat{\\beta}\\overline{x} \\end{align}\\] 11.4 The RMS of the Residuals Just like the SD offers a give-or-take number around the average, the “RMS of the residuals.” offers a give-or-take number around the regression line. Sometimes the RMS of the residuals is called the “RMS error (of the regression),” the “standard error of the regression,” or denoted as \\(\\hat{\\sigma}\\). Indeed, just like the SD is the RMS of the deviations from the average, the RMS of the residuals is the RMS of the deviations from the regression line. The RMS of the residuals tells us how far typical points fall from the regression line. We can compute the RMS of the regression by computing each residual and then taking the root-mean-square. But we can also use the much simpler formula \\(\\sqrt{1 - r^2} \\times \\text{ SD of }y\\). This formula makes sense because \\(y\\) has an SD, but \\(x\\) explains some of that variation. As \\(r\\) increases, \\(x\\) explains more and more of the variation. As \\(x\\) explains more variation, then the RMS of the residuals shrinks away from SD of \\(y\\) toward zero. It turns out that the SD of \\(y\\) shrinks toward zero by a factor of \\(\\sqrt{1 - r^2}\\). 11.5 \\(R^2\\) Some authors use the quantity \\(R^2\\) to assess the fit of the regression model. I prefer the RMS of the residuals because it’s on the same scale as \\(y\\). Also, \\(R^2\\) computes the what fraction of the variance of \\(y\\), which is the SD squared, is explained by \\(x\\). I have a hard time making sense of variances, because they are not on the original scale. 11.5.1 Adequacy of a Line In some cases, a line can describe the average value \\(y\\) quite well. In other cases, a line describes the data poorly. Remember, the regression line describes the average value of \\(y\\) for different values of \\(x\\). In the figure below, the left panel shows a dataset in which a line does not (and cannot) adequately describe the average values of \\(y\\) for describe low, middle, and high values (at least ad the same time). The right panel shows a data in which a line can adequately describe how the average value of \\(y\\) changes with \\(x\\). We can see that when \\(x \\approx -2\\), then \\(y \\approx 0\\). Similarly, when \\(x \\approx 0\\), then \\(y \\approx 4\\). A line can describe the average value of \\(y\\) for varying values of \\(x\\) when the average of \\(y\\) changes linearly with \\(x\\). A line does a great job of describing the relationship between seat shares and portfolio shares in government coalitions. A line poorly describes the relationship between Polity IV’s DEMOC measure and GDP per capita. When we have variable that’s skewed heavily to the right, we can sometimes more easily describe the log of the variable. For this dataset, the line poorly describes the average logged GDP per capita for the various democracy scores. 11.6 Fitting Regression Models To fit a regression model in R, we can use the following approach: Use lm() to fit the model. Use coef(), arm::display(), texreg::screenreg(), or summary() to quickly inspect the slope and intercept. 11.6.1 geom_smooth() In the context of ggplot, we can show the fitted line with geom_smooth(). gamson &lt;- read_rds(&quot;data/gamson.rds&quot;) ggplot(gamson, aes(x = seat_share, y = portfolio_share)) + geom_point() + geom_smooth() By default, geom_smooth() fits a smoothed curve rather than a straight line. There’s nothing wrong with a smoothed curve—sometimes it’s preferable to a straight line. But we don’t understand how to fit a smoothed curve. To us the least-squares fit, we supply the argument method = \"lm\" to geom_smooth(). geom_smooth() also includes the uncertainty around the line by default. Notice the grey band around the line, especially in the top-right. We don’t have a clear since of how uncertainty enters the fit, nor do we understand a standard error, so we should not include the uncertainty in the plot (at least for now). To remove the grey band, we supply the argument se = FALSE to geom_smooth(). The line \\(y = x\\) is theoretically relevant–that’s the line that indicates a perfectly proportional portfolio distribution. To include it, we can use geom_abline(). ggplot(gamson, aes(x = seat_share, y = portfolio_share)) + geom_point() + geom_smooth(method = &quot;lm&quot;, se = FALSE) + geom_abline(intercept = 0, slope = 1, color = &quot;red&quot;) 11.6.2 lm() The lm() function takes two key arguments. The first argument is a formula, which is a special type of object in R. It has a left-hand side and a right-hand side, separated by a ~. You put the name of the outcome variable \\(y\\) on the LHS and the name of the explanatory variable \\(x\\) on the RHS. The second argument is the dataset. fit &lt;- lm(portfolio_share ~ seat_share, data = gamson) 11.6.3 Quick Look at the Fit We have several ways to look at the fit. Experiment with coef(), arm::display(), texreg::screenreg(), and summary() to see the differences. For now, we only understand the slope and intercept, so coef() works perfectly. coef(fit) (Intercept) seat_share 0.06913558 0.79158398 The coef() function outputs a numeric vector with named entries. The intercept is named (Intercept) and the slope is named after its associated variable. To find the RMS of the residuals, do the following: r &lt;- residuals(fit) sqrt(mean(r^2)) # rms of residuals [1] 0.06880963 11.7 Standard Errors and p-Values Almost all software reports p-values or standard errors by default. These are testing the null hypothesis that the coefficient (slope or intercept) equals zero. You can multiply the standard errors by plus-and-minus two to get a 95% confidence interval. The two functions below summarize the fitted model, giving you some quantities you’ve seen and understand and others that you haven’t. arm::display(fit, detail = TRUE) lm(formula = portfolio_share ~ seat_share, data = gamson) coef.est coef.se t value Pr(&gt;|t|) (Intercept) 0.07 0.00 17.12 0.00 seat_share 0.79 0.01 80.81 0.00 --- n = 826, k = 2 residual sd = 0.07, R-Squared = 0.89 texreg::screenreg(fit) ======================= Model 1 ----------------------- (Intercept) 0.07 *** (0.00) seat_share 0.79 *** (0.01) ----------------------- R^2 0.89 Adj. R^2 0.89 Num. obs. 826 ======================= *** p &lt; 0.001; ** p &lt; 0.01; * p &lt; 0.05 11.8 A Warning When we simply describe a single set of measurements with a histogram or an average, then we intuitively remain in the world of description. Indeed, making a causal claim requires a comparing factual and counterfactual scenarios. In the case of a single histogram or average we only have one scenario and there is no comparison. When we have descriptions of multiple sets of measurements, say the average life expectancy for democracies and the average life expectancy for autocracies, we an easily interpret one scenario as factual and the other as counterfactual. On its face, though, both scenarios are factual. We can comfortably say that democracies have healthier populations than autocracies without claiming that the regime type causes this difference. But it is… oh. so. tempting. Regression models, by design, describe an outcome across a range of scenarios. Indeed, a regression model describes how the average value of \\(y\\) changes as \\(x\\) varies. The temptation to treat these neatly arranged scenarios as factual and counterfactual grows even stronger. But unless one makes a strong argument otherwise, statistical models describe the factual world. With few exceptions, statistical data analysis describes the outcomes of real social processes and not the processes themselves. It is therefore important to attend to the descriptive accuracy of statistical models, and to refrain from reifying them. —Fox (2008, p.3) Note that some methodologists claim that their statistical models can obtain estimates of the causal effects. These models might actually succeed on occasion. However, the researcher should carefully avoid seeing counterfactual worlds from regression models. Usually, credible causal inferences come from careful design in the data collection stage, not from complicated conditioning at the modeling stage. 11.9 Review Exercises Exercise 11.4 Use devtools::install_github(\"pos5737/pos5737data\") to get the latest version of the pos5737 data package. Load the data set anscombe into R with data(anscombe, package = \"pos5737data\"). Use glimpse(anscombe) to get a quick look at the data. Realize that this one data frame actually contains four different datasets stacked on top of each other and numbered I, II, III, and IV. Fit a regression model on each “dataset” in the anscombe dataset. To only use a subset of the dataset, you can filter() the dataset before supplying the data to lm() or use can supply the subset argument to lm(). In this case, just supplying subset = dataset == \"I\" to lm(), for example, is probably easiest. Fit the regression to all four datasets and put the intercept, slope, RMS of the residuals, and number of observations for each regression in a little table. Interpret the results. For each of the four regression fits, create a scatterplot of the fitted values versus the residuals. Describe any inadequacies. Exercise 11.5 Use regression to test Clark and Golder’s (2006) theory using the parties dataset. First, create scatterplots between ENEG and ENEP faceted by the electoral system with with the least-squares fit included in each. Then fit three separate regression models. Fit the model \\(\\text{ENEP}_i = \\alpha + \\beta \\text{ENEG}_i + r_i\\) for SMD systems, small-magnitude PR systems, and large-magnitude PR systems. (Hint: Use filter() to create three separate data sets–one for each electoral system.) Exercise 11.6 This continues Exercise ??. Get the economic-model CSV dataset from GitHub. In three separate regressions, use GDP, RDI, and unemployment to explain the incumbent’s margin of victory. Which measure of economic performance best describes incumbents’ vote shares? Hint: Maybe the RMS of the residuals will be helpful? Using the best model of the three, which incumbents did much better than the model suggests? Which incumbents did much worse? Hint: Maybe make a scatterplot of your prefered variable and the margin of victory and use geom_label() with the aesthetic label = incumbent_name. "],
["appendix-a-normal-table.html", "A Appendix: A Normal Table", " A Appendix: A Normal Table z % between -z and z 0.00 0.00% 0.05 3.99% 0.10 7.97% 0.15 11.92% 0.20 15.85% 0.25 19.74% 0.30 23.58% 0.35 27.37% 0.40 31.08% 0.45 34.73% 0.50 38.29% 0.55 41.77% 0.60 45.15% 0.65 48.43% 0.70 51.61% 0.75 54.67% 0.80 57.63% 0.85 60.47% 0.90 63.19% 0.95 65.79% 1.00 68.27% 1.05 70.63% 1.10 72.87% 1.15 74.99% 1.20 76.99% 1.25 78.87% 1.30 80.64% 1.35 82.30% 1.40 83.85% 1.45 85.29% 1.50 86.64% 1.55 87.89% 1.60 89.04% 1.65 90.11% 1.70 91.09% 1.75 91.99% 1.80 92.81% 1.85 93.57% 1.90 94.26% 1.95 94.88% 2.00 95.45% 2.05 95.96% 2.10 96.43% 2.15 96.84% 2.20 97.22% 2.25 97.56% 2.30 97.86% 2.35 98.12% 2.40 98.36% 2.45 98.57% 2.50 98.76% 2.55 98.92% 2.60 99.07% 2.65 99.20% 2.70 99.31% 2.75 99.40% 2.80 99.49% 2.85 99.56% 2.90 99.63% 2.95 99.68% 3.00 99.73% 3.05 99.77% 3.10 99.81% 3.15 99.84% 3.20 99.86% 3.25 99.88% 3.30 99.90% 3.35 99.92% 3.40 99.93% 3.45 99.94% 3.50 99.95% 3.55 99.96% 3.60 99.97% 3.65 99.97% 3.70 99.98% 3.75 99.98% 3.80 99.99% 3.85 99.99% 3.90 99.99% 3.95 99.99% 4.00 99.99% 4.05 99.99% 4.10 100.00% 4.15 100.00% 4.20 100.00% 4.25 100.00% 4.30 100.00% 4.35 100.00% 4.40 100.00% 4.45 100.00% "],
["references.html", "References", " References "]
]
